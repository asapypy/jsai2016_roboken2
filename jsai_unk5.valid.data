how do i scale <unk> ( <unk> ) features ? hello <unk> ! thanks for your time , i 'll be <unk> . i have a <unk> dataset <unk> x <unk> about <unk> are <unk> [ <unk> <unk> ] ( http : <unk> ? <unk> ) , <unk> negative <unk> or <unk> <unk> <unk> to <unk> literature . these <unk> <unk> over several <unk> of <unk> and are not <unk> . i am <unk> to use svm or nn for cross validation . 1. do i need to scale these data ? it is frequently <unk> ( ng , <unk> , etc . ) that features should be <unk> to the range <unk> or <unk> for <unk> . does this apply for nn as well ? 2. if <unk> is required , what would you suggest for these <unk> variables ? is it <unk> to use the <unk> <unk> distribution function ( <unk> ) to scale the values to <unk> ? <eoq> 1. yes ( scale for nn ) . for <unk> , ( <unk> if using a kernel ) - you 'll want to scale them as well . ( many methods expect the features to have been <unk> in fact ) . easy way to do this is scikit learn - standard <unk> . 2. just <unk> and/or scale to values to between <unk> i <unk> <eoa> 
help needed for <unk> of <unk> <unk> x-post from http : <unk> ( please post answers there if you can ) hi , a few <unk> and i have an idea : <unk> the <unk> of <unk> <unk> to show <unk> <unk> and <unk> of the product . this <unk> out of the time i <unk> reading <unk> before <unk> a product . we 're starting with <unk> , and need to know where to start looking to search for answers to ( <unk> of ) this problem . we 've read `` <unk> learning to extract and <unk> by <unk> <unk> , <unk> <unk> and <unk> <unk> '' , and have a dataset : https : <unk> thanks : ) <unk> . : this is my first post on reddit . <eoq> found some useful suggestions on : http : <unk> could still use your opinion ! : ) <eoa> 
row vs column vectors is there a <unk> as to whether features are <unk> as row or column vectors ? or is it a free for all on that front ? <eoq> from what i 've seen ( using sklearn , matlab , <unk> ... ) features are always columns ( each sample is row in matrix ) . however , it is only <unk> as far as i know . <eoa> 
what does the <unk> symbol mean when <unk> neural networks ? i 'm trying to read up on some papers about neural networks , and without a background in math or computer science , some terms and <unk> <unk> me . several papers i have come across <unk> that `` we <unk> a column vector in <unk> '' or similar ( where d is a <unk> ) . but the only meaning of <unk> that i know of is the <unk> one . does it mean that every vector <unk> is a real number , or does it mean something else ? <eoq> yes , your understanding is correct <eoa> 
what does the <unk> symbol mean when <unk> neural networks ? i 'm trying to read up on some papers about neural networks , and without a background in math or computer science , some terms and <unk> <unk> me . several papers i have come across <unk> that `` we <unk> a column vector in <unk> '' or similar ( where d is a <unk> ) . but the only meaning of <unk> that i know of is the <unk> one . does it mean that every vector <unk> is a real number , or does it mean something else ? <eoq> r usually <unk> real numbers so you are correct . <eoa> 
number of parameters in <unk> class and two class logistic regression if you have f number of features , then in 2 class <unk> you have f parameters ( <unk> the bias ) . however in multiclass <unk> with k classes you have <unk> parameters , correct ? which means that using multiclass ( softmax ) <unk> for a 2 class problem would have <unk> the parameters of using 2 class ( sigmoid ) <unk> . what is <unk> me is why is it not <unk> ( <unk> ) parameters for multiclass <unk> ? <eoq> actually it is <unk> ( <unk> ) [ <unk> ] ( http : <unk> ) <eoa> 
predicting when a resource will run out ? i am trying to figure out what approach would be best for predicting when a resource will run out or be low and needs to be <unk> ? for example , let 's say i have a <unk> <unk> at a <unk> and am able to tell the amount of <unk> left in the <unk> ( by weight , <unk> , <unk> , or whatever means is best ) . how could i use this data <unk> over time to predict when it will be empty and will need to be <unk> back up ? i need to be able to see that the <unk> may get used more on <unk> , or maybe during <unk> months , so taking time into <unk> is pretty important . i have been thinking about using a neural network with <unk> ( time ) or <unk> ( time ) as a time input variable , but i feel <unk> as in my machine learning class two <unk> ago we <unk> <unk> problems more than these kinds of problems . <eoq> why would you need a neural network for this ? this can be easily <unk> with a linear equation based on the number of customer , then a <unk> or gamma distribution can be used to represent the use of the <unk> <unk> . <eoa> 
predicting customer <unk> behavior i work at a <unk> site and i have a <unk> of data at my <unk> . we want to be able to predict if a user will <unk> or not , and for users that are likely to not <unk> , <unk> them a free <unk> . i am pretty <unk> in python ( pandas , <unk> , etc . ) and working with large datasets , but not so much with ml . where do i start ? <eoq> perhaps with [ coursera ml course ( link to reddit ml thread ) ] ( http : <unk> ) <eoa> 
random forests i 'm a machine learning newbie . to learn something in ml , i have implemented a random forest ( scikit ) based classifier for <unk> prediction based on <unk> data . i.e the classifier <unk> states like <unk> , <unk> , etc based on <unk> <unk> output . the rf classifier <unk> <unk> well in predicting state , however , i would like to add something <unk> to a feedback <unk> to it . what i mean is that , if the classifier <unk> and i hand correct it . for the next run , on same dataset , there should n't be a <unk> . i 'm using scikit . any pointers ? <eoq> with rf you should simply include that sample with correct ( by hand ) prediction in your training set and then <unk> your classifier . <unk> is to use some [ online learning ] ( http : <unk> ) <unk> . <eoa> 
why does <unk> consider -1 to be an `` <unk> '' tanh hidden neuron output ? from the [ stanford <unk> tutorial ] ( http : <unk> ) : > <unk> , we will think of a neuron as being <unk> ( or as <unk> ) if its output value is close to 1 , or as being <unk> if its output value is close to <unk> we would like to <unk> the neurons to be <unk> most of the time . this <unk> <unk> a sigmoid activation function . if you are using a tanh activation function , then we think of a neuron as being <unk> when it outputs values close to -1 . tanh is <unk> about the <unk> . if you <unk> the <unk> of the <unk> weights then an `` <unk> '' neuron is effectively active again . this <unk> of advice s even <unk> in the <unk> <unk> of their tutorial . is it a <unk> or is there a good reason that an output of -1 is somehow less `` active '' than 1 ? <eoq> <unk> were <unk> <unk> as <unk> <unk> of binary threshold neurons . then people started using tanh because it 's faster than <unk> and has the same shape . it 's just a sigmoid with a bias of <unk> , <unk> by 2 , basically . so essentially they are still thinking of tanh 's as <unk> of binary threshold neurons . however it 's not even true for binary neurons . nns do n't care whether a neuron is `` active '' or `` <unk> '' . they can just <unk> by a negative weight and <unk> the bias , and a zero <unk> 1 and a one <unk> 0 . <eoa> 
[ <unk> algorithm for hidden markov models ] i was going over the <unk> algorithm for <unk> and <unk> into a <unk> question about the new values for the symbol distribution -- basically that , unless every observation contains every symbol in the model , it looks like the algorithm will update the missing symbol probabilities to zero for all states . i 'm probably just missing something <unk> simple here , but i 'd really appreciate it if someone would point it out to me . for the <unk> of a common <unk> , [ here 's ] ( http : <unk> % <unk> % <unk> % <unk> ) the <unk> . any updated probability of symbol <unk> in state i [ b i ( <unk> ) ] can be <unk> as the sum of a <unk> subset of the <unk> for that observation and state , so the sum <unk> the updated values of b i ( <unk> ) will always be equal to one . but unless every symbol in b is in every observation the model <unk> on , at least one of the <unk> wo n't have a gamma that <unk> to it for any state , so it 'll get updated to zero across all states ( the <unk> for <unk> i ( k ) is zero ) . assuming that the model <unk> on a sequence like that ( say <unk> , b ' , where each state has a distribution over <unk> ' , <unk> ' , and <unk> ' ) , then it ca n't update on a new sequence like <unk> , c ' with the <unk> symbol in it . it 'll also estimate the probability of such a sequence as zero . after <unk> on <unk> , b ' , the probability of <unk> c in any state is zero , which makes the the <unk> <unk> <unk> to it also zero for all states . the <unk> for gamma is the sum over states i , of [ alpha i ( <unk> ) * <unk> i ( <unk> ) ] . all of those are zero , so the <unk> is zero . just <unk> the probability of <unk> , c ' runs into a similar problem -- the probability of <unk> c is zero ; which takes the probability of an observation with c in it to zero . <unk> do n't just work on <unk> of their symbol sets , so i 'm clearly missing something . could someone please explain what ? <eoq> you should have non-zero initial <unk> probabilities for all <unk> . this way , within each observation , the probability of the hidden states for that observation are used <unk> the probability of <unk> that sequence in the first place . when your <unk> probabilities are later <unk> , the <unk> <unk> and states will <unk> non-zero . <eoa> 
noob question , how to deal with <unk> amount of data for each class , using deep networks hi i am trying to do classification using a cnn , but the amount of training data i have <unk> for each class . what is the correct method in order to deal with this ? <eoq> i do n't think it <unk> . it could possibly learn <unk> <unk> probabilities , and possibly <unk> performance on cases where it is very <unk> . try <unk> the cases of the classes you want more of , more . i.e . <unk> multiple <unk> of them in the training set or multiplying the error so those cases matter more . <eoa> 
noob question , how to deal with <unk> amount of data for each class , using deep networks hi i am trying to do classification using a cnn , but the amount of training data i have <unk> for each class . what is the correct method in order to deal with this ? <eoq> there is no one easy way to do this and it 's in no way specific to cnns . first , are you classes imbalanced <unk> in your training data with respect to the <unk> <unk> ? or is the <unk> distribution imbalanced in the same way ? having a handle on the class distribution is very helpful here . if you google for things like `` class <unk> '' or `` <unk> <unk> '' you may get some ideas . i 've tested this <unk> : http : <unk> but it did n't appear to have much <unk> . <eoa> 
<unk> out for convolutional <unk> and convolutional <unk> if i implement dropout in convolutional <unk> or convolutional <unk> , do i dropout entire <unk> ( i.e . consider say only 30 out of <unk> kernels for a <unk> update ) , or do i dropout individual hidden units from the kernel <unk> . ( i.e . i use all <unk> kernels , but within each kernel 's own hidden layer i dropout <unk> of units ) . <eoq> the <unk> is the more common approach . but , <unk> and see what works better for your problem . <eoa> 
how to get data from a site ? i do n't know if this is the right place to post this , so please correct me if i am wrong . id like to make a program that can search for recipes by the name of food item . there is a big <unk> of recipes at http : <unk> i have never worked with getting data from the net , but from what i understand people generally use api <unk> by the <unk> ? so how can i search for recipes from this site automatically , do i need them to provide some `` <unk> api '' , or can is there some <unk> program people use to <unk> such <unk> ? <eoq> when there is no api i <unk> use c # <unk> # <unk> with their standard http client <unk> for <unk> and some <unk> to extract <unk> . you might have to play with some http <unk> / <unk> to get <unk> to talk to you . <eoa> 
how to get data from a site ? i do n't know if this is the right place to post this , so please correct me if i am wrong . id like to make a program that can search for recipes by the name of food item . there is a big <unk> of recipes at http : <unk> i have never worked with getting data from the net , but from what i understand people generally use api <unk> by the <unk> ? so how can i search for recipes from this site automatically , do i need them to provide some `` <unk> api '' , or can is there some <unk> program people use to <unk> such <unk> ? <eoq> most likely too <unk> for you , but it could be useful to <unk> : <unk> <unk> a <unk> course on <unk> . <unk> 2 is on <unk> <unk> , with example code in python + <unk> library . there is no <unk> in this <unk> . <eoa> 
dealing with imbalanced data set i 'm working on a course project to classify tweets as either `` interesting '' or `` not interesting '' . i have hand <unk> about <unk> tweets , and <unk> up with a <unk> of about <unk> ( interesting : not interesting ) . i <unk> the `` not interesting '' tweets and got <unk> distribution . i use <unk> cross validation on this set and get very good performance with a naive bayes classifier . however , i feel it is a <unk> <unk> to <unk> <unk> of the <unk> tweets . so the question : could i still train my classifier using the imbalanced data set , as long as my validation set is <unk> ? or am i <unk> some issues with this ? <eoq> there 's a lot of literature on imbalanced data sets . <eoa> 
dealing with imbalanced data set i 'm working on a course project to classify tweets as either `` interesting '' or `` not interesting '' . i have hand <unk> about <unk> tweets , and <unk> up with a <unk> of about <unk> ( interesting : not interesting ) . i <unk> the `` not interesting '' tweets and got <unk> distribution . i use <unk> cross validation on this set and get very good performance with a naive bayes classifier . however , i feel it is a <unk> <unk> to <unk> <unk> of the <unk> tweets . so the question : could i still train my classifier using the imbalanced data set , as long as my validation set is <unk> ? or am i <unk> some issues with this ? <eoq> if you know the <unk> context ( i.e . the class <unk> when you are <unk> the model ) then you can use <unk> analysis to <unk> an optimal classifier on the <unk> <unk> . the <unk> for accuracy will be a line with a <unk> degree <unk> for <unk> classes and will change for <unk> classes . ( <unk> or <unk> ) . the <unk> gives a good <unk> if you do not know the <unk> context the model would be <unk> in , but <unk> in mind some areas under the curve might not be <unk> . in particular if you are doing an information <unk> type task ( sounds like it ) then you will be more interested in having a <unk> <unk> on the left hand side of the <unk> curve than the <unk> score . this is a good paper : http : <unk> <eoa> 
how can you predict the next <unk> of video ? lets say i have a very large series of sequential images . the images are all <unk> similar , and sequential images are particularly similar . there <unk> some <unk> rule that <unk> each image from the previous few images . i want to create a program that can <unk> that rule . i want to feed my program the last few sequential images , and have it guess the next . how would i go about this ? as a <unk> i have been reading around trying to <unk> things out for myself , but i feel i understand less the more i read . <eoq> [ this talk ] ( http : <unk> ? <unk> ) has a <unk> of deep gaussian <unk> doing exactly this . it 's a very hard task though . <eoa> 
recommended way to <unk> a set of probability distributions into a feature vector ? i <unk> a number of short <unk> into its <unk> and <unk> stanford 's <unk> sentiment analysis on each one . this <unk> the words and works with the <unk> trees only . as a result , for each text now i have a number of probability distributions of being in one out of <unk> possible sentiment classes ( from very negative to very positive ) . now i want to define the feature vector for the <unk> . my first approach was to add <unk> elements to the feature vector for each unique sentence tree in the output of the sentiment classifier , then just <unk> it with the values of the probability distributions . does this make sense ? would n't i be somehow <unk> the <unk> of the sentiment <unk> ? thanks ! <eoq> `` i <unk> a number of short <unk> into its <unk> and <unk> stanford 's <unk> sentiment analysis on each one . this <unk> the words and works with the <unk> trees only . as a result , for each text now i have a number of probability distributions of being in one out of <unk> possible sentiment classes ( from very negative to very positive ) . now i want to define the feature vector for the <unk> . '' in addition to what you suggest , it might make sense to also include the mean as a feature . `` my first approach was to add <unk> elements to the feature vector for each unique sentence tree in the output of the sentiment classifier , then just <unk> it with the values of the probability distributions . does this make sense ? would n't i be somehow <unk> the <unk> of the sentiment <unk> ? '' do you mean representing the probabilities as a vector ( i.e . [ <unk> , <unk> , <unk> , <unk> , <unk> ] ) <unk> to take into <unk> the fact that the <unk> are <unk> - that 1 and 2 are more similar than 1 and 5 ? while this is a <unk> <unk> , i do n't think that it will be a big deal if you have enough data . it is common to put <unk> features into buckets when using linear models . this has the same issue but it works well in <unk> . <eoa> 
