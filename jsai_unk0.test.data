would anyone be willing to look over my simple nn octave code ? i 'm currently taking the stanford ml course on coursera and have been more or less breezing through all the programming assignments . this includes neural networks and the back-propagation algorithm . however , i tried re-creating it on my own from memory and i just ca n't get it to work right . first i tried in java , then in octave , and i 'm getting similar results - a bit of learning at first and then it levels off at a low success percentage . obviously something is wrong ( probably in my back-propagation algorithm ) , but i 've been over and over it and ca n't figure out what . there are no errors when i run the code , but i 'm thinking it has to be something obvious that i 'm overlooking . would anyone be willing to take a look at some heavily commented octave code to see if anything looks wrong ? code is [ here ] ( https : //gist.github.com/jumpy89/e7d5197cd7c9e39092e7 ) . <eoq> if you still need to fix it , have a look at : http : //ufldl.stanford.edu/wiki/index.php/gradient_checking_and_advanced _optimization <eoa> 
problem : simulate a mechanical device that outputs numbers from 1-100 . so i ve recently gotten into statistical modeling and this problem popped into my head . so here s the rest of the setup : the device outputs a list like : 5,34,65,88 , 07 , 32 without replacements and no repetition so according to my readings this can be thought of as the classical ball and urn model and so the whole theory of hidden markov models can be used to develop a solution to this problem . so what i 'd like is reassurance that yes i m on the right track with a solution to this and also what software should i use to actually try and solve this . <eoq> when i first read your problem i thought of hidden markov models . another approach may be recurrent neural networks . <eoa> 
common mistakes for beginners ? my ann is converging poorly . i 'm getting into machine learning , and tried implementing a neural net in java based off of [ this ] ( http : //neuralnetworksanddeeplearning.com/chap1 ) online book . i 've tested two different training scenarios - one was simply adding two numbers between 0 and 9 together , and the other was recognizing handwritten digits from the mnist data set . the problem is convergence is slow and tops out at a fairly low level ( ~70-80 % correct answers for addition and 25-35 % for mnist data ) . it 's obviously working somewhat because i 'm getting more correct answers than i would by chance alone , but i feel like i should be getting a lot more . the addition problem is easy and the book shows a python implementation getting over 90 % correct after a single training epoch . varying learning rate and size/number of hidden layers has n't helped much ( also i feel like i need to have a pretty high learning rate for things to get anywhere , like 1-10 ) . i 've been over and over my backpropagation algorithm many times and ca n't find any mistakes . is there anything obvious i may have missed or should recheck ? any help would be much appreciated . <eoq> you may want to try the ufldl tutorial 's technique for checking your feed-forward network code : <eoa> 
common mistakes for beginners ? my ann is converging poorly . i 'm getting into machine learning , and tried implementing a neural net in java based off of [ this ] ( http : //neuralnetworksanddeeplearning.com/chap1 ) online book . i 've tested two different training scenarios - one was simply adding two numbers between 0 and 9 together , and the other was recognizing handwritten digits from the mnist data set . the problem is convergence is slow and tops out at a fairly low level ( ~70-80 % correct answers for addition and 25-35 % for mnist data ) . it 's obviously working somewhat because i 'm getting more correct answers than i would by chance alone , but i feel like i should be getting a lot more . the addition problem is easy and the book shows a python implementation getting over 90 % correct after a single training epoch . varying learning rate and size/number of hidden layers has n't helped much ( also i feel like i need to have a pretty high learning rate for things to get anywhere , like 1-10 ) . i 've been over and over my backpropagation algorithm many times and ca n't find any mistakes . is there anything obvious i may have missed or should recheck ? any help would be much appreciated . <eoq> you should be getting a lot more . if you are doing the same number of iterations as that online book , you should be getting the same results . for a similar architecture , you can get up to 98.4 % accuracy on mnist ( eg http : //deeplearning.net/tutorial/mlp.html ) . you clearly have a bug . i would write unit tests for the pieces until you find it . good luck ! <eoa> 
the cmu course was my favorite <eoa> 
i think the [ unsupervised feature learning and deep learning tutorial ] ( http : //ufldl.stanford.edu/wiki/index.php/ufldl_tutorial ) is a good follow up . <eoa> 
study note automation hi everybody . should start out by saying that i am new to ml . i have a strong background in stats and calculus , but none in programming ( i 'll be taking the coursera class in programming this june ) . for a while now , i 've had an idea for a program thats been bouncing around in my head . specifically , i want to create a program that takes text ( likely in .pdf or .doc formats ) and convert it into a series of questions and answers . for example , given the following : > text- the capital of canada is ottawa . > converted into q & a format- q : what is the capital of canada ? a : what capital of canada is ottawa . any suggestions for starting this would be greatly appreciated . i know there are a lot of courses out there , but if someone could narrow down what areas i should focus on , then i could take a targeted approach to learning about what i need to know . thanks ! <eoq> two quick comments . first , you would probably want to deal with raw text rather than proprietary formats . there are tools to extract text from doc and pdf files , but they are error prone . second , this is n't so much a machine learning task as it is a syntax task . see any good nlp/linguistics/syntax textbook on the topic of wh- movement , and you 'll see that there 's most likely a simple rule-based solution to this problem . <eoa> 
difficulty training simple xor function . i 've created a neural network , with the following structure : input1 - input2 - **input layer** . n0 - n1 - **hidden layer** . 3 weights per node ( one for bias ) . n2 - **output layer** . 3 weights ( one for bias ) . i am trying to train it the xor function with the following test data : * **0 1** - desired result : **1** * **1 0** - desired result : **1** * **0 0** - desired result : **0** * **1 1** - desired result : **0** after training , the **mean square error** of test ( when looking for a 1 result ) { 0 , 1 } = 0 , which is good i presume . however the mean square error of test ( when looking for a 0 result ) { 1 , 1 } = 0.5 , which surely needs to be zero ? during the train stage i notice the mse of true results drops to zero very quickly , whereas mse of false results lingers around 0.5 . i 'm using back propagation to train the network , with a sigmoid function . the issue is that when i test any combination after the training , i get a ouput result of **1.000014 for true** , and **1.000104 for false** . i 'm trying to get 1 for true , 0 for false . the network seems to learn very fast , even with an extremely small learning rate . if it helps , here is the weights that are produced , with a learning rate of **0.1** : n0-w0 = **-0.999** , n0-w1 = **0.655** , n0-w2 = **0.304** ( **bias weight** ) - hidden layer n1-w0 = **0.674** , n1-w1 = **-0.893** , n1-w2 = **0.516** ( **bias weight** ) - hidden layer n2-w0 = **2.135** , n2-w1 = **2.442** , n3-w2 = **1.543** ( **bias weight** ) - output node back propagation steps : 1 . **feed forward a feature set , summing the weights x input set . calculating sigmoid per node . ** 2 . **apply bias . ** 3 . **calculate output node error , desired output - actual output ( sigmoid ) . ** 4 . **back propagate error , layer above error x connecting weight . per node . ** 5 . **adjust weights , repeat till mse low enough . ** apologies for the long post , but i 've been scratching my header over this for awhile now , and i ca n't determine what is wrong with my back propagation algorithm . thanks in advance . <eoq> can you show me your code ? its unclear what order you 're doing some of the steps in the feed-forward process . and also you need to backpropogate through the whole network . <eoa> 
difficulty training simple xor function . i 've created a neural network , with the following structure : input1 - input2 - **input layer** . n0 - n1 - **hidden layer** . 3 weights per node ( one for bias ) . n2 - **output layer** . 3 weights ( one for bias ) . i am trying to train it the xor function with the following test data : * **0 1** - desired result : **1** * **1 0** - desired result : **1** * **0 0** - desired result : **0** * **1 1** - desired result : **0** after training , the **mean square error** of test ( when looking for a 1 result ) { 0 , 1 } = 0 , which is good i presume . however the mean square error of test ( when looking for a 0 result ) { 1 , 1 } = 0.5 , which surely needs to be zero ? during the train stage i notice the mse of true results drops to zero very quickly , whereas mse of false results lingers around 0.5 . i 'm using back propagation to train the network , with a sigmoid function . the issue is that when i test any combination after the training , i get a ouput result of **1.000014 for true** , and **1.000104 for false** . i 'm trying to get 1 for true , 0 for false . the network seems to learn very fast , even with an extremely small learning rate . if it helps , here is the weights that are produced , with a learning rate of **0.1** : n0-w0 = **-0.999** , n0-w1 = **0.655** , n0-w2 = **0.304** ( **bias weight** ) - hidden layer n1-w0 = **0.674** , n1-w1 = **-0.893** , n1-w2 = **0.516** ( **bias weight** ) - hidden layer n2-w0 = **2.135** , n2-w1 = **2.442** , n3-w2 = **1.543** ( **bias weight** ) - output node back propagation steps : 1 . **feed forward a feature set , summing the weights x input set . calculating sigmoid per node . ** 2 . **apply bias . ** 3 . **calculate output node error , desired output - actual output ( sigmoid ) . ** 4 . **back propagate error , layer above error x connecting weight . per node . ** 5 . **adjust weights , repeat till mse low enough . ** apologies for the long post , but i 've been scratching my header over this for awhile now , and i ca n't determine what is wrong with my back propagation algorithm . thanks in advance . <eoq> the bias is just another input to the neuron . do n't treat it special ( besides always getting an input value = 1 ) . also , does your code work for the or and and cases ? <eoa> 
difficulty training simple xor function . i 've created a neural network , with the following structure : input1 - input2 - **input layer** . n0 - n1 - **hidden layer** . 3 weights per node ( one for bias ) . n2 - **output layer** . 3 weights ( one for bias ) . i am trying to train it the xor function with the following test data : * **0 1** - desired result : **1** * **1 0** - desired result : **1** * **0 0** - desired result : **0** * **1 1** - desired result : **0** after training , the **mean square error** of test ( when looking for a 1 result ) { 0 , 1 } = 0 , which is good i presume . however the mean square error of test ( when looking for a 0 result ) { 1 , 1 } = 0.5 , which surely needs to be zero ? during the train stage i notice the mse of true results drops to zero very quickly , whereas mse of false results lingers around 0.5 . i 'm using back propagation to train the network , with a sigmoid function . the issue is that when i test any combination after the training , i get a ouput result of **1.000014 for true** , and **1.000104 for false** . i 'm trying to get 1 for true , 0 for false . the network seems to learn very fast , even with an extremely small learning rate . if it helps , here is the weights that are produced , with a learning rate of **0.1** : n0-w0 = **-0.999** , n0-w1 = **0.655** , n0-w2 = **0.304** ( **bias weight** ) - hidden layer n1-w0 = **0.674** , n1-w1 = **-0.893** , n1-w2 = **0.516** ( **bias weight** ) - hidden layer n2-w0 = **2.135** , n2-w1 = **2.442** , n3-w2 = **1.543** ( **bias weight** ) - output node back propagation steps : 1 . **feed forward a feature set , summing the weights x input set . calculating sigmoid per node . ** 2 . **apply bias . ** 3 . **calculate output node error , desired output - actual output ( sigmoid ) . ** 4 . **back propagate error , layer above error x connecting weight . per node . ** 5 . **adjust weights , repeat till mse low enough . ** apologies for the long post , but i 've been scratching my header over this for awhile now , and i ca n't determine what is wrong with my back propagation algorithm . thanks in advance . <eoq> i just ran xor on a 2-2-1 nn and got these weights h1 h2 w0 1.4933 7.1091 //bias weight w1 2.2253 -4.3301 //weight to input 1 w2 -0.6427 -5.3821 //weight to input 2 o1 w3 -10.3734 //bias weight w4 -5.2024 //weight to h1 w5 10.0029 //weight to h2 throw these numbers into your code to see if your forward propagation code works . results that i get using sigmoid units are : 0.0097 // result for 0 0 0.9922 // result for 0 1 0.9871 // result for 1 0 0.0136 // result for 1 1 <eoa> 
neural network learning fast , giving me false positives . i 've recently started implementing a feed-forward neural network and i 'm using back-propagation as the learning method . i 've been using http : //galaxy.agh.edu.pl/~vlsi/ai/backp_t_en/backprop.html as a guide . however , after just the first epoch , my **error is 0** . before using the network for my real purpose i 've tried with the simple network structure : * 4 binary inputs , 1 , 1 , 0 , 0 . * 2 hidden layers , 4 neurons each . * 1 output neuron , 1.0 should = valid input . each training epoch runs the test input ( 1 , 1 , 0 , 0 ) , calculates the output error ( sigmoid derivative * ( 1.0 - sigmoid ) ) , back propagates the error and finally adjusts the weights . each neuron 's new weight = **weight + learning_rate * the neuron 's error * the input to the weight . ** each hidden neuron 's error = ** ( sum of all output neuron 's error * connected weight ) * the neuron 's sigmoid derivative . ** the issue is that my **learning rate has to be 0.0001** for me to see any sort of 'progress ' between the epochs in terms of lowering the error . in this case , the error starts around ~30.0 . any greater learning rate and the error results in 0 after the first pass , and thus results in **false positives** . also when i try this network with my real data ( a set of 32 audio features from sample - 32 neurons per hidden layer ) - i get the same issue . to the point where any noise will trigger a false positive . possibly this could be an input feature issue , but as i 'm testing using a high pitch note i can clearly see the raw data differs from a low pitch one . i 'm a neural networks newbie , so i 'm almost positive the issue is with my network . any help would be greatly appreciated . <eoq> i really do n't know enough to say for sure but it sounds like you are just overfitting . nns are very good at that . try using a smaller network or some other generalization method . <eoa> 
neural network learning fast , giving me false positives . i 've recently started implementing a feed-forward neural network and i 'm using back-propagation as the learning method . i 've been using http : //galaxy.agh.edu.pl/~vlsi/ai/backp_t_en/backprop.html as a guide . however , after just the first epoch , my **error is 0** . before using the network for my real purpose i 've tried with the simple network structure : * 4 binary inputs , 1 , 1 , 0 , 0 . * 2 hidden layers , 4 neurons each . * 1 output neuron , 1.0 should = valid input . each training epoch runs the test input ( 1 , 1 , 0 , 0 ) , calculates the output error ( sigmoid derivative * ( 1.0 - sigmoid ) ) , back propagates the error and finally adjusts the weights . each neuron 's new weight = **weight + learning_rate * the neuron 's error * the input to the weight . ** each hidden neuron 's error = ** ( sum of all output neuron 's error * connected weight ) * the neuron 's sigmoid derivative . ** the issue is that my **learning rate has to be 0.0001** for me to see any sort of 'progress ' between the epochs in terms of lowering the error . in this case , the error starts around ~30.0 . any greater learning rate and the error results in 0 after the first pass , and thus results in **false positives** . also when i try this network with my real data ( a set of 32 audio features from sample - 32 neurons per hidden layer ) - i get the same issue . to the point where any noise will trigger a false positive . possibly this could be an input feature issue , but as i 'm testing using a high pitch note i can clearly see the raw data differs from a low pitch one . i 'm a neural networks newbie , so i 'm almost positive the issue is with my network . any help would be greatly appreciated . <eoq> randomize your initial weights . i 've had good results with random numbers between +/- 0.5 <eoa> 
neural network learning fast , giving me false positives . i 've recently started implementing a feed-forward neural network and i 'm using back-propagation as the learning method . i 've been using http : //galaxy.agh.edu.pl/~vlsi/ai/backp_t_en/backprop.html as a guide . however , after just the first epoch , my **error is 0** . before using the network for my real purpose i 've tried with the simple network structure : * 4 binary inputs , 1 , 1 , 0 , 0 . * 2 hidden layers , 4 neurons each . * 1 output neuron , 1.0 should = valid input . each training epoch runs the test input ( 1 , 1 , 0 , 0 ) , calculates the output error ( sigmoid derivative * ( 1.0 - sigmoid ) ) , back propagates the error and finally adjusts the weights . each neuron 's new weight = **weight + learning_rate * the neuron 's error * the input to the weight . ** each hidden neuron 's error = ** ( sum of all output neuron 's error * connected weight ) * the neuron 's sigmoid derivative . ** the issue is that my **learning rate has to be 0.0001** for me to see any sort of 'progress ' between the epochs in terms of lowering the error . in this case , the error starts around ~30.0 . any greater learning rate and the error results in 0 after the first pass , and thus results in **false positives** . also when i try this network with my real data ( a set of 32 audio features from sample - 32 neurons per hidden layer ) - i get the same issue . to the point where any noise will trigger a false positive . possibly this could be an input feature issue , but as i 'm testing using a high pitch note i can clearly see the raw data differs from a low pitch one . i 'm a neural networks newbie , so i 'm almost positive the issue is with my network . any help would be greatly appreciated . <eoq> sounds like it 's overfitting the training data ... have you tried adding a regularization term to your cost function ? <eoa> 
how much data do you need to do ml ? i 'm personally most familiar with split testing on websites . but to really do some ml , what kind of data sets do you need ? <eoq> the non-answer answer is that it depends . the more complicated your model is the more data you need in order to avoid over-fitting . i 've [ read ] ( http : //www.amazon.com/learning-from-data-yaser-abu-mostafa/dp/1600490069/ref=sr_1_1 ? ie=utf8 & qid=1396934536 & sr=8-1 & keywords=learning+from+data ) that a good rule of thumb for a supervised linear model is ten training examples for every parameter of the model . it is my understanding that linear models are generally considered relatively simple models so that number should probably increase for other sorts of models . <eoa> 
8-3-8 neural network does not converge . hello , i 'm trying to recreate the 8-3-8 neural network . i 'm having trouble getting my network to converge . it 'll get close , but there is always one output that wo n't converge . for example : output = 0.9740 0.0000 0.0147 0.0000 0.0000 0.0628 0.0002 0.0002 0.0000 0.9755 0.0041 0.0000 0.0000 0.0681 0.0000 0.0000 0.0141 0.0002 0.9756 0.0000 0.0008 0.0630 0.0000 0.0000 0.0000 0.0001 0.0000 0.9685 0.0000 0.0889 0.0000 0.0000 0.0000 0.0000 0.0018 0.0001 0.9798 0.0563 0.0000 0.0000 0.0568 0.0736 0.0535 0.0906 0.0617 0.3661 0.0768 0.0673 0.0019 0.0000 0.0000 0.0000 0.0000 0.0780 0.9745 0.0000 0.0080 0.0000 0.0000 0.0000 0.0000 0.0648 0.0000 0.9789 i 'm using sigmoid threshold units , and i have one input bias node . i run 100,000 epochs , and i always get one or two outputs that will not converge . [ here is the matlab code ] ( http : //pastebin.com/jalgtnyx ) any advice to troubleshoot a nn would be helpful . thanks , max edit : i found my mistake . every layer needs a bias unit . [ updated matlab code ] ( http : //pastebin.com/z1wxkssp ) 0.9813 0.0148 0.0006 0.0000 0.0101 0.0000 0.0145 0.0000 0.0117 0.9762 0.0029 0.0077 0.0000 0.0000 0.0000 0.0013 0.0024 0.0151 0.9758 0.0000 0.0000 0.0000 0.0142 0.0140 0.0000 0.0168 0.0000 0.9805 0.0103 0.0002 0.0000 0.0110 0.0122 0.0000 0.0000 0.0160 0.9834 0.0083 0.0031 0.0000 0.0000 0.0000 0.0001 0.0012 0.0069 0.9739 0.0191 0.0180 0.0127 0.0000 0.0220 0.0000 0.0039 0.0172 0.9712 0.0000 0.0000 0.0066 0.0165 0.0133 0.0000 0.0159 0.0000 0.9770 <eoq> i answered my own question : ) <eoa> 
how could i beat 2048 with ml ? ( x-post : /r/machinelearning ) x-post from : http : //www.reddit.com/r/machinelearning/comments/20fmym/how_could_i_beat_2048_with_ml/ there is already an ai for playing the game 2048 here . it uses minimax , but i was wondering about the possibility of using machine learning to make an ai . how would i set up a neural network or something similar to play the game ? i am not very experienced with ml , so i 'm looking for advice on how to set up neural network ( nodes per layer ) as well as what algorithms to use for adjusting weights . or maybe neural networks are n't the best approach ? <eoq> i 'm currently working on doing exactly this . did anything come of your project ? <eoa> 
how could i beat 2048 with ml ? ( x-post : /r/machinelearning ) x-post from : http : //www.reddit.com/r/machinelearning/comments/20fmym/how_could_i_beat_2048_with_ml/ there is already an ai for playing the game 2048 here . it uses minimax , but i was wondering about the possibility of using machine learning to make an ai . how would i set up a neural network or something similar to play the game ? i am not very experienced with ml , so i 'm looking for advice on how to set up neural network ( nodes per layer ) as well as what algorithms to use for adjusting weights . or maybe neural networks are n't the best approach ? <eoq> i tried an approach with reinforcement learning using a genetic algorithm to optimize a decision tree forest ( https : //github.com/underflow/reinforcement-2048 ) . i 'm really not conviced of my method , and it does n't do better than 1024 . <eoa> 
is there any way to find the 'real/effective bandwidth ' of an audio file ? i have some previously recorded audio that i 'm using for some ml projects . the audio is currently encoded with a certain compression/codec . but , before that , it may have gone through a number of other codecs/compression algorithms . for instance , maybe gsm for a cell phone call , then ulaw , then g729 . etc . is there a way to calculate the how much useful information is still contained in the audio , as compared to a reference , ( like say pcm16 ) . i do n't have access to the original non-compressed files of course . seems like there should be some information theoretic approach that would work ... . <eoq> i 've never worked with this , but my understanding is that useful information is synonymous with shannon entropy . http : //en.wikipedia.org/wiki/shannon_entropy # definition if i were doing this , i would start with the negative sum over i of p ( xi ) * log_b [ p ( xi ) ] expression . here are a couple of approaches which do this : http : //www.kennethghartman.com/calculate-file-entropy/ http : //www.kennethghartman.com/shannon-entropy-of-file-formats/ and : http : //stackoverflow.com/questions/990477/how-to-calculate-the-entropy-of-a-file <eoa> 
pattern recognition : have method , need name . this seems like it might be the best place to post this . i do n't know any of the pattern recognition lingo in any real depth , but i need to know if there is a name for the algorithm in a program i 've already finished . if there is n't , then i need to know the closest thing to compare it to . i have a simple signal of one variable vs. another . i fit a number of polynomials at various places in the signal to extract some points , and simplify the signal by some fundamental simple shapes which pass through these points . this seems to be something like a hough transform . i then calculate a few hundred attributes , based on these shapes for the set of all signals . for instance length of a side or angle between two sides and so on . i have categorized by eye a subset of these signals into two categories , good or bad . i use this training set to find upper and lower limits acceptable for these attributes , and apply a boolean test for all the attributes of all the signals using the test set limits . in other words finding the box of dimension ( # of attributes ) that the training set lives in and reporting which events of the total set are within it . this defines the output set of good signals for which there are no false negatives , and very very few false positives ( with these signals anyhow ) . this seems to be something like linear discriminant analysis with hard limits instead of dealing with probabilities . i then do various operations to find the smallest set of these cuts which return the exact same output . this seems to be like feature selection . what do you think ? is there possibly a name for this exact thing overall ? have i gotten close with the sub-method names ? <eoq> your description is a bit too vague . you say that > i have a simple signal of one variable vs. another . which makes it sound like you have 2 variables in total ( i.e , you could plot all of your dataset in a 2d plane ) but the rest of your text absolutely does n't sound like that . i have n't got the foggiest what you mean by `` fitting polynomials at various places in the signal '' , so i 'm just going to ignore that part and assume that you are dealing with a set of geometric shapes , because that 's what it sounds like . then , you obviously totally confuse training and testset ( maybe just a typo ) , because apparently you find some thresholds for some attributes on a trianingset , and then apply a boolean test `` using the *test set* limits '' ? ? ? > this defines the output set of good signals for which there are no false negatives , and very very few false positives ( with these signals anyhow ) . i do n't know why you think there are no false negatives in that set > this seems to be something like linear discriminant analysis with hard limits instead of dealing with probabilities . absolutely not . first of , lda is n't dealing with probabilities either , but more importantly : what you 're doing sounds nothing like lda . from what i 've understood of your method , it is probably most similar to a decision tree . <eoa> 
pros and cons of user-based vs item-based collaborative filtering so let 's say you have a bunch of nondescript users and a bunch of nondescript items . you want to perform collaborative filtering in order to recommend user a a new item . no metadata is used in any case . i 'm trying to better understand when you 'd do user-based vs item-based collaborative filtering . user-based collaborative filtering makes sense to me - given a user , you match him/her up with all other users and come up with a similarity-weighted average rating of all songs , at which point you can find the most highly-rated thing that the user has n't seen yet . what are the pros and cons of using one vs the other ? if you construct the matrix of ratings , with let 's say users as columns and items as rows , both end up being extremely similar - in user-based cf , you look at correlations between user columns ( using cosine similarity or something similar ) , and in item-based cf , you look at correlations between item rows ( using the same thing ) . what applications make sense for using one vs the other ? <eoq> it seems to me that user-based or item-based are just inverses of each other . in terms of the approach that makes the most sense to me -- clustering -- you would have two possibilities . given *m* users and *n* items : * users are a point in *n*-dimensional item space , where each of the *n* dimensions is that user 's rating for the *n*th item * items are a point in *m*-dimensional user space , where each of the *m* dimensions is that item 's rating given by the *m*th user the former lets you find users who are similar to each other based upon their item ratings , allowing you to make predictions for the unrated items by comparing this user to their most similar users and how they rated that item , and making the assumption that similar users will continue to make similar ratings . i.e . `` here are the users most like you '' , or going one-level deeper , `` users most similar to you prefer these items '' , or for the nsa : `` person a is a terrorist ; he is most similar to these people in his buying or rating patterns , so check them out as persons of interest '' . the latter lets you easily find items who are similar to each other based upon their ratings by users , allowing you to make predictions for the users who have n't rated that item by comparing this item to the most similar items and how the user rated that item ( which is just the inverse of the previous paragraph ) , and making the assumption that similar items would be ranked similarly by similar users . i.e . `` here are the items most similar to this item '' , or going one-level deeper , `` items similar to this were bought by these users '' , or for narcotics police : `` item a is discovered to be a mind-altering substance ; these items are most similar based on user buying patterns , so investigate them as potential mind-altering substances as well '' , or perhaps `` item a is a mind-altering substance ; items similar to this were bought by these users , so check them out as potential drug users '' . **it 's just two sides of the same coin . ** edit : removed amazon examples -- they were n't working out for me . : -p <eoa> 
are markov-chain models of language used for anything productive ? i 've encountered markov models of language before , and they are very entertaining because they can be used to generate sequences of realistic sounding nonsense , but do they have any other uses ? are these models ever employed in machine translation or speech recognition , or have they been superseded by something better ? also , i recall there being some kind of markov chain tool in ubuntu but i ca n't remember the name . if you know it , please share : ) <eoq> yes , those language generation tools essentially sample from an [ n-gram markov model ] ( http : //en.wikipedia.org/wiki/n-gram ) . <eoa> 
are markov-chain models of language used for anything productive ? i 've encountered markov models of language before , and they are very entertaining because they can be used to generate sequences of realistic sounding nonsense , but do they have any other uses ? are these models ever employed in machine translation or speech recognition , or have they been superseded by something better ? also , i recall there being some kind of markov chain tool in ubuntu but i ca n't remember the name . if you know it , please share : ) <eoq> they are used in word prediction . for example , autocorrect on mobile keyboards . there was a really cool keyboard app i saw awhile ago that could fit in a very small space and relied on predicting what you meant to type . google search uses them to make suggestions and detect misspellings . i *think* they might use something like them in google translate but i 'm not certain . i do n't know if they are used in speech recognition but it 's certainly a good application . e.g . is it more likely to user said `` i recognize *beach* '' or `` i recognize *speech* . '' it 's also theoretically possible to use them to get good text compression , but i do n't know if anyone actually does that . <eoa> 
svr advice wanted . i am trying my hand at a [ hackerrank ] ( https : //www.hackerrank.com/challenges/predicting-house-prices `` challenge link '' ) challenge . after determining after a couple of days that writing my own linear regression in c was going to be rife with trouble i decided i would try to use an svr from libsvm . after playing with svm toy for a couple of minutes i felt like i had a few configurations worth giving a shot . but when i try them on the test data the resulting plane seems to be fairly horizontal and near the average of all the training values . for more specifics i am using the libsvm nu-svr and experimenting with both the various kernels and tweaking gamma . pointers and links to explanations to help me better understand what i 'm missing would be most appreciated . thanks . <eoq> what parameters for the nu-svr are you using ? its possible you 're regularizing too much . have you gotten r^2 or mean absolute errors of your cross-validation outputs ? <eoa> 
svr advice wanted . i am trying my hand at a [ hackerrank ] ( https : //www.hackerrank.com/challenges/predicting-house-prices `` challenge link '' ) challenge . after determining after a couple of days that writing my own linear regression in c was going to be rife with trouble i decided i would try to use an svr from libsvm . after playing with svm toy for a couple of minutes i felt like i had a few configurations worth giving a shot . but when i try them on the test data the resulting plane seems to be fairly horizontal and near the average of all the training values . for more specifics i am using the libsvm nu-svr and experimenting with both the various kernels and tweaking gamma . pointers and links to explanations to help me better understand what i 'm missing would be most appreciated . thanks . <eoq> just replying to let any passers by know that it was a piece of cake to solve this problem with scikit-learn . the hardest part was using python with nearly no previous experience . if scikit-learn continues to be this straightforward i look forward to being very grateful for the recommendation . for now consider this problem solved . <eoa> 
why is machine learning important and how is it applied in business ? background : i am an business analyst . my analysis to this point has been mainly restricted to excel and access , with some use of business objects and sql . in my personal life , i am learning more about computer science and practicing python . my degree was in anthropology and poli sci . complete ml noob . <eoq> it becomes more difficult to see trends , and to make intelligent decisions from data , when the amount of data increases beyond what a person or team of people can handle . in order to make the most of a large amount of data , it is necessary to automate the processes we use to analyze , and make decisions from it . <eoa> 
why is machine learning important and how is it applied in business ? background : i am an business analyst . my analysis to this point has been mainly restricted to excel and access , with some use of business objects and sql . in my personal life , i am learning more about computer science and practicing python . my degree was in anthropology and poli sci . complete ml noob . <eoq> i 'm not sure what sector of business you 're in , but i know finance uses a lot of machine learning . consider the idea of stock prediction . say you want to predict the stock price at the end of the day . what types of information may be useful ? the price at the previous day would be useful , the price the day before would be as well . stocks in similar industries may also be useful . you can then use these pieces of information together to build a predictive model of what the stock price at the end of the day will be . in general more information is used as well and i can get you some links for that if you would be interested . <eoa> 
