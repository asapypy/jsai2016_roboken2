how do i scale <unk> ( nb ) features ? hello /r/mlquestions ! thanks for your time , i 'll be <unk> . i have a labeled dataset <unk> x <unk> about half are discrete [ <unk> counts ] ( http : <unk> ? <unk> ) , approximately negative <unk> or <unk> poisson according to <unk> literature . these counts <unk> over several orders of magnitude and are not <unk> . i am hoping to use svm or nn for cross validation . 1. do i need to scale these data ? it is frequently suggested ( ng , <unk> , etc . ) that features should be scaled to the range <unk> or 0:1 for svms . does this apply for nn as well ? 2. if scaling is required , what would you suggest for these count variables ? is it appropriate to use the <unk> <unk> distribution function ( <unk> ) to scale the values to 0:1 ? <eoq> 1. yes ( scale for nn ) . for svms , ( especially if using a kernel ) - you 'll want to scale them as well . ( many methods expect the features to have been normalized in fact ) . easy way to do this is scikit learn - standard <unk> . 2. just normalize and/or scale to values to between 0-1 i think.. <eoa> 
help needed for <unk> of amazon.com reviews x-post from http : <unk> ( please post answers there if you can ) hi , a few <unk> and i have an idea : <unk> the reviews of amazon.com products to show <unk> pros and cons of the product . this <unk> out of the time i waste reading reviews before buying a product . we 're starting with nlp , and need to know where to start looking to search for answers to ( <unk> of ) this problem . we 've read `` jointly learning to extract and <unk> by <unk> <unk> , dan <unk> and dan <unk> '' , and have a dataset : https : <unk> thanks : ) p.s . : this is my first post on reddit . <eoq> found some useful suggestions on : http : <unk> could still use your opinion ! : ) <eoa> 
row vs column vectors is there a convention as to whether features are represented as row or column vectors ? or is it a free for all on that front ? <eoq> from what i 've seen ( using sklearn , matlab , <unk> ... ) features are always columns ( each sample is row in matrix ) . however , it is only convention as far as i know . <eoa> 
what does the ℝ symbol mean when constructing neural networks ? i 'm trying to read up on some papers about neural networks , and without a background in math or computer science , some terms and syntax escape me . several papers i have come across express that `` we associate a column vector in ℝ^d '' or similar ( where d is a dimensionality ) . but the only meaning of ℝ that i know of is the conventional one . does it mean that every vector element is a real number , or does it mean something else ? <eoq> yes , your understanding is correct <eoa> 
what does the ℝ symbol mean when constructing neural networks ? i 'm trying to read up on some papers about neural networks , and without a background in math or computer science , some terms and syntax escape me . several papers i have come across express that `` we associate a column vector in ℝ^d '' or similar ( where d is a dimensionality ) . but the only meaning of ℝ that i know of is the conventional one . does it mean that every vector element is a real number , or does it mean something else ? <eoq> r usually <unk> real numbers so you are correct . <eoa> 
number of parameters in multi class and two class logistic regression if you have f number of features , then in 2 class lr you have f parameters ( <unk> the bias ) . however in multiclass lr with k classes you have <unk> parameters , correct ? which means that using multiclass ( softmax ) lr for a 2 class problem would have double the parameters of using 2 class ( sigmoid ) lr . what is <unk> me is why is it not f* ( k-1 ) parameters for multiclass lr ? <eoq> actually it is f* ( k-1 ) [ wiki ] ( http : <unk> ) <eoa> 
predicting when a resource will run out ? i am trying to figure out what approach would be best for predicting when a resource will run out or be low and needs to be <unk> ? for example , let 's say i have a salt shaker at a <unk> and am able to tell the amount of salt left in the shaker ( by weight , <unk> , count , or whatever means is best ) . how could i use this data shown over time to predict when it will be empty and will need to be filled back up ? i need to be able to see that the shaker may get used more on weekends , or maybe during <unk> months , so taking time into account is pretty important . i have been thinking about using a neural network with <unk> ( time ) or <unk> ( time ) as a time input variable , but i feel <unk> as in my machine learning class two <unk> ago we covered <unk> problems more than these kinds of problems . <eoq> why would you need a neural network for this ? this can be easily modeled with a linear equation based on the number of customer , then a poisson or gamma distribution can be used to represent the use of the salt shaker . <eoa> 
predicting customer <unk> behavior i work at a <unk> site and i have a ton of data at my <unk> . we want to be able to predict if a user will purchase or not , and for users that are likely to not purchase , offer them a free trial . i am pretty well-versed in python ( pandas , <unk> , etc . ) and working with large datasets , but not so much with ml . where do i start ? <eoq> perhaps with [ coursera ml course ( link to reddit ml thread ) ] ( http : <unk> ) <eoa> 
random forests i 'm a machine learning newbie . to learn something in ml , i have implemented a random forest ( scikit ) based classifier for activity prediction based on accelerometer data . i.e the classifier predicts states like standing , running , etc based on <unk> accelerometer output . the rf classifier performs reasonably well in predicting state , however , i would like to add something akin to a feedback loop to it . what i mean is that , if the classifier <unk> and i hand correct it . for the next run , on same dataset , there should n't be a <unk> . i 'm using scikit . any pointers ? <eoq> with rf you should simply include that sample with correct ( by hand ) prediction in your training set and then <unk> your classifier . alternative is to use some [ online learning ] ( http : <unk> ) <unk> . <eoa> 
why does ufldl consider -1 to be an `` inactive '' tanh hidden neuron output ? from the [ stanford ufldl tutorial ] ( http : <unk> ) : > <unk> , we will think of a neuron as being <unk> ( or as <unk> ) if its output value is close to 1 , or as being <unk> if its output value is close to 0. we would like to <unk> the neurons to be inactive most of the time . this discussion <unk> a sigmoid activation function . if you are using a tanh activation function , then we think of a neuron as being inactive when it outputs values close to -1 . tanh is <unk> about the <unk> . if you flip the <unk> of the <unk> weights then an `` inactive '' neuron is effectively active again . this piece of advice s even repeated in the older version of their tutorial . is it a mistake or is there a good reason that an output of -1 is somehow less `` active '' than 1 ? <eoq> sigmoids were <unk> <unk> as continuous versions of binary threshold neurons . then people started using tanh because it 's faster than sigmoids and has the same shape . it 's just a sigmoid with a bias of <unk> , multiplied by 2 , basically . so essentially they are still thinking of tanh 's as approximations of binary threshold neurons . however it 's not even true for binary neurons . nns do n't care whether a neuron is `` active '' or `` inactive '' . they can just multiply by a negative weight and adjust the bias , and a zero becomes 1 and a one becomes 0 . <eoa> 
[ baum-welch algorithm for hidden markov models ] i was going over the baum-welch algorithm for updating and ran into a <unk> question about the new values for the symbol distribution -- basically that , unless every observation contains every symbol in the model , it looks like the algorithm will update the missing symbol probabilities to zero for all states . i 'm probably just missing something incredibly simple here , but i 'd really appreciate it if someone would point it out to me . for the sake of a common notation , [ here 's ] ( http : <unk> % <unk> % <unk> % <unk> ) the wiki . any updated probability of symbol o in state i [ b i ( o ) ] can be <unk> as the sum of a <unk> subset of the <unk> for that observation and state , so the sum overall the updated values of b i ( o ) will always be equal to one . but unless every symbol in b is in every observation the model updates on , at least one of the symbols wo n't have a gamma that corresponds to it for any state , so it 'll get updated to zero across all states ( the <unk> for <unk> i ( k ) is zero ) . assuming that the model updates on a sequence like that ( say 'a , b ' , where each state has a distribution over 'a ' , 'b ' , and <unk> ' ) , then it ca n't update on a new sequence like 'b , c ' with the <unk> symbol in it . it 'll also estimate the probability of such a sequence as zero . after updating on 'a , b ' , the probability of seeing c in any state is zero , which makes the the <unk> estimates corresponding to it also zero for all states . the <unk> for gamma is the sum over states i , of [ alpha i ( t ) * <unk> i ( t ) ] . all of those are zero , so the <unk> is zero . just estimating the probability of 'b , c ' runs into a similar problem -- the probability of seeing c is zero ; which takes the probability of an observation with c in it to zero . <unk> do n't just work on <unk> of their symbol sets , so i 'm clearly missing something . could someone please explain what ? <eoq> you should have non-zero initial emission probabilities for all symbols . this way , within each observation , the probability of the hidden states for that observation are used <unk> the probability of <unk> that sequence in the first place . when your pseudo probabilities are later normalized , the <unk> symbols and states will remain non-zero . <eoa> 
noob question , how to deal with unequal amount of data for each class , using deep networks hi i am trying to do classification using a cnn , but the amount of training data i have varies for each class . what is the correct method in order to deal with this ? <eoq> i do n't think it matters . it could possibly learn <unk> prior probabilities , and possibly affect performance on cases where it is very <unk> . try weighing the cases of the classes you want more of , more . i.e . <unk> multiple copies of them in the training set or multiplying the error so those cases matter more . <eoa> 
noob question , how to deal with unequal amount of data for each class , using deep networks hi i am trying to do classification using a cnn , but the amount of training data i have varies for each class . what is the correct method in order to deal with this ? <eoq> there is no one easy way to do this and it 's in no way specific to cnns . first , are you classes imbalanced <unk> in your training data with respect to the underlying population ? or is the underlying distribution imbalanced in the same way ? having a handle on the class distribution is very helpful here . if you google for things like `` class imbalance '' or `` <unk> shift '' you may get some ideas . i 've tested this briefly : http : <unk> but it did n't appear to have much effect . <eoa> 
dropping out for convolutional rbms and convolutional autoencoders if i implement dropout in convolutional rbms or convolutional autoencoders , do i dropout entire <unk> ( i.e . consider say only 30 out of 60 kernels for a <unk> update ) , or do i dropout individual hidden units from the kernel activations . ( i.e . i use all 60 kernels , but within each kernel 's own hidden layer i dropout half of units ) . <eoq> the latter is the more common approach . but , experiment and see what works better for your problem . <eoa> 
how to get data from a site ? i do n't know if this is the right place to post this , so please correct me if i am wrong . id like to make a program that can search for recipes by the name of food item . there is a big repository of recipes at http : //www.recipesource.com/ i have never worked with getting data from the net , but from what i understand people generally use api provided by the sites ? so how can i search for recipes from this site automatically , do i need them to provide some `` usable api '' , or can is there some all-purpose program people use to automate such tasks ? <eoq> when there is no api i normally use c # <unk> # <unk> with their standard http client libs for downloading and some regex to extract urls . you might have to play with some http <unk> / <unk> to get sites to talk to you . <eoa> 
how to get data from a site ? i do n't know if this is the right place to post this , so please correct me if i am wrong . id like to make a program that can search for recipes by the name of food item . there is a big repository of recipes at http : //www.recipesource.com/ i have never worked with getting data from the net , but from what i understand people generally use api provided by the sites ? so how can i search for recipes from this site automatically , do i need them to provide some `` usable api '' , or can is there some all-purpose program people use to automate such tasks ? <eoq> most likely too late for you , but it could be useful to others : udacity <unk> a <unk> course on mongodb . lesson 2 is on screen scraping , with example code in python + <unk> library . there is no mongodb in this lesson . <eoa> 
dealing with imbalanced data set i 'm working on a course project to classify tweets as either `` interesting '' or `` not interesting '' . i have hand labeled about 2000 tweets , and ended up with a ratio of about 1:10 ( interesting : not interesting ) . i downsampled the `` not interesting '' tweets and got 200/200 distribution . i use k-fold cross validation on this set and get very good performance with a naive bayes classifier . however , i feel it is a terrible waste to discard 1500 of the not-interesting tweets . so the question : could i still train my classifier using the imbalanced data set , as long as my validation set is balanced ? or am i overlooking some issues with this ? <eoq> there 's a lot of literature on imbalanced data sets . <eoa> 
dealing with imbalanced data set i 'm working on a course project to classify tweets as either `` interesting '' or `` not interesting '' . i have hand labeled about 2000 tweets , and ended up with a ratio of about 1:10 ( interesting : not interesting ) . i downsampled the `` not interesting '' tweets and got 200/200 distribution . i use k-fold cross validation on this set and get very good performance with a naive bayes classifier . however , i feel it is a terrible waste to discard 1500 of the not-interesting tweets . so the question : could i still train my classifier using the imbalanced data set , as long as my validation set is balanced ? or am i overlooking some issues with this ? <eoq> if you know the operating context ( i.e . the class imbalance when you are <unk> the model ) then you can use roc analysis to select an optimal classifier on the <unk> <unk> . the <unk> for accuracy will be a line with a 45 degree angle for balanced classes and will change for <unk> classes . ( <unk> or <unk> ) . the auc gives a good measure if you do not know the operating context the model would be <unk> in , but bare in mind some areas under the curve might not be realistic . in particular if you are doing an information <unk> type task ( sounds like it ) then you will be more interested in having a <unk> <unk> on the left hand side of the roc curve than the auc score . this is a good paper : http : <unk> <eoa> 
how can you predict the next <unk> of video ? lets say i have a very large series of sequential images . the images are all <unk> similar , and sequential images are particularly similar . there exists some <unk> rule that <unk> each image from the previous few images . i want to create a program that can approximate that rule . i want to feed my program the last few sequential images , and have it guess the next . how would i go about this ? as a novice i have been reading around trying to <unk> things out for myself , but i feel i understand less the more i read . <eoq> [ this talk ] ( http : <unk> ? <unk> ) has a <unk> of deep gaussian processes doing exactly this . it 's a very hard task though . <eoa> 
recommended way to convert a set of probability distributions into a feature vector ? i splitted a number of short texts into its sentences and ran stanford 's corenlp sentiment analysis on each one . this discards the words and works with the pos trees only . as a result , for each text now i have a number of probability distributions of being in one out of five possible sentiment classes ( from very negative to very positive ) . now i want to define the feature vector for the texts . my first approach was to add five elements to the feature vector for each unique sentence tree in the output of the sentiment classifier , then just populate it with the values of the probability distributions . does this make sense ? would n't i be somehow losing the representativity of the sentiment magnitude ? thanks ! <eoq> `` i splitted a number of short texts into its sentences and ran stanford 's corenlp sentiment analysis on each one . this discards the words and works with the pos trees only . as a result , for each text now i have a number of probability distributions of being in one out of five possible sentiment classes ( from very negative to very positive ) . now i want to define the feature vector for the texts . '' in addition to what you suggest , it might make sense to also include the mean as a feature . `` my first approach was to add five elements to the feature vector for each unique sentence tree in the output of the sentiment classifier , then just populate it with the values of the probability distributions . does this make sense ? would n't i be somehow losing the representativity of the sentiment magnitude ? '' do you mean representing the probabilities as a vector ( i.e . [ 0.1 , 0.3 , 0.2 , 0.2 , 0.2 ] ) fails to take into account the fact that the <unk> are ordered - that 1 and 2 are more similar than 1 and 5 ? while this is a valid <unk> , i do n't think that it will be a big deal if you have enough data . it is common to put numerical features into buckets when using linear models . this has the same issue but it works well in practice . <eoa> 
