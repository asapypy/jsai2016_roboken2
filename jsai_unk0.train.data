can this problem be solved using ml ? i do descriptive analytics and reporting at a company that sells a wide range of products . we record sales transactions , and everytime a item is sold , the following is recorded : * customer id ( each customer has a unique id ) * product id ( each product has a unique id ) * sale date ( other fields are recorded too - location of purchase , quantity , payment type , etc . ) we sell a few big ticket items , and what i 'm wondering is if it 's possible to predict whether a customer will buy one of the big ticket items based on their purchase history , using transactional data as described above . we have about 2 million rows of sales data spanning seven years , and in that time maybe 14,000 big ticket items have been sold to 5,000 out of 50,000 customers . is this a problem that ml applies to ? i realize ml is n't something you learn overnight but i would love to know more about how something like this can be tackled . <eoq> you might want to look into affinity analysis : https : //en.wikipedia.org/wiki/affinity_analysis you might also look at `` recommender systems '' . as an example , if most people who buy diapers also buy sippy-cups , you might see someone buying diapers who has n't yet bought a sippy-cup . in that case , they might be more likely to buy a sippy-cup , especially if you recommend it to them . something like : http : //link.springer.com/article/10.1007/s11334-016-0274-x <eoa> 
started to implement svm from scratch just to see if i can ... turns out it does not go so well , could anyone care to take a look ? i 'm following cs231n.github.io and decided to implement support vector machines from scratch in python . turns out , it does not work so well - i made up sets that are clearly separable , yet my algorithm has a lot of trouble finding accurate line . am i doing something wrong or my failure was imminent since algorithm itself requires much more powerful math to even work correctly ? i uploaded source code to github , please forgive polish variable names here and there . https : //github.com/czlowiekrakieta/machine-learning-from-scratch/blob/master/svm.py <eoq> i just glanced at it and noticed you were not explicitly plotting the support vectors . that should be the first thing you try . <eoa> 
started to implement svm from scratch just to see if i can ... turns out it does not go so well , could anyone care to take a look ? i 'm following cs231n.github.io and decided to implement support vector machines from scratch in python . turns out , it does not work so well - i made up sets that are clearly separable , yet my algorithm has a lot of trouble finding accurate line . am i doing something wrong or my failure was imminent since algorithm itself requires much more powerful math to even work correctly ? i uploaded source code to github , please forgive polish variable names here and there . https : //github.com/czlowiekrakieta/machine-learning-from-scratch/blob/master/svm.py <eoq> make sure the objective you are minimizing decreases . <eoa> 
what is the most popular file type for datasets ? arff ? why they do n't use database ? as my question , i doubt that what is popular file type for datasets ? arff ? <eoq> i 've been wondering this , too ? anyone know the fastest file format for reading from disk into memory ? <eoa> 
what is the most popular file type for datasets ? arff ? why they do n't use database ? as my question , i doubt that what is popular file type for datasets ? arff ? <eoq> should i ask this on machine learning subreddit ? <eoa> 
transition from econometrics to machine learning ? hello everyone i 'm currently an economics major and would like to know if there is a way to transition from economics , particularly econometrics to a carreer in machine learning . i 've found out that i do n't like economic theory and want to look for an alternative without changing majors . the school im in has fame of being very good in econometrics , compared to the other schools in my country , but i have n't reached those classes yet so i do n't really know what are they about . thank you for your answers in advance ( and sorry for bad english ) <eoq> machine learning is a mix of math and programming , so if you have n't started already , i would recommend starting to learn r. it 's a statistical language and an easier transition from other math languages like matlab . we teach it at our [ bootcamp ] ( http : //datasciencedojo.com ) because it is fast to learn . this book is also recommended : [ r for everyone ] ( http : //www.amazon.com/everyone-advanced-analytics-graphics-addison-wesley/ ) <eoa> 
how is weight initialization done today ? i 've just read [ understanding the difficulty of training deep feedforward neural networks ] ( http : //jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf ) . it seems to me that no regularization is used . today , now that we have [ batch normalization ] ( http : //arxiv.org/abs/1502.03167 ) , [ dropout ] ( https : //www.cs.toronto.edu/~hinton/absps/jmlrdropout.pdf ) and [ deep networks with stochastic depth ] ( https : //arxiv.org/abs/1603.09382 ) as well as typically relu activation functions , i wonder how weights are initialized . was there a follow-up paper which checked the importance of initialization / how much of a difference the initialization still makes ? what are common ways to initialize weights in 2016 ? for example , is rbm initialization still done ? ( why / why not ? ) <eoq> yes , there was indeed a follow up paper tackling initialization when using relu non-linearities : [ delving deep into rectifiers : surpassing human-level performance on imagenet classification ] ( https : //arxiv.org/abs/1502.01852 ) [ he ] . <eoa> 
how is weight initialization done today ? i 've just read [ understanding the difficulty of training deep feedforward neural networks ] ( http : //jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf ) . it seems to me that no regularization is used . today , now that we have [ batch normalization ] ( http : //arxiv.org/abs/1502.03167 ) , [ dropout ] ( https : //www.cs.toronto.edu/~hinton/absps/jmlrdropout.pdf ) and [ deep networks with stochastic depth ] ( https : //arxiv.org/abs/1603.09382 ) as well as typically relu activation functions , i wonder how weights are initialized . was there a follow-up paper which checked the importance of initialization / how much of a difference the initialization still makes ? what are common ways to initialize weights in 2016 ? for example , is rbm initialization still done ? ( why / why not ? ) <eoq> ( more questions to this paper are in [ my summary ] ( http : //www.shortscience.org/paper ? bibtexkey=journals/jmlr/glorotb10 # martinthoma ) ) <eoa> 
basic python machine learning - why use vstack and hstack ? i am going through a book called python machine learning . i am working through how to use scikit and there is an example in the book where we write a method called plot_decision_regions . i am having a hard time understanding the reasoning behind some of the code and would love some help . here is my notebook from github : https : //github.com/gohuygo/machine_learning/blob/master/iris/scikit-learn.ipynb there are two lines that i am particularly interested in : > x_combined_std = np.vstack ( ( x_train_std , x_test_std ) ) and > y_combined = np.hstack ( ( y_train , y_test ) ) why do we want a vertically stacked array in one case and a horizontally stacked array in another case ? any guidance on supplementary material is also appreciated . <eoq> x is input , usually a matrix of shape ( batch_size , n_features ) . x_train_std and x_test_std are two batches of data , so you want to combine them to be a shape of ( batch_size*2 , n_features ) . y is target/label , usually a vector of shape ( batch_size , ) . you want to combine y_train and y_test to a shape of ( batch_size *2 , ) <eoa> 
vector space models and support vector machines i 'm applying vector space modelling and support vector machines to my thesis ( specifically to see how frequently certain terms appear in documents within a particular corpus ) . i was wondering whether i could ask some specific questions about the relationship between vsm and svm . following salton , wong and yang 's paper , i 'm guessing that first each of the terms within the document are plotted as vectors using the tf-idf ( so that terms are weighted higher if they appear more frequently in a document and weighted lower if they appear less frequently in the entire collection ) . svm could then be applied to each of these vectors in order to categorise them depending on the terms which appear most frequently in each . is this the correct way of understanding the relationship between the two ? thank you . <eoq> the vector space model is simply a way to represent your document , while a support vector machine is a classification algorithm that is going to work with that representation . they operate on different ends of the machine learning process , and you could swap any of them with an other method . if it can help , imagine that you are working with documents which only have 2 words ( for visualisation purposes ) . that means that each document can be plotted as a point on a plane ( that 's your vector space ) . your tf-idf weighting is going to impact where you are going to put those points on the plane . your svm is going to try and draw a line ( the separating hyperplane ) to separate ( as well as possible ) the points in the plane . <eoa> 
how to use create simple linear regression model with sklearn.dataset using pymc3 ? <eoq> `` how to create simple linear regression model with sklearn.dataset using pymc3 ? '' is what i meant to say lol . <eoa> 
how to use create simple linear regression model with sklearn.dataset using pymc3 ? <eoq> i really want to learn how to create predictive bayesian models so i can move away from frequentism and broaden my understanding of statistics . i am having trouble taking the theoretical concepts and applying them real data ( i.e . bayesian linear model w/ supervised data ) . <eoa> 
thousands of humans , dozens of decisions each , best clustering method to use for recommending increasing certain decisions i have thousands of records ( of humans ) that each have dozens of percentage attributes ( of their decisions ) where for each human all their decision percentages add up to 1 . the assumption is that these records should be able to be clustered based on their percentages , and then i should be able to get a new record that matches closely to one of these clusters and i would provide recommendations on how to better *conform* to one or more of the clusters by increasing the percentages of one or more ( but hopefully not too many ) decisions . also i 'd prefer it did n't recommend decreasing a decision percentage , because the only way that is possible is by increasing many other percentages . i 'm currently learning methods and scikit with the udacity course but have n't gotten to the section on clustering yet , and i need someone with a high level understanding to make sure i 'm going down the right path . i tried to abstract away the actual problem , and hope i 've done a good job , but i would be happy to explain that if curious . <eoq> sounds like you are on the right path . to get nearer the nearest cluster you may need to increase / decrease many percentages like you said - this depends on your data and clusters and clustering method . possibly you could only suggest 3 percentages to change in your 'frontend ' of the application , and then you only suggest the most important changes . finally , you will have to think about how many clusters you want , and you will need to somehow gain some knowledge what these clusters are about . you will need to somehow verify a bit that your clusters you find using some algorithm ( for example kmeans ) are meaningful . good luck ! <eoa> 
why does lstm need 4 inputs per node , compared to sigmoid nodes which only use 1 var a sum ? could something simpler get the job done ? https : //en.wikipedia.org/wiki/long_short-term_memory defines a node as ... sumstate < - ina*inb + sumstate*remember out = sumstate*outmult ina , inb , outmult , and remember are ordinary sigmoid neural nodes whose input is a weighted sum of many other nodes . the purpose is for a network to self modify its patterns of how long to remember things . but couldnt it be done with something simpler like ... out < - out* ( 1-decay ) + decay*in in , out , and decay are all fractions . in and decay are normal sigmoid nodes , and out is derived by that equation and can be used the same way . for example , any existing neuralnet could have a decay var added so it changes its state slower as chosen by some other node . training any of these is a harder problem than defining a workable model of data flow of a node type . how are rnns normally trained ? how are lstm normally trained ? how could a rnn with decay var per node be trained ? <eoq> hey . so the lstm can remember , forget and output a custom value ( that it memorized previously ) . all these operations are dynamic ( decided based on the inputs , with trainable weights ) . think of it as a programmable memory unit . your net ( if i understand it correctly ) would output just a weighted average ( or unnormalized sum ) of the previous output and current input transformed ( you forgot to wrap input with a weight matrix , bias and nonlinearity ) . what would that give ? well , it would be quite similar to deep residual networks conceptually . they work well , but check out their detailed architecture in the original paper . they do not use a trainable decay parameter though , just sum the output . how to train both an lstm or your rnn ? you 'd use backpropagation through time - you unroll your rnn in time by k timesteps , giving a deep feedforward net , and then apply backprop . if you 're unfamiliar with this , i would suggest trying to work with an unrolled rnn treated as a feed-forward net . just assume you have a constant number of inputs and zero-pad your sequences if they 're shorter . happy experimenting ! edit : deep residual networks http : //arxiv.org/abs/1512.03385 ( code on github ) <eoa> 
spectral networks and deep locally connected networks on graphs has anybody read this paper.i have been reading it and with all honesty i am pretty lost .if anybody has read this paper please can you please let me know how the network construction is taking place . <eoq> [ article ] ( http : //arxiv.org/abs/1312.6203v2 ) i have n't read this but it looks really hard ... what 's your background with ml ? <eoa> 
working with credit scoring data i want to find a potentially better classifier than what i have right now so i 'm working with this data set in r http : //www.statistik.lmu.de/service/datenarchiv/kredit/kreditvar_e.html i want to predict credit ( approved or unapproved ) using the other variables . most of them are categorical or binary , only about 3 are quantitative . anyways , i 've built quite a few models for my class project and the best one that i made used 800 training points and 200 test points and achieved about 15 % error rate , it was a decision tree that incorporated all the variables . most of the other models were in the 20 % error rate . i wan na know if you guys have any suggestions for lowering this as much as possible ? is it possible that there is a limit as to what can be extracted from the data ? things i 've tried : all the variables , best & mixed subset selection applied to - > logistic regression , knn , lda , qda , lasso methods , svm and decision trees , simple , pruned & random forest . i also tried a neural network in r but the results were not as good as my simple decision tree . what model do you think will work best for this type of data ? <eoq> be sure to try some featuremappings , such as pca , before trying a classifier , that might improve results . <eoa> 
experience with amazon aws ? just wondering if anyone here has any experience working with amazon aws ? looking through the website it felt like it was written more to convince management , and less geared to someone who would actually be `` doing '' it . anyone have any thoughts/experience on how it is ? its multiclass , binary and regression description seems a little sparse to call it ml in my ( unexperienced ) opinion <eoq> set up a spark cluster w/aws ! <eoa> 
experience with amazon aws ? just wondering if anyone here has any experience working with amazon aws ? looking through the website it felt like it was written more to convince management , and less geared to someone who would actually be `` doing '' it . anyone have any thoughts/experience on how it is ? its multiclass , binary and regression description seems a little sparse to call it ml in my ( unexperienced ) opinion <eoq> i use torch on their gpus . once you get it setup it 's really nice . there are other services that offer much faster gpu clusters though <eoa> 
experience with amazon aws ? just wondering if anyone here has any experience working with amazon aws ? looking through the website it felt like it was written more to convince management , and less geared to someone who would actually be `` doing '' it . anyone have any thoughts/experience on how it is ? its multiclass , binary and regression description seems a little sparse to call it ml in my ( unexperienced ) opinion <eoq> aws is set of more than 70 services . are you asking about amazon machine learning ? if yes , that service is more aimed at ml beginners , i do n't think you will get the same results as a hand tuned model . if no the main benefit is that you run whatever software you want on their servers and only pay per hour . <eoa> 
experience with amazon aws ? just wondering if anyone here has any experience working with amazon aws ? looking through the website it felt like it was written more to convince management , and less geared to someone who would actually be `` doing '' it . anyone have any thoughts/experience on how it is ? its multiclass , binary and regression description seems a little sparse to call it ml in my ( unexperienced ) opinion <eoq> at [ data science dojo ] ( http : //datasciencedojo.com ) , we 've started teaching aws ml , but there are many less options than in azure ml . azure has more algorithms and parameters to select ( and the way it 's set up is easier for beginners to understand ) . aws ml is still pretty new - but it works for simple modeling if you already have all your data in redshift or s3 . <eoa> 
using jupyter for machine learning ? hey all ! i was just doing some reading and came across jupyter . it looks like it 's pretty useful for data analysis but i was wondering if it 's used in industry for machine learning ? or is a more traditional editor what is commonly used ? <eoq> i 've never really known for editors to be standardized . most of my lab mates use sublime . most of them also mainly use c++ . <eoa> 
using jupyter for machine learning ? hey all ! i was just doing some reading and came across jupyter . it looks like it 's pretty useful for data analysis but i was wondering if it 's used in industry for machine learning ? or is a more traditional editor what is commonly used ? <eoq> i work in an r & d shop and we use jupyter a lot for any python work -- when you 're prototyping and idea , or demonstrating an analysis its excellent . of course once everything has been hammered down we move to ides for the heavy lifting ( eclipse , emacs , and pycharm are all popular ) of actually coding up a complete solution in anger . <eoa> 
when is probabilistic clustering applied ? the em algorithm seems to be the only example of probabilistic clustering . it seems to me that it is very similar to k-means , but deals better with noise . however , what is the advantage of probabilisitc clustering compared to density based clustering like dbscan / optics ? <eoq> if you have some expectation that your data has clusters with a particular parameterised distribution then obviously an em approach is going to be the right answer . dbscan and optics and hdbscan* ( a further improvement over optics ) can actually be viewed as a non-parametric probabilistic clustering where these exists some pdf that generated the data , and the goal is to find suitable level sets of that pdf -- this is most clear in hdbscan* and its obvious relationship with robust single linkage ( which takes precisely this probabilistic view and even proves convergence to the level set tree of the pdf with sufficient data ) . <eoa> 
how to calculate the performance of a neural network ? i made a neural network using python and numpy , and was wondering how i could calculate the performance of my classifier on a given test set . any explanation , or link would be appreciated . <eoq> by performance do you mean accuracy or speed ? in either case you should build two identical nns , one with your code and the other with an existing nn solution . train them both on the same dataset ( perhaps mnist ) and see what happens . it 's very likely that the nn you 've built will be significantly slower and if there are bugs in your code you could get diff results . solutions like tensor flow or theano do a lot of optimizations to make their computation very fast . <eoa> 
intuition for using matrices instead of vectors as gate parameter in an lstm when dealing with word embeddings . if i am correct then the gates of an lstm always take the same shape as the object , on which they are used to either forget , update or output the cell state . but i am wondering whether it is a stupid idea to just use for example a scalar to forget about a cell state which is a vector . since i did n't find any such implementations if there is a intuition related to word embeddings why this is not recommended . <eoq> so you might want a different scalar to decide which dimensions of the cell state you want to forget about.. so we need a vector of scalars to predict which will be forgotten . okay.. but we want this vector to depend on the rnn input , previous time point output , ( and possibly cell state ) ... so how can we transform those things into this vector of forget scalars ? we use a matrix and a nonlinearity <eoa> 
point me in the right direction for simple machine learning project ? hello , i have a simple project where people can text a phone number and i need to determine what category of sentence they are saying . the sentences do follow a pattern , so much so that i could just write a logic tree or if statement . however , i do this this falls into the category of classifcation ? and i saw this library : https : //github.com/monkeylearn/monkeylearn-ruby i 'm a good developer , but new to machine learning . id like to use my project as a way of going a bit deeper , but not too deep : ) thoughts ? am i on the right track ? <eoq> if the sentences follow a fixed pattern you are likely better of writing the if statement . machine learning only makes sense for problems that ca n't be solved with a simple computer program or algorithm . in your case it would be a bad approximation of the if statement . <eoa> 
is using a relu activation function always better then using using sigmoid/tanh ? ( neural networks ) from what i 've read online and from my limited experience training neural networks , it seems like relu is always a better choice than using a sigmoid/tanh activation function . i know one advantage relu has is that it does not suffer from the vanishing gradient problem ( both sigmoid and tanh are affected by this ) . on http : //playground.tensorflow.org/ , using relu always seems to get to a better result faster than the other functions . are there cases where sigmoid/tanh work better then relu ? if so , what are those cases and how would i identify them ? <eoq> if you wan na to learn an input representation , like sparse coding , then sigmoid activation is a good choice . <eoa> 
handling multiple feature types in an adaboosted-decision stump classifier [ matlab or python ] hi , i 'm working on a project where i 'm using an adaboosted decision stump classifier to predict a binary label ( 1 or -1 ) . i have working code that works on feature vectors that are exclusively binary , and am now trying to handle certain elements of the feature vector that are not binary . for example , some of them are numeric ( i.e . 0-9 ) , or categorical ( i.e . `` strings '' ) . they are interspersed throughout the feature vector . i 'm considering reformatting the data set provided to put categorical variables at the end and handle them later , or expanding each categorical feature to multiple 1/-1 binary features ( which seems very expensive ) . how should i go about dealing with categorical features , given that i 'm trying to use the adaboost formula on decision stumps that have primarily dealt with binary features only ? thank you ! <eoq> numeric features should be no problem of the original adaboost formulation ( see wikipedia / google ) , your decision stump should just be able to handle them , and then you can integrate it into your adaboost code . categorical features should be coded with dummy variables or one hot encoding : https : //en.wikipedia.org/wiki/one-hot <eoa> 
what is the current state of the art in speech recognition ? sorry for copying a question straight from the sidebar , but it is one that interests me quite a bit , and i did not find it having been asked here recently . so what is the current state of the art in speech recognition ? open source and/or otherwise ? <eoq> [ deep speech ii ] ( http : //arxiv.org/abs/1512.02595 ) should be in that ballpark . <eoa> 
need course recommendation hey /r/mlquestions ! i want to get into machine learning , and i 've been following andrew ng 's machine learning course on coursera . however , i 'm finding it kinda hard to follow , i 'm finding the math kinda shallow . i 'm rusty on my math skills , and thus i 'm looking for a course/book/learning path recommendation which will also cover the math side by side , or at least point me in the right direction . any recommendations would be helpful ! thanks ! <eoq> hi ! i work at data science dojo . besides our bootcamp , i recommend the books that we use in the bootcamp : 1- a cartoon guide to statistics ( easy intro to hard math ) 2- predictive analytics by eric siegel 3 - r for everyone 4 - doing data science : straight talk from the frontline good luck ! <eoa> 
what are advantages of ensemble learning compared to a single model ? ensembles ( e.g . multiple decision trees ) can reduce overfitting and give a better classifier than only a single system . is there another advantage ? <eoq> the very statement of your question suggests a pretty good advantage right away . but it 's also the case that various model classes ( or hypothesis classes ) can be combined in ensembles , producing a classifier ( or regression ) profile not possible with any single model . another advantage is that certain methods of training , such as what you find in random forests , provide a form of global feature selection , as any weaknesses introduced by greedy , local feature selection ( e.g . information gain ) in any individual model are often offset by choosing from a random subset of features from model to model . ensembles can also be parallelized more easily , leading to more efficient performance . there is also the issue of *boosting* , which allows one to create a classifier that is tremendously accurate from a set of models that are individually mediocre . there are more advantages , but these are some of the big ones that spring to mind . <eoa> 
theano vs tensorflow ? in your opinion which of the above two is easier to learn and prototype new neural networks . i have little experience in theano and it was very hard to get a grasp on how things are working . how do these two compare ? <eoq> i 'm no expert but tensor flow is still brand new where 's theano has been out for a while so there may be a bigger community around theano for when you need help ? ! ? <eoa> 
theano vs tensorflow ? in your opinion which of the above two is easier to learn and prototype new neural networks . i have little experience in theano and it was very hard to get a grasp on how things are working . how do these two compare ? <eoq> in my course we had used torch and it was really easy to get started it had lot of the built in stuff and learning lua is easy.but i prefer python so switched to theano recently and i must say it 's too tough but it gives you a much better understanding of what you are doing .symbolic variable , scan , function these are things you have to get a hold of before you start theano . <eoa> 
theano vs tensorflow ? in your opinion which of the above two is easier to learn and prototype new neural networks . i have little experience in theano and it was very hard to get a grasp on how things are working . how do these two compare ? <eoq> i recently decided to go with tensorflow . the reason was simply because it is open source , developed by a big company of which i 'm sure it will continue development for a while . due to this fact i expect more and more people to switch and other nice software to be written around tensorflow ( e.g . [ tensorboard ] ( https : //www.tensorflow.org/versions/r0.8/how_tos/summaries_and_tensorboard/index.html ) ) . <eoa> 
theano vs tensorflow ? in your opinion which of the above two is easier to learn and prototype new neural networks . i have little experience in theano and it was very hard to get a grasp on how things are working . how do these two compare ? <eoq> this is actually a very complex question with no correct answer ... it 's also worth noting that with tensorflow and theano ( and probably the other ml frameworks as well ) there are some great libraries that sit on top of them that make development far simpler . such as tflearn for tf and lasagne for theano . <eoa> 
theano vs tensorflow ? in your opinion which of the above two is easier to learn and prototype new neural networks . i have little experience in theano and it was very hard to get a grasp on how things are working . how do these two compare ? <eoq> tensorflow is the preferred because it supports more gpus and is faster . <eoa> 
knn without predictions ? excuse my ignorance , i am very new to all this : ) . for reference : * using python 3 ( numpy , pandas , ( hypothetically ) scikit ) * a small-ish data set , ( 4k rows , 15 columns ) . i know you can use knn to find the nearest neighbor ( s ) and it predicts something ( the example i 've seen in a bunch of tutorials is iris type ) . my question is : what if i do n't want to predict anything ? i 've got all my data , i just want to input one row ( with all 15 categories known ) and find the most similar data point . i am using a simple euclidean distance measurement ( after normalization of the data ) . but is there a good way to do this with ml ? are there other options i am missing ? perhaps you have a suggestion for something else ? thanks ! <eoq> unsupervised learning <eoa> 
knn without predictions ? excuse my ignorance , i am very new to all this : ) . for reference : * using python 3 ( numpy , pandas , ( hypothetically ) scikit ) * a small-ish data set , ( 4k rows , 15 columns ) . i know you can use knn to find the nearest neighbor ( s ) and it predicts something ( the example i 've seen in a bunch of tutorials is iris type ) . my question is : what if i do n't want to predict anything ? i 've got all my data , i just want to input one row ( with all 15 categories known ) and find the most similar data point . i am using a simple euclidean distance measurement ( after normalization of the data ) . but is there a good way to do this with ml ? are there other options i am missing ? perhaps you have a suggestion for something else ? thanks ! <eoq> are you interested in clustering your data into groups ? if so sklearn provides a bunch of methods for doing so : http : //scikit-learn.org/stable/modules/clustering.html <eoa> 
knn without predictions ? excuse my ignorance , i am very new to all this : ) . for reference : * using python 3 ( numpy , pandas , ( hypothetically ) scikit ) * a small-ish data set , ( 4k rows , 15 columns ) . i know you can use knn to find the nearest neighbor ( s ) and it predicts something ( the example i 've seen in a bunch of tutorials is iris type ) . my question is : what if i do n't want to predict anything ? i 've got all my data , i just want to input one row ( with all 15 categories known ) and find the most similar data point . i am using a simple euclidean distance measurement ( after normalization of the data ) . but is there a good way to do this with ml ? are there other options i am missing ? perhaps you have a suggestion for something else ? thanks ! <eoq> if you have one row and want to find the most similair point in your data , simply calculate all distances of all rows to your new row , and find the one with the minimum distance , that will be the most similair row . <eoa> 
need help understanding the maths behind gradient descent using neuralnetworksanddeeplearning.com book i have hit a wall at an exercise ( part http : //neuralnetworksanddeeplearning.com/chap1.html # learning_with_gradient_descent ) . i think i have understood most of the ideas of the gradient descent for training a neural network , however i ca n't understand the first exercise of the topic : prove that the choice of δv which minimizes ∇c⋅δv is δv=−η∇c , where η=ϵ/∥∇c∥ is determined by the size constraint ∥δv∥=ϵ . where c is the cost function ( objective function ) , v is a variable of c . the question also suggests : hint : if you 're not already familiar with the cauchy-schwarz inequality , you may find it helpful to familiarize yourself with it . i have tried fumbling around with the formulas like so : η=∥δv∥/∥∇c∥ then η=−∥η∇c∥/∥∇c∥ but i get stuck at this point . am i going in the right direction ? <eoq> what you really have to use is the equality case of cauchy-schwarz . cs states that for two vectors x and y and the scalar product < , > , you have : | < x , y > | < = ||x|| ||y|| with equality if and only if x and y are collinear e.g there exists some constant ( let 's call it - η ) such that x = - η y . as you want to minimize < x , y > , the best you can hope to achieve is < x , y > = - ||x|| ||y|| . for x = δv and y = ∇c you get the result because the equality case is a upper bond . then as δv = - η ∇c , you have η = ||δv|| / ||∇c|| if the gradient is non-zero . <eoa> 
plotting hypothesis function in linear regression hi guys ! sorry in advance for what i assume is a very stupid question . i am taking the machine learning course on coursera , and i 'm hitting a roadblock on the univariate linear regression hypothesis function plot . [ image ] ( http : //puu.sh/opdzm/bd7f4ced44.png ) the image linked above is the plotting i 'm referring to . i 'm unable to understand how theta 1 being 0.5 adds a slope to the line , and what the relevance of 2 is . also [ this ] ( https : //youtu.be/eanr4yttxiq ? t=88 ) is the video in question where the plot is being talked about . once again sorry for being dense about this . <eoq> hi , h ( x ) = theta0 + theta1x just realize that h ( x ) is your y value . in the first example : theta0 = 1.5 theta1 = 0 so y will always be 1.5 no matter what . in the second example , theta1 is now 0.5 , so whatever x is , y = x/2 . i 'm sure you get it by now , too lazy to do example points . cheers . <eoa> 
classification of biosignals i have an electromyograph device that will give me a 200 hz signal from 8 different sensors placed around the arm . they come to me as a byte array . i 'd like to classify these , however i 'm not sure where to start . since these are signals , i 'm assuming just passing in the raw array for each time sample is n't a viable option . i 've tried using a knn on them and it 's results were around 45 % accurate . is my use case right for ml or am i missing another algorithm that can do this for me ? <eoq> the scenario is correct but feeding the knn with the raw signals is not the right way to proceed . you 'll need some pre-processing step on the signals like filtering , than you have to extract features from the signals probably windowing them , than you have to identify the more relevant features for your problem and use those features ( predictors ) to train your ml algorithm . this is a very very general overview , i 'm not an expert as well but i suggest you to search for `` times series , machine learning '' , i think that would be a good start , but as said , i 'm not an expert . <eoa> 
how to choose an unsupervised classification algo ? food macronutrients dataset i have a dataset with macronutrient information for ~8000 foods . for each food i have the number of grams of protein , carbs , fat that are contained in 100g of that food . when i plot the data , i can identify a few clusters visually : http : //imgur.com/cl1kotd how do i go about choosing a unsupervised classification algo for this problem ? using the dbscan algorithm i was able to get this result : http : //imgur.com/1e7alyy is there a diff algo that would allow me to do better ? <eoq> you could try hdbscan . it effectively runs dbscan for all possible epsilon values and tries to identify the best possible flat clustering from that . this makes parameter selection alot easier and gets you to a good clustering quickly . <eoa> 
how to choose an unsupervised classification algo ? food macronutrients dataset i have a dataset with macronutrient information for ~8000 foods . for each food i have the number of grams of protein , carbs , fat that are contained in 100g of that food . when i plot the data , i can identify a few clusters visually : http : //imgur.com/cl1kotd how do i go about choosing a unsupervised classification algo for this problem ? using the dbscan algorithm i was able to get this result : http : //imgur.com/1e7alyy is there a diff algo that would allow me to do better ? <eoq> you could also try to use hierarchical clustering for your data and see if it fits better . http : //scikit-learn.org/stable/modules/clustering.html # hierarchical-clustering <eoa> 
how can i get a remote , beginner level job in ml ? hi , i am a blind programmer from iran . if you want more background on my question , you can check out my blog post called [ the tools of a blind programmer ] ( https : //www.parhamdoustdar.com/2016/04/03/tools-of-blind-programmer/ ) . i 've been developing the back-end for web applications for more than 5 years . however , with web development being drawn more and more toward presentation , i 'm thinking of changing my field . i have fallen in love with machine learning . however , here are the issues i have with it : 1. there are no machine learning jobs in iran . the market has n't gotten big enough to have big data . this will probably change in the next 2-3 years , but right now , it 's zilch . 2. all jobs require you to already be well-versed in the field , specially remote jobs , because they are not looking for people they will have to micromanage . i understand that point of view , but i have no way of connecting with these people in person and asking them to give me a chance . 3. i am not strong in mathematics . in fact , i 'm not strong in any kind of theory that i ca n't immediately apply . while i will definitely get around to learning the theories that drive machine learning once i get more into it , i do n't want to start with theory and then learn how i can bring it into the real world . so , is there a way of getting a remote job in my predicament ? if not , how can i get myself to a place where i would be hired , while keeping my day job of writing back-end systems with php ? thank you guys in advance for your help . you 're awesome ! <eoq> i do n't know why you would think it 's easier somewhere else . most of these jobs require a postgraduate ( masters or phd ) degree . not being strong in mathematics means your resume is in the trash before i even finish reading it because i do n't want to spend 5 years training someone if i need someone now . your comment # 3 has all kinds of red flags , even if you wrote the opposite of the first sentence . your best bet is to take some statistics courses on edx . you 'll probably need prerequisite math up to calculus ii to get through it . then , start working with some open source libraries and public data to prove you have the chops . i 'd make it a 5-10 year plan so not having anything in your home country for the next couple years wo n't matter if you really want to get into the field . <eoa> 
does there exist any ml algorithm to detect pivotal input parameter ? i want to know if there exist any neural network algorithm that allows you to determine which out of all the input parameters is the pivotal factor . or in other words can i detect the input parameter , changing the values of which yields maximum variation in the output value . for instance , i am doing a project to determine the air quality index of any given city using neural networks . as input parameters i have considered a variety of meteorological factors ( like wind speed , temperature , air pressure , humidity etc ) + air pollution from cars + air pollution from industries and my output parameter is pm 2.5 value . now i want to deduce , out of all these input factors which is the major contributor to the pm 2.5 value or changes in the values of which of the input parameters changes the output value significantly . <eoq> you may want to look at traditional statistical techniques like regression if you need an interpretable model . <eoa> 
does there exist any ml algorithm to detect pivotal input parameter ? i want to know if there exist any neural network algorithm that allows you to determine which out of all the input parameters is the pivotal factor . or in other words can i detect the input parameter , changing the values of which yields maximum variation in the output value . for instance , i am doing a project to determine the air quality index of any given city using neural networks . as input parameters i have considered a variety of meteorological factors ( like wind speed , temperature , air pressure , humidity etc ) + air pollution from cars + air pollution from industries and my output parameter is pm 2.5 value . now i want to deduce , out of all these input factors which is the major contributor to the pm 2.5 value or changes in the values of which of the input parameters changes the output value significantly . <eoq> i agree with /u/eurchus . there will be statistical methods that are n't `` machine learning '' that can answer this - and whenever that 's the case , i always recommend going with the standard statistical method . <eoa> 
does there exist any ml algorithm to detect pivotal input parameter ? i want to know if there exist any neural network algorithm that allows you to determine which out of all the input parameters is the pivotal factor . or in other words can i detect the input parameter , changing the values of which yields maximum variation in the output value . for instance , i am doing a project to determine the air quality index of any given city using neural networks . as input parameters i have considered a variety of meteorological factors ( like wind speed , temperature , air pressure , humidity etc ) + air pollution from cars + air pollution from industries and my output parameter is pm 2.5 value . now i want to deduce , out of all these input factors which is the major contributor to the pm 2.5 value or changes in the values of which of the input parameters changes the output value significantly . <eoq> as a rough rule of thumb , you can look at the size of an input 's outgoing weights to estimate it 's importance . ( and you could multiply it by the importance of the node a weight is connected to , which you determine in the same way . ) you can also do `` ablation tests '' where you remove each input ( or combination of inputs ) and see how much this changes the output , either in absolute terms or in terms of error . you can do this after training to find out the important of a ( set of ) parameter ( s ) in the final network , or before training to find out how important it is to begin with . <eoa> 
does having a degree from a ranked university ultimately matter ? i started taking cs courses at a local university ( university of missouri st. louis ) and wound up really enjoying it . a lot of the faculty here do research in genetic/evolutionary algorithms/programming and i ended up taking some graduate level courses too . do you all think it would be worth it to get the ms at an unranked university ( though its sister school in columbia is [ ranked # 101 ] ( http : //grad-schools.usnews.rankingsandreviews.com/best-graduate-schools/top-science-schools/university-of-missouri-columbia-178396 ) ) , or would it be wiser for me to just bite the bullet and apply elsewhere , such as urbana champaign or somewhere else of the like ? thanks ! ( edit : spelling* ) <eoq> you 're in one of the fastest growing fields in the world . do n't worry about ranks and do what you like . be interested , have fun and the rest will happen automatically . <eoa> 
good language for introduction to self-modifying algorithms ? hello , so i am trying to find a language with which i can write code to build/search through deductive reasoning 'nets ' , as well as self-modify it 's search algorithms based on information learned from these nets . i also want a language that i can use to write scripts for a 2d game engine , as i would like to build visual models of my projects for a web page ( to help my job/school prospects ) . so far i am only really familiar with mysql ( been working full time as a backend developer for about 7 months ) , but i have spent quite a bit of time developing relatively formal models for problem solving that i would like to attempt to put into code . any advice/suggestions would be greatly appreciated , thank you ! <eoq> well . what kind models ? <eoa> 
good language for introduction to self-modifying algorithms ? hello , so i am trying to find a language with which i can write code to build/search through deductive reasoning 'nets ' , as well as self-modify it 's search algorithms based on information learned from these nets . i also want a language that i can use to write scripts for a 2d game engine , as i would like to build visual models of my projects for a web page ( to help my job/school prospects ) . so far i am only really familiar with mysql ( been working full time as a backend developer for about 7 months ) , but i have spent quite a bit of time developing relatively formal models for problem solving that i would like to attempt to put into code . any advice/suggestions would be greatly appreciated , thank you ! <eoq> python is very popular for ai/ml stuff in general and it 's pretty widely used to make websites . that means there 's potentially a lot of support ( frameworks etc. ) . i would n't say it 's uniquely suited for self modifying programs though . for that i 'd go with ( some variant of ) lisp . <eoa> 
resources for text mining/auto writing ? there seems to be a lot of data on generalized numeric data , or vision systems . but where are there resources on text mining/writing applications ? i 've had a lot of difficulty finding resources to learn about these fields . do you have any to share ? most frameworks seem to focus purely on numeric data for analyzing vision/sound as first class citizens . <eoq> i 've continued searching so far i 've found [ this ] ( https : //www.cs.umd.edu/~miyyer/pubs/2014_nips_generation.pdf ) <eoa> 
back propagation dnn , question about layers when you are using a dnn is a good strategy to make the number of neurons in a layer always decreasing , from the input to the output ? <eoq> no . easy example : auto encoder <eoa> 
help with svm math i 'm trying to do support vector machine classification . i understand in general how it works , although not all of the nitty gritty math behind it . there 's one thing i 'm confused about in particular . with reference to this : https : //www.csie.ntu.edu.tw/~cjlin/ ... guide/guide.pdf it says we want to minimize 1/2 * w^t * w or alternatively 1/2 * ||w||^2 i know that the width of the margin is 2/||w|| so we want to minimize ||w|| . can someone explain to me where the 1/2 and square term come in ? <eoq> the 1/2 is added because it leaves the solution unchanged but it gives a better looking solution after derivation . <eoa> 
how to re-train a nn with more data ? if i have a trained nn with a training set of 100.000 inputs with diferent features , and i want to re-train the nn with lets say 1.000 new inputs , how can i train the nn , given that it was trained with 100.000 data and now i have only 1.000 . i hope you understand me . <eoq> by inputs you mean the number of samples in your data sets , right ? not the number of input nodes of your network . if the inputs and outputs in both data sets follow the same format , then you can basically do whatever you want . you can now start training on your new data set exclusively , or you can merge them together into a single 101,000 sample data set . or if you want to keep training on both , but you want to make the second data set more important , you can duplicate it a few times . for instance , you can make 10 copies of each sample in the second data set and combine it with the first to make a 110,000 sample data set . then you just continue training on that . <eoa> 
can you train a network to recognize a pattern , then produce a function that can be used independently of the network ? for example : i want to make an app that detects when i say `` hello '' . from my understanding , i could train a neural network with a bunch of clips of me saying `` hello '' . would it be possible to extract the network 's `` knowledge '' of what my hello sounds like ? admittedly i know next to nothing about machine learning , just curious about its limitations and uses . <eoq> the short answer is yes , that is totally possible ! a great example would be the mnist dataset , which contains thousands of handwritten digits as 28×28 images . you can train a neural network , which finds patterns in these images and learns what the numbers 0-9 look like . once your network has been fed a lot of example images and learned these different patterns , you can the then output what the nn thinks a certain digit looks like . i 'm on transit atm , will provide links when i get home . if this sort of stuff interests you , there some great resources to get started online , just have a look about in this subreddit : ) edit : links as promised 1. visualization of features learned in the mnist dataset mentioned above https : //www.tensorflow.org/versions/r0.7/tutorials/mnist/beginners/index.html 2. get started with some literature https : //www.reddit.com/r/machinelearning/comments/1jeawf/machine_learning_books/ 3. online courses https : //www.springboard.com/blog/machine-learning-online-courses/ i would definitely recommend you start easy , do some linear regression with python/r/octave and get a feel for the workflow and the frameworks available out there ! <eoa> 
presenting large image data to cnn hello all , i am currently working on a project to recognize features on mars ' surface using a cnn . an example would be to recognize sand dunes from a large .tif image of an area of interest . the only dataset i have is a large 30k by 100k .tif image of an area that has sand dunes and the same image with the sand dunes being marked in white . i plan to use the annotated image as training and the untouched image as testing . how would i approach presenting the data to the cnn ? slicing the image in smaller blocks seems ideal , but that 's still a lot of images i think , and would require a lot of memory . what is the best way to go about this ? edit : i should also mention that i 'm using theano+lasagne and python 2.7 <eoq> 1. you do n't have to use all of the patches , use the amount your hw allow you to . 2. you could split the patches and read them from the hd when needed i would go with option 1 as reading from the disk will slow you down quite a bit . and probably lots of the drawn patches are correlated anyway . <eoa> 
mlquestions tutoring on lasagne , or lasagne for dummies . hi , i have being studying nn 's for hobby , and now i decided to move to libs instead of coding simple mlp with sigmoid activations . i found lasagne , the idea of stacking layers was to me very interesting , but while reading the doc 's a couple of things seems confusing to me ( probably the part of theano ) , and that is the motive of this post . since i did n't find a better way to present my doubts , i 'll post the code and make questions along the way . let 's say that we have a train.csv , which has 10000 entries , each row with 15 attributes and one label ( 1 or 0 ) . the the program would probably begin with : import pandas as pd import theano.tensor as t from lasagne.layers import inputlayer , denselayer , get_output from lasagne.nonlinearities import rectify , softmax from lasagne.objectives import categorical_crossentropy from lasagne.updates import nesterov_momentum train = pd.read_csv ( 'train.csv ' ) l_in = inputlayer ( ( none,15 ) ) l_hi = denselayer ( l_in , num_units = 500 , nonlinearity = rectify ) l_out = denselayer ( l_hi , num_units = 2 , nonlinearity = softmax ) pred = get_output ( l_out ) so far , so good . the problem start below target_var = theano.tensor.scalar ( 'target ' ) loss = categorical_crossentropy ( pred , target_var ) if i understood theano correctly , all this is some sort of memory allocation without `` real '' computation been made . but , trying to run this piece of code generates the fallowing error : traceback ( most recent call last ) : file `` nn.py '' , line 20 , in < module > loss = categorical_crossentropy ( pred , target_var ) file `` /usr/local/lib/python2.7/site-packages/lasagne/objectives.py '' , line 129 , in categorical_crossentropy return theano.tensor.nnet.categorical_crossentropy ( predictions , targets ) file `` /usr/local/lib/python2.7/site-packages/theano/tensor/nnet/nnet.py '' , line 1842 , in categorical_crossentropy raise typeerror ( 'rank mismatch between coding and true distributions ' ) typeerror : rank mismatch between coding and true distributions since the label is '1 ' or '0 ' , should n't 'target_var ' be an scalar ? loss = loss.mean ( ) bonus question , is it possible to retrieve the values of the loss function in each epoch , to plot a graph ? params = get_all_params ( l_out , trainable=true ) bonus question 2 , been the network trained , params can be saved in on file and be read later ? updates = nesterov_momentum ( loss , params , learning_rate=0.01 , momentum=0.9 ) test_pred = get_output ( l_out , deterministic=true ) test_loss = categorical_crossentropy ( test_pred , target_var ) test_loss = test_loss.mean ( ) test_acc = t.mean ( t.eq ( t.argmax ( test_pred , axis=1 ) , target_var ) , dtype=theano.config.floatx ) input_var = theano.tensor.vector ( 'input ' ) # correct type ? train_fn = theano.function ( [ input_var , target_var ] , loss , updates=updates ) and , for the training part , is it the correct procedure ? for i in range ( len ( train ) ) : input = train.iloc [ range ( 15 ) ] target = train [ 'target ' ] train ( input , target ) to finish , if i just want to evaluate the output of the network , can i just use output = get_output ( l_out ) or it has to be a theano function ? note , that the code above ( with the exception of the code till the error message ) is not a `` real '' code i just typed ( based on the tutorial ) to try to understand better the mechanics of lasagne . ps : it might be a little confusing because it is 4am and i have the wake at 6am . still , any help would be appreciated . <eoq> > loss = categorical_crossentropy ( pred , target_var ) this is wrong . if you have 5 predictions , you should have 5 target variables . they should match shape . aka , both be ( 1 , k ) or ( k,1 ) or ( k , ) . -- > raise typeerror ( 'rank mismatch between coding and true distributions ' ) that 's why you have this error . the shapes do n't match . -- > bonus question , is it possible to retrieve the values of the loss function in each epoch , to plot a graph ? loss is returned from your compiled function . > train_fn = theano.function ( [ input_var , target_var ] , loss , updates=updates ) this is your compiled function . it 's basically like specifying : def train_fn ( input_var , target_var ) : return loss -- > train ( input , target ) you should have loss = train_fn ( inputs , targets ) -- in general , i find exploring theano code via a shell or notebook very informative . most of your confusions would go away if you did this and tested your questions empirically . <eoa> 
probably not a great question . i do n't know that this is the proper venue for this question but it 's here that my searching has led me so it 's here that i 'll stage myself . i dropped out of college 3 years ago and have recently begun learning programming again . i have a decent grasp on basic programming fundamentals and am relearning a lot of college math . currently working through a college algebra mooc then moving on to bigger and better things . so as someone with relatively little background , where should i start learning about machine learning . i 'm not looking to get a job or anything like that , it 's more of a curiosity . i 'd like to do a project of some sort using it but first i need to actually learn how to use it . i understand that there is a lot of math , so any direction toward online resources or books or what have you would be immensely helpful . thank you in advance for your time . <eoq> machine learning - coursera , stanford university is your next stop after you know the basics of linear algebra <eoa> 
probably not a great question . i do n't know that this is the proper venue for this question but it 's here that my searching has led me so it 's here that i 'll stage myself . i dropped out of college 3 years ago and have recently begun learning programming again . i have a decent grasp on basic programming fundamentals and am relearning a lot of college math . currently working through a college algebra mooc then moving on to bigger and better things . so as someone with relatively little background , where should i start learning about machine learning . i 'm not looking to get a job or anything like that , it 's more of a curiosity . i 'd like to do a project of some sort using it but first i need to actually learn how to use it . i understand that there is a lot of math , so any direction toward online resources or books or what have you would be immensely helpful . thank you in advance for your time . <eoq> the three most important math classes for machine learning are probability theory , multivariable calc , and linear algebra . those are all roughly sophomore level applied math classes . after you 're acquainted with those topic check out the coursera mooc . <eoa> 
python2 or python3 for ml ? <eoq> **3** edit , since you asked for it : because there is almost no argument for 2 anymore . it still has some libraries that are not available for 3 , but those are legacy and you can probably find good , if not better replacements . * python 2 will lose it 's support eventually , while 3 continues to be supported ; it is the newest version . * new libraries are primarily developed for 3 , although some still support both versions . * python 3 natively supports unicode , which avoids some problems , especially for newcomers . * python 3 comes bundled with pip , a package manager . while people argue that ( ana ) conda is a better package manager , pip often suffices . * python 3 is ( in my opinion ) more consistent . things that were done in python 2 implicitly now have to be done explicitly , which avoids quite some confusion . also have a look at : https : //wiki.python.org/moin/python2orpython3 note that the lack of library support is not really relevant anymore . i have never found a library that i needed that was available in 2 , but not in 3 . <eoa> 
code for image segmentation ? semantic segmentation ? anyone know a good github repo or other code source for state-of-the-art **image segmentation** or semantic segmentation implementations ? any language or library is fine , i 'm just looking for source to compare with and maybe run as a benchmark . <eoq> [ here 's one ] ( https : //github.com/bvlc/caffe/wiki/model-zoo # fully-convolutional-semantic-segmentation-models-fcn-xs ) credit for finding it goes to u/beatlejuce for answering my question on the 'ml simple questions thread ' on /r/machinelearning <eoa> 
convolutional network : how to determine filter size and number ? hi , say you have an 80x80 image , how does one determine the filter size and the number of filters in the first conv layer ? <eoq> rules of thumb + trial and error ( cross-validation ) https : //www.reddit.com/r/machinelearning/comments/43va6p/is_there_any_logic_behind_the_design_of/ https : //www.reddit.com/r/machinelearning/comments/36rd91/any_reason_behind_the_fact_that_the_filter_size/ https : //www.reddit.com/r/machinelearning/comments/3l5qu7/rules_of_thumb_for_cnn_architectures/ <eoa> 
what are some good concepts/algorithms for this problem ? disclaimer : i 'm not a ml person , and this is not a homework assignment . there is a deterministic two-player full-information game . ( tic-tac-toe , chess , go , diplomacy , etc . etc. ) . i do know what the game is , but it 's pretty silly , so let 's just pretend its chess instead . so the task that i have to complete is simply to write a heuristic that takes a state of my game and evaluates to some number-value . for example , for chess , maybe i can write a program that just counts the number of white pieces on the board . a naive heuristic , but that 's the type of program that i could use . i was thinking it would be pretty cool if i could use some machine learning algorithms to learn a heuristic and train it on some data of game records . what machine learning algorithm is best for this sort of task ? taking a state - > return a number value describing how good the state is for example , i was taking to my friend about making a neural network . but he said that would be totally inappropriate for my kind of task . if that 's bad . what 's good ? <eoq> have a look at the alphago design https : //googleblog.blogspot.com/2016/01/alphago-machine-learning-game-go.html ? m=1 <eoa> 
what are some good concepts/algorithms for this problem ? disclaimer : i 'm not a ml person , and this is not a homework assignment . there is a deterministic two-player full-information game . ( tic-tac-toe , chess , go , diplomacy , etc . etc. ) . i do know what the game is , but it 's pretty silly , so let 's just pretend its chess instead . so the task that i have to complete is simply to write a heuristic that takes a state of my game and evaluates to some number-value . for example , for chess , maybe i can write a program that just counts the number of white pieces on the board . a naive heuristic , but that 's the type of program that i could use . i was thinking it would be pretty cool if i could use some machine learning algorithms to learn a heuristic and train it on some data of game records . what machine learning algorithm is best for this sort of task ? taking a state - > return a number value describing how good the state is for example , i was taking to my friend about making a neural network . but he said that would be totally inappropriate for my kind of task . if that 's bad . what 's good ? <eoq> i tried to do something like this with mancala once . <eoa> 
what are some good concepts/algorithms for this problem ? disclaimer : i 'm not a ml person , and this is not a homework assignment . there is a deterministic two-player full-information game . ( tic-tac-toe , chess , go , diplomacy , etc . etc. ) . i do know what the game is , but it 's pretty silly , so let 's just pretend its chess instead . so the task that i have to complete is simply to write a heuristic that takes a state of my game and evaluates to some number-value . for example , for chess , maybe i can write a program that just counts the number of white pieces on the board . a naive heuristic , but that 's the type of program that i could use . i was thinking it would be pretty cool if i could use some machine learning algorithms to learn a heuristic and train it on some data of game records . what machine learning algorithm is best for this sort of task ? taking a state - > return a number value describing how good the state is for example , i was taking to my friend about making a neural network . but he said that would be totally inappropriate for my kind of task . if that 's bad . what 's good ? <eoq> honestly , for this task i have used monte carlo tree search and suggest it . it requires only minimal tuning for each game you apply it to . training a game-specific state evaluation is going to be very dependent upon the specifics of the game . i mean you can just use a svm or ann or what have you on your state vector but for it to really work well you want to figure out what is the relevant information to do your regression on ... this can be very tricky . what is the game ? i mean , why are you embarrassed because you are trying to solve a silly game ? look at the cool work these guys did with chutes & ladders : http : //www.datagenetics.com/blog/november12011/ edit : monte carlo tree search ( mcts ) is a simple algorithm . it is n't a machine learning technique per se , but it can be combined with them . in its simplest form : from a single game state assume all subsequent plays are made randomly . play many random games starting at that state . record the ratios of wins to losses . there are many , many ways to improve on this , and the most important is to bias the random plays towards ones that are more 'reasonable ' or 'likely ' . <eoa> 
`` are you sure you should be doing that ? '' hey people , i have a non-critical system i 'm working on that logs when people decide to do certain things . i want to flag when users do things that do n't match what they 've done before . an analogy would be `` joe posts on /r/machinelearning at noon every tuesday '' . here is how i 'm thinking about this : * output : which subreddit joe posted on * inputs : time of day , day of week over time , the system `` learns '' when joe is likely to post on what subreddit with the ultimate goal being something like this . one day , joe posts on /r/cars on tuesday at noon . the system notices this and says `` hey joe , are you sure you should be posting on /r/cars ? usually , you post on /r/machinelearning at this time . '' another case would be that tuesday at noon rolls around and joe does not post on anything . the system notices this and flags it for joe : `` hey joe , should you be posting on /r/machinelearning ? '' the system gets to watch joe 's posting history as long as is necessary for there to be some confidence in the flags that are being generated . in reality , joe is not actually watching the flags , these are flags being aggregated across many different people and the flags are just reports generated for the phbs to decide whether to investigate or not . then those decisions are fed back into the system . besides reading a book on neural networks many many years ago , i am a noob to machine learning . however , this screams to me to be some sort of machine learning project . am i way off ? how would i approach it ? thanks . <eoq> i think what you 're looking for is `` anomaly detection '' . <eoa> 
`` are you sure you should be doing that ? '' hey people , i have a non-critical system i 'm working on that logs when people decide to do certain things . i want to flag when users do things that do n't match what they 've done before . an analogy would be `` joe posts on /r/machinelearning at noon every tuesday '' . here is how i 'm thinking about this : * output : which subreddit joe posted on * inputs : time of day , day of week over time , the system `` learns '' when joe is likely to post on what subreddit with the ultimate goal being something like this . one day , joe posts on /r/cars on tuesday at noon . the system notices this and says `` hey joe , are you sure you should be posting on /r/cars ? usually , you post on /r/machinelearning at this time . '' another case would be that tuesday at noon rolls around and joe does not post on anything . the system notices this and flags it for joe : `` hey joe , should you be posting on /r/machinelearning ? '' the system gets to watch joe 's posting history as long as is necessary for there to be some confidence in the flags that are being generated . in reality , joe is not actually watching the flags , these are flags being aggregated across many different people and the flags are just reports generated for the phbs to decide whether to investigate or not . then those decisions are fed back into the system . besides reading a book on neural networks many many years ago , i am a noob to machine learning . however , this screams to me to be some sort of machine learning project . am i way off ? how would i approach it ? thanks . <eoq> if joe always posts on r/machinelearning on tuesday , and then one week he posts to r/cars on wednesday , should n't that also be a red flag ? if joe is a r/____ enthusiast , then why would the time be important at all ? <eoa> 
`` are you sure you should be doing that ? '' hey people , i have a non-critical system i 'm working on that logs when people decide to do certain things . i want to flag when users do things that do n't match what they 've done before . an analogy would be `` joe posts on /r/machinelearning at noon every tuesday '' . here is how i 'm thinking about this : * output : which subreddit joe posted on * inputs : time of day , day of week over time , the system `` learns '' when joe is likely to post on what subreddit with the ultimate goal being something like this . one day , joe posts on /r/cars on tuesday at noon . the system notices this and says `` hey joe , are you sure you should be posting on /r/cars ? usually , you post on /r/machinelearning at this time . '' another case would be that tuesday at noon rolls around and joe does not post on anything . the system notices this and flags it for joe : `` hey joe , should you be posting on /r/machinelearning ? '' the system gets to watch joe 's posting history as long as is necessary for there to be some confidence in the flags that are being generated . in reality , joe is not actually watching the flags , these are flags being aggregated across many different people and the flags are just reports generated for the phbs to decide whether to investigate or not . then those decisions are fed back into the system . besides reading a book on neural networks many many years ago , i am a noob to machine learning . however , this screams to me to be some sort of machine learning project . am i way off ? how would i approach it ? thanks . <eoq> disclaimer : i 'm not a computer scientist . regarding methodologies , if you have a string of characters representing [ day of week ] and [ time of day ] , then predicting the characters that follow ( representing subreddit ) is like an n-gram problem in natural language processing . if you express the input as a fixed-length input sequence then you can indeed use a simple feed-forward neural network to learn the fixed-length output sequence representing subreddit . nn is n't necessary though ( it 's just my hammer ) . if you just take the data points of [ time of week , subreddit ] , and plot them in 2d , then you will see clusters around certain locations . this is assuming you have paired a number with each subreddit ( which others might point out as being a bad practice unless the number assignments are not arbitrary ) . checking if an action is out of place is then like checking to see if a new point is within some distance of an existing cluster . i do n't know anything about cluster analysis ... from the perspective of finding an outlier , i would just fit the data to something like a high-degree polynomial curve and check how far away new datapoints are from that line , relative to the standard deviation of the fitting . if you discretize the [ time of week ] parameter , then each subreddit can simply have a probability associated with it based on previous statistics . so if i 've visited r/a 5 times in time-slot 1 and r/b 10 times in time-slot 1 in the past , then there 's a 66 % chance that i 'll visit r/b during the next time-slot 1 , where `` time-slot 1 '' would represent a broad bin like `` monday afternoon '' . the bins can be made less broad as more data is collected . <eoa> 
building an ml team for commodities trading i hope this is the correct place to post and apologies beforehand if not . i am a commodities trader and i run my own business . we have a large unique data set and so i am trying to build an ml team to sift through and look for possible trades / trends etc . i am however - not a tech person . what is the easiest and minimum path to gaining sufficient understanding of ml/dl to be able to have a rudimentary understanding of what we should be doing as a firm and to be able to converse meaningfully with the members of my team . i have a masters in economics but i have not done math for a long time - so assume i would be starting from zero ... ... <eoq> start with this . https : //class.coursera.org/ml-005/lecture <eoa> 
how to explain sudden jump in precision i was training a single layer lstm model to do sentiment analysis on imdb corpse with a binary bucket . i use pretrained word embedding and padding the sentence to the max and softmax to determine the loss and binary result . i set the learning rate at a pretty high level , and the precision/correctness keeps around 50 % for a while , and then suddenly jumps to 98 % in one single epoch and then stables at 98 % . however , cost/loss does n't change much . how would someone explain such a jump ? all the training process i have seen involve gradual increase in precision , not like this . is it possible that the prediction just shift over 0.5 line , and thus change the precision a great deal , but not that much for loss ? <eoq> did you mean corpus ? <eoa> 
how to explain sudden jump in precision i was training a single layer lstm model to do sentiment analysis on imdb corpse with a binary bucket . i use pretrained word embedding and padding the sentence to the max and softmax to determine the loss and binary result . i set the learning rate at a pretty high level , and the precision/correctness keeps around 50 % for a while , and then suddenly jumps to 98 % in one single epoch and then stables at 98 % . however , cost/loss does n't change much . how would someone explain such a jump ? all the training process i have seen involve gradual increase in precision , not like this . is it possible that the prediction just shift over 0.5 line , and thus change the precision a great deal , but not that much for loss ? <eoq> could you share any graphs or code ? <eoa> 
what is the difference between biological and artificial neural networks ? <eoq> the main difference is that a biological nueron is uni directional . basically , if we think of electricity as water , and a neuron as a bucket , biological information flow happens like this : imagine 4 empty buckets arranged as tightly as possible together . now , lifted over the center of those buckets , imagine another empty bucket suspended somehow . imagine you start to pour cups of water into the top bucket . in the beginning , none of that water goes into the bottom buckets . the bucket is filling , or the neuron is polarizing . imagine that each bucket is set up to tip over and spill all its water at once as soon as it reaches a certain volume of water . so you keep pouring cups of water in , still nothing goes into the buckets below . you 're pouring and pouring . suddenly , as soon as you reach the tipping point , the bucket tips over and spills all of its water into the downstream buckets . it slowly returns back to normal ( i.e . standing and empty ) meanwhile all the downstream buckets , are only partially filled and none of their water gets to the buckets below them and so it continues . the cups of water you poured into the first neuron represent each time the neuron was activated from upstream . the size of the buckets are usually the same . ( the amount of water it takes for them to tip ) . but the amount of water they recieve is not always the same . the buckets underneath can not communicate with the buckets above usually . so that 's how biological neurons work in a really simplified fashion . millions of buckets arranged on top of other buckets on top of other buckets . when it comes to artificial networks backpropagation maybe sort of works like the initial evolution of the biological network in the developing mind . but it does n't really work that way once the neurons are laid out . also in the animal brain , there is a network for each thing our brain can do . one for sight , within sight one for motion , one for contrast etc , one for hearing , one for words , one for singing , one for smell , etc , and they 're all integrated centrally . there is a memory relay that the sensory centers connect to , it gets solidified during dreaming , etc , there ate networks for balance , hunger , love , muscle movements , tools , actions , names , etc . the 6 layeted cortex in humans acts like a 6 layer convergence pattern . you just throw an entire tidal wave of data at a sensor . the first layer just reads how the sensor responds without making any sense of it . the next layer detects patterns in that noise , the next detects patterns in that layer and so on and so forth . the result at the end is often fed into a network for concepts and then vocabulary of any language . the network for vocabulary is tied with the network for mouth movement , sound etc and so forth , and that 's how we create skynet . <eoa> 
using regression to solve for multiple variables i 'm just getting started in my first ml project . i wanted to choose something i am interested in for my first project so i decided to try to predict daily fantasy sports outcomes using ml . i read this paper http : //cs229.stanford.edu/proj2015/104_report.pdf and i have a few questions that are probably pretty basic . can linear regression be used to predict more than 1 variable at once ? we care about multiple variables after a game which make up a players `` score '' but only have a small amount of information pregame to feed for a prediction . before the game we have features such as player name , home/away , opponent , team , team win % , opp win % , vegas o/u , etc . but we want to predict multiple variables such as points , assists , rebounds , etc . do we predict each one individually or am i missing something obvious ? <eoq> in mlr proper , you have y = x \beta + \epsilon , with y being the response vector , x being the n x p matrix of observations x predictors , \beta the regression coefficients , \epsilon the error vector . if you care about multiple variables that are n't transformations of your original y , then no , because you 'd be fitting the same regression coefficients for different variables . additionally , unless you 're certain that all predictors are useful for predicting all the different variables ( due to domain knowledge for example ) , then it may not even make sense to fit the same model for all variables . <eoa> 
is there any book/lecture series/mooc for unsupervised learning ? apart from k-means clustering and some gaussian stuff , i could not find much on unsupervised learning . are there any set of tutorials to get started with unsupervised learning ? or a book or a thesis of a phd candidate ? p.s . i did my fair share of searching before asking . <eoq> try the mooc on udacity . it is great for beginners and it is really fun ! <eoa> 
what activation function to use for convolutional layers ? i 've read that relu activation functions are used in convolutional layers , but i 'm wondering why . from playing around with a self-made implementation , i 'm seeing that the sigmoid activation function works great but the relu and softplus activation functions are resulting in an exploding gradient . <eoq> relus are the best default choice , they should outperform sigmoid if you are just doing standard cnn stuff . if you use relus in your case and it fails to converge , you should check your implementation . <eoa> 
i do not understand a step in neural networks , am i missing something ? so i am a programmer , getting into ai/neural networks . kind of started reading into it and a few tutorials . mostly basic stuff ( teach a few neurons to simulate/figure out how to be an and/or gate etc . ) now i get how those neural networks work , they have input , then neurons with gain and all ( ? ) inputs on them with a weight . then they provide an output , which you put through another set of neurons which connects to all of the first lines of neurons , and they generate output for all your outputs . given this thing , and say a car ( output : forward , steer left , steer right ) and input : ( distance to wall forward , distance to wall left of nose , distance to wall right of nose , distance to goal ) then write a feedback loop to reward the neural network for not colliding ( or punish collisions ) and reward it got getting closer to the goal . ( using back propagation ) now we give it an area , set it in a simple maze , simple , but required backtracking or going away from the goal to get to the goal . we let this neutal network run , give it random values , tons of iterations and generations , whatever way you like it . no matter how i imagine this thing working , i ca n't imagine any other situation then ending up with a car , that will steer straight to the goal , and stops just in front of wall , and then just sits there . doing nothing at all , because it ca n't for the life of him identify or work with the maze in order to go through . am i missing something here ? does the neural network have hidden magic i do n't get . or am i simply lacking inputs to complete the task ? i ca n't really find any good examples online to look at either . they either do basic stuff , mayor hand holding ( for the ai ) , or are insanely difficult to understand . i want to understand if a neural network can actually figure out a complicated problem like this given enough time/processing power and limited inputs and fixed outputs like described . <eoq> i think you 're correct in assuming that the system you 've described would n't really work . it 's not because of any intrinsic weakness of neural networks - i think you 've just chosen a poor objective function . <eoa> 
i do not understand a step in neural networks , am i missing something ? so i am a programmer , getting into ai/neural networks . kind of started reading into it and a few tutorials . mostly basic stuff ( teach a few neurons to simulate/figure out how to be an and/or gate etc . ) now i get how those neural networks work , they have input , then neurons with gain and all ( ? ) inputs on them with a weight . then they provide an output , which you put through another set of neurons which connects to all of the first lines of neurons , and they generate output for all your outputs . given this thing , and say a car ( output : forward , steer left , steer right ) and input : ( distance to wall forward , distance to wall left of nose , distance to wall right of nose , distance to goal ) then write a feedback loop to reward the neural network for not colliding ( or punish collisions ) and reward it got getting closer to the goal . ( using back propagation ) now we give it an area , set it in a simple maze , simple , but required backtracking or going away from the goal to get to the goal . we let this neutal network run , give it random values , tons of iterations and generations , whatever way you like it . no matter how i imagine this thing working , i ca n't imagine any other situation then ending up with a car , that will steer straight to the goal , and stops just in front of wall , and then just sits there . doing nothing at all , because it ca n't for the life of him identify or work with the maze in order to go through . am i missing something here ? does the neural network have hidden magic i do n't get . or am i simply lacking inputs to complete the task ? i ca n't really find any good examples online to look at either . they either do basic stuff , mayor hand holding ( for the ai ) , or are insanely difficult to understand . i want to understand if a neural network can actually figure out a complicated problem like this given enough time/processing power and limited inputs and fixed outputs like described . <eoq> i think understanding some more basic machine learning algorithms would help . <eoa> 
i do not understand a step in neural networks , am i missing something ? so i am a programmer , getting into ai/neural networks . kind of started reading into it and a few tutorials . mostly basic stuff ( teach a few neurons to simulate/figure out how to be an and/or gate etc . ) now i get how those neural networks work , they have input , then neurons with gain and all ( ? ) inputs on them with a weight . then they provide an output , which you put through another set of neurons which connects to all of the first lines of neurons , and they generate output for all your outputs . given this thing , and say a car ( output : forward , steer left , steer right ) and input : ( distance to wall forward , distance to wall left of nose , distance to wall right of nose , distance to goal ) then write a feedback loop to reward the neural network for not colliding ( or punish collisions ) and reward it got getting closer to the goal . ( using back propagation ) now we give it an area , set it in a simple maze , simple , but required backtracking or going away from the goal to get to the goal . we let this neutal network run , give it random values , tons of iterations and generations , whatever way you like it . no matter how i imagine this thing working , i ca n't imagine any other situation then ending up with a car , that will steer straight to the goal , and stops just in front of wall , and then just sits there . doing nothing at all , because it ca n't for the life of him identify or work with the maze in order to go through . am i missing something here ? does the neural network have hidden magic i do n't get . or am i simply lacking inputs to complete the task ? i ca n't really find any good examples online to look at either . they either do basic stuff , mayor hand holding ( for the ai ) , or are insanely difficult to understand . i want to understand if a neural network can actually figure out a complicated problem like this given enough time/processing power and limited inputs and fixed outputs like described . <eoq> try experimenting with [ genetic algorithms ] ( http : //blog.otoro.net/2015/03/28/neural-slime-volleyball/ ) and [ reinforcement learning ] ( http : //cs.stanford.edu/people/karpathy/convnetjs/demo/rldemo.html ) . <eoa> 
ml options for selling ranking ? i am currently experimenting with being a seller on amazon . one of the features they have is to allow for advertising campaigns . they allow for you to 'bid ' on certain keywords so your product appears when you search for that particular phrase . for a particular campaign , you can have many keywords and give them a daily budget . at the end of each day , you see how these performed through various metrics , culminating in the roi for that keyword . i wanted to use an algorithm to help figure out after every week which keywords were worth investing more funds into . for these types of problems , what algorithms would be best suited ? are there any services ( azure , aws , etc ) that would aid in this research ? <eoq> i do n't want to belittle ml , but would n't this seem to be something that could more easily/efficiently be accomplished by just comparing things such as clickthrough/items shipped/etc . ? just having some kind of comparison of revenue to cost would seem to do exactly what you want , and is a lot less taxing to implement ! <eoa> 
is donald trump an artifact of overfitting ? <eoq> over-regularization on noisy data , leading to regression to mean . <eoa> 
is donald trump an artifact of overfitting ? <eoq> i 'm more thinking some kind of deep instability in the optimization algorithm , leading to a pathological but unphysical solution . <eoa> 
is donald trump an artifact of overfitting ? <eoq> his unpredictability and changing opinions would point to it yes . <eoa> 
is donald trump an artifact of overfitting ? <eoq> does this thread belong in this subreddit ? <eoa> 
replicating neural style it seems a lot of people have been replicating the results of this paper http : //arxiv.org/pdf/1508.06576v2.pdf , training a cnn to recreate an image in the style of another . i would like to try doing this , but i 'm not perfectly clear on how it 's done . from what i understand , the cnn is essentially trained to map randomly generated pixels to the pixels of the desired picture ? or is it much more complicated than that ? <eoq> have you checked something like this : https : //github.com/jcjohnson/neural-style ? or this : https : //github.com/anishathalye/neural-style ? <eoa> 
validation accuracy higher when feed-forward with dropout than without i am training with dropout . if i feed the validation set forward with dropout still on the validation error is lower . if i turn it off , the validation error is far higher . ideas ? note that this issue is n't present for a shallow net , but it is for a very deep net . <eoq> are you dividing the weights by half ? at rest time or at validation , you need to divide the weights by half if you 're using 0.5 as dropout probability . <eoa> 
hoping for some guidance in selecting models for feature extraction hi , i 've been studying machine learning over the last couple of months with the hope of solving a specific problem . i 'm hoping someone can help me with advice on what approach to take . i have a large collection of labeled , connected graphs on which i would like to do unsupervised feature extraction . ideally i want to discover higher level features present in the graphs , ( similar to these features described by prof ng in this [ video ] ( https : //youtu.be/zmnoatzigik ? t=29m36s ) ) and be able to generate random graphs composed of only those features . from my research , autoencoders seem to be used for similar problems . does that sound right ? any advice or suggestions would be much appreciated . thanks ! <eoq> okay , so estimating `` features '' of a graph and simulating graphs can be difficult . the best way i know about to do that is based on the `` kronecker graph '' idea . you suppose your graph can be generated by kronecker product from a small graph . this actually yields excellent results , close to natural graphs . now what you want to do is estimate the generator of your graph , and the simulate other graphs from it . here is the paper that introduced kronecker graphs , and how to estimate our generator . https : //cs.stanford.edu/people/jure/pubs/kronecker-jmlr10.pdf <eoa> 
get dataset used in learning to execute zaremba put up his learning to execute code . https : //github.com/wojciechz/learning_to_execute i want to use data from this program , but i do not know torch so i do n't know how to obtain the data . what lines should i add to get the data , or does anyone have a set saved anywhere ? <eoq> did you read his paper ? he uses the penn tree bank data set . its a common benchmark . i believe the paper explicitly states how to get it from mikolov 's website . <eoa> 
need some direction for doing shared parameter regression . i have some y vs x data for a number of sub-populations in a population . for the sake of this question , the data for each sub-population can be modelled using linear regression , y = m x + b. there are only a handful of unique m and b values that are shared amongst the different sub-populations , and there are typically many fewer unique m 's and b 's than there are sub-populations . we do n't always know a priori which sub-population matches which m and b values , nor do we always know how many unique values of each there are . for this problem it is equally as important to get the correct number of unique values as it is to determine what those values are . i have tried two ways to do this so far : * the first way is to decide a priori which sub-populations will share which parameters ( for our data this works some of the time ) . this way is very fast , but it is n't always a great model because there is no general rule for deciding this . * the second way is to find m and b for all sub-populations , then find the unique values by using some clustering methods . this way takes much more cpu time , which is n't great for our setup but is n't the end of the world ; and sometimes we do n't have enough data for all sub-populations to make reasonable estimates of the parameters . ideally there is a third way that both decides on which parameters are shared by which sub-populations and what the unique values of those parameters are . what i am looking for is some literature , or a specific topic to research on . i have prior distributions for the model parameters , so bonus made up internet points to you if you can suggest a bayesian method . <eoq> how about gradient descent https : //spin.atomicobject.com/2014/06/24/gradient-descent-linear-regression/ <eoa> 
giving probability distribution as label i want to implement a neural net that will recognize images but get a discreet probability distribution as labels while training . right now , i have it for fixed labels . now , i want to change this such that i give it the probability distribution instead of the one fixed label . can i simply do this by changing the label to a sequence of labels , where each label is one probability ? i ca n't find any information on this online . <eoq> why do n't you use your softmax layer to directly learn the distribution instead of the labels ? that 's one way to go about it . <eoa> 
what model should i use for predicting a certain value ( task completion time ) ? details inside . hi all , i am new to this and i recently finished a beginner course on pluralsight [ this ] ( https : //app.pluralsight.com/library/courses/r-understanding-machine-learning ) . and i recalled that at my workplace , i have a process which takes quite long time to complete and lot of other deliverables are dependent on this process to complete . i have the historic data of that process which gives information like it 's start date , end date and the start time and end time of the subprocesses that it is comprised of . how can use this data to predict the estimated end time of any fresh instance of this process ? which algorithm should i use ? any pointers are highly appreciated . <eoq> does the process itself have any input ? how variable is the duration of the process ? <eoa> 
what model should i use for predicting a certain value ( task completion time ) ? details inside . hi all , i am new to this and i recently finished a beginner course on pluralsight [ this ] ( https : //app.pluralsight.com/library/courses/r-understanding-machine-learning ) . and i recalled that at my workplace , i have a process which takes quite long time to complete and lot of other deliverables are dependent on this process to complete . i have the historic data of that process which gives information like it 's start date , end date and the start time and end time of the subprocesses that it is comprised of . how can use this data to predict the estimated end time of any fresh instance of this process ? which algorithm should i use ? any pointers are highly appreciated . <eoq> firstly what is your baseline distribution of what you want to predict ? what is the business value of the thing you are going to predict , and what representation would be most useful to those people ? what would they do with the answer ? <eoa> 
rnn - vanishing or exploding problem i 'm trying to understand an exercise from [ hinton 's course on neural networks ] ( https : //www.coursera.org/course/neuralnets ) . [ complete exercise ] ( http : //i.imgur.com/9qpybsn.jpg ) basically , i need to know if it 's a vanishing or exploding problem . so what i did was first calculate all the partial derivatives corresponding to `` chain ruling '' ∂e/∂wxy and search for some light there : ∂e/∂wxy = ∂e/∂y * ∂y/∂h3 * ∂h/∂z3 * ∂z3/∂h2 * ∂h2/∂z2 * ∂z2/∂h1 * ∂h1/∂z1 * ∂z1/∂wxh calculating each component : ∂e/∂y = - ( t3 - y ) = - ( 0.5 - y ) ∂y/∂h3 = why = 1 ∂h3/∂z3 = h3 ( 1 - h3 ) ∂z3/∂h2 = whh = -2 ∂h2/∂z2 = h2 ( 1 - h2 ) ∂z2/∂h1 = whh = -2 ∂h1/∂z1 = h1 ( 1 - h1 ) ∂z1/∂wxh = x1 so : ∂e/∂wxy = - ( 0.5 - y ) * 1 * h3 ( 1 - h3 ) * ( -2 ) * h2 ( 1 - h2 ) * ( -2 ) * h1 ( 1 - h1 ) * x1 i saw nothing there . so i payed attention to ∂y/∂z as the exercise says , and this is what i found : ∂y/∂z3 = ∂y/∂h3 * ∂h3/∂z3 ∂y/∂h3 = why = 1 ∂h3/∂z3 = h3 ( 1 - h3 ) so : ∂y/∂z3 = 1 * h3 ( 1 - h3 ) what i see there : - h3 is the logistic function , output always will be between ( 0 ; 1 ) - 1 minus something between ( 0 ; 1 ) is something between ( 0 ; 1 ) - and something between ( 0 ; 1 ) multiplied by something between ( 0 ; 1 ) is something between ( 0 ; 1 ) so in conclusion , not only it will be something between ( 0 ; 1 ) , but it will be pretty small because the multiplication in ∂y/∂z3 , and then it will be much smaller because in ∂e/∂wxy , that logistic function appears 3 times ( one for each logistic hidden unit ) multiplying together , and that will shrink the whole gradient a lot , independently of the other terms . my question is , am i correct ? and when can it be an *exploding* problem ? because as i see here and with that logic ( that maybe is wrong ) , it 'll always be a vanishing problem . <eoq> to my knowledge , as long as you 're using the sigmoid activation function then you 'll only have the vanishing gradient problem . notice that the the derivative of the sigmoid produces outputs on the range ( 0 , 1/4 ] . every time you backpropagate you 're multiplying by the derivative of the sigmoid , and since it 's always a fraction you 're always making it smaller . <eoa> 
rnn - vanishing or exploding problem i 'm trying to understand an exercise from [ hinton 's course on neural networks ] ( https : //www.coursera.org/course/neuralnets ) . [ complete exercise ] ( http : //i.imgur.com/9qpybsn.jpg ) basically , i need to know if it 's a vanishing or exploding problem . so what i did was first calculate all the partial derivatives corresponding to `` chain ruling '' ∂e/∂wxy and search for some light there : ∂e/∂wxy = ∂e/∂y * ∂y/∂h3 * ∂h/∂z3 * ∂z3/∂h2 * ∂h2/∂z2 * ∂z2/∂h1 * ∂h1/∂z1 * ∂z1/∂wxh calculating each component : ∂e/∂y = - ( t3 - y ) = - ( 0.5 - y ) ∂y/∂h3 = why = 1 ∂h3/∂z3 = h3 ( 1 - h3 ) ∂z3/∂h2 = whh = -2 ∂h2/∂z2 = h2 ( 1 - h2 ) ∂z2/∂h1 = whh = -2 ∂h1/∂z1 = h1 ( 1 - h1 ) ∂z1/∂wxh = x1 so : ∂e/∂wxy = - ( 0.5 - y ) * 1 * h3 ( 1 - h3 ) * ( -2 ) * h2 ( 1 - h2 ) * ( -2 ) * h1 ( 1 - h1 ) * x1 i saw nothing there . so i payed attention to ∂y/∂z as the exercise says , and this is what i found : ∂y/∂z3 = ∂y/∂h3 * ∂h3/∂z3 ∂y/∂h3 = why = 1 ∂h3/∂z3 = h3 ( 1 - h3 ) so : ∂y/∂z3 = 1 * h3 ( 1 - h3 ) what i see there : - h3 is the logistic function , output always will be between ( 0 ; 1 ) - 1 minus something between ( 0 ; 1 ) is something between ( 0 ; 1 ) - and something between ( 0 ; 1 ) multiplied by something between ( 0 ; 1 ) is something between ( 0 ; 1 ) so in conclusion , not only it will be something between ( 0 ; 1 ) , but it will be pretty small because the multiplication in ∂y/∂z3 , and then it will be much smaller because in ∂e/∂wxy , that logistic function appears 3 times ( one for each logistic hidden unit ) multiplying together , and that will shrink the whole gradient a lot , independently of the other terms . my question is , am i correct ? and when can it be an *exploding* problem ? because as i see here and with that logic ( that maybe is wrong ) , it 'll always be a vanishing problem . <eoq> i 'm gon na add this here just in case someone has a similar question . so yes , that deduction was correctly , and could find it [ on wikipedia ] ( https : //en.wikipedia.org/wiki/vanishing_gradient_problem ) ( yeap ... ) : > *traditional activation functions such as the hyperbolic tangent function have gradients in the range ( -1 , 1 ) or [ 0 , 1 ) , and backpropagation computes gradients by the chain rule . this has the effect of multiplying n of these small numbers to compute gradients of the `` front '' layers in an n-layer network , meaning that the gradient ( error signal ) decreases exponentially with n and the front layers train very slowly . * i suppose that the exploding problem occurs only when other kind of activation functions are used , that allow values over 1 , that multiplying together over backprop can `` explode '' into some really big numbers . <eoa> 
lagrange formulation of svm model - question i 'm going through the caltech lectures on machine learning and am confused by one step in the calculating the solution to the svm model . in slide 13 ( link below ) , why is there a minus sign in front of the summation ? i thought that for constraints of the form g ( x ) > =0 , you add a lamba * g ( x ) term to the lagrangian , not -lambda * g ( x ) . slide 9 shows that the constraint is of the form g ( x ) > =0 . i must be missing something small . http : //work.caltech.edu/slides/slides14.pdf <eoq> that 's because if you have an optimization problem min f ( x ) subject to h ( x ) < = 0 , the lagrangian will be l = f + lambda * h where the lambda ( i ) are positive . here with h = - g , he used - lambda with lambda ( i ) > = 0 instead of +lambda with lambda ( i ) < = 0 for readability . <eoa> 
using bayes classifier in home automation to build an alexa like assistant hi , i have a lot of connected devices at home , i can control everything from my phone on a interface i made , but i would like to go further . my goal is to be able to text or speak with my home system , like a kind of alexa/siri ( with very basic commands to begin ) . i think machine learning can be a good idea to implement that , using a bayes classifier ( i found this awesome library to do that : https : //github.com/naturalnode/natural # classifiers ) my goal is to train the system with sentence in input ( `` turn on the light '' for example ) , and in output the action ( `` light-on '' for example ) , and then use the trained system to detect what i want to do when i speak to my house . do you think that 's a good way of doing this ? thanks a lot , <eoq> mark ? <eoa> 
why is this nn so dumb ? <eoq> it 's an lstm put together with jupyter , keras and bokeh . the yhat ( green lines ) should have distinct steps in them , like the y ( black ) but it does n't seem to matter what hyperparameters are , it just wont fit nicely . [ code is here ] ( https : //gist.github.com/emailgregn/e26a29557178101a2350 ) and [ sample data generator here ] ( https : //gist.github.com/emailgregn/ca7ae21181ae3d98abf6 ) <eoa> 
python keyerror keyerror obtained when reading from a csv file . i 've created an empty dict and want the most frequent values of a headed column ( attribute ) to populate the dict . could someone please explain why this is occurring ? def gain ( data , attr , target_attr ) : `` `` '' calculates the information gain ( reduction in entropy ) that would result by splitting the data on the chosen attribute ( attr ) . `` `` '' val_freq = { } subset_entropy = 0.0 # calculate the frequency of each of the values in the target attribute for record in data : if record in val_freq [ record [ attr ] ] : val_freq [ record [ attr ] ] += 1.0 else : val_freq [ record [ attr ] ] = 1.0 <eoq> you are testing if `record` is in `val_freq [ record [ attr ] ] ` without being sure if `record [ attr ] ` is in `val_freq` . what you might want to do is to check : if record [ attr ] in val_freq : also you should make sure if `attr` is actually in `record` . i do n't know how exactly your data looks like , but that could be another error waiting to happen . <eoa> 
machine learning for mri/ct scans i have a set of ct scans that i 'd like to use for disease classification and severity classification ( likely a regression problem ) . is it possible to use networks like cnns given that the data is 3 dimensional ( a set of horizontal slices which put together form a 3d array ) ? and if not what is the best algorithm/way to go about this ? <eoq> there 's no inherent reason why cnns ca n't work on 3d data - see this link for some examples . https : //www.researchgate.net/post/is_there_an_example_deep_learning_ie_convolution_neural_networks_code_for_3d_image_segmentation a big problem with medical data though is that you may not have enough per class ( or output value in your regression case ) to train the cnn effectively . cnns are notorious for requiring a very large amount of training data . they learn to extract the most important image features per target class , but because images have a large amount of variability in image data , this means requiring several thousand ( at the very least ) images per class to avoid the network from overfitting to unimportant image details . you could perhaps get away with fewer images if you can do some pre-processing ( e.g . if your ct scans are of the brain , perhaps map them onto a predefined brain template ) to reduce the variability . without knowing more about your problem , i think your best bet is to go down the traditional route of figuring out a set of features that you can compute from the images , and then using them to train a normal ( i.e . non-image-based ) classifier . but feel free to ping me if you 'd like to bounce some ideas around . <eoa> 
how do you guys download massive datasets ? how are you guys getting massive datasets ? anything over 300gigs which is the datacap by popular isp 's like comcast ? <eoq> if you work at a university or company , ask your it department . otherwise i do n't really know . from what i hear about comcast , the chances will probably be slim , but maybe you can ask them to make an exception because you 're using the data for ( non-profit ) research or something . <eoa> 
how do you guys download massive datasets ? how are you guys getting massive datasets ? anything over 300gigs which is the datacap by popular isp 's like comcast ? <eoq> i 'm not sure what datasets you 're looking at , but you could always ask the uploader to split it up into separate files , and then put them back together yourself . <eoa> 
trying to match transactions with receipts - where to start i 'm trying to match receipts ( ocr scanned ) with their transaction entry from my credit card provider . i have training data for receipts i already matched . but where to start ? and can i use an existing service api ? <eoq> are you trying to automatically reconcile your statements with your receipts ? <eoa> 
machine learning , deep and wide data , how to start ? this is a very general overview question that i have not seen addressed elsewhere , so any pointers to literature is appreciated . i have been using scikit-learn with some success on a variety of ml problems . generally a main challenge is just to get the data parsed and formatted for sk to take it in , ie categorical variables etc . now have a much larger and richer dataset of healthcare related data to process for insights and there are multiple data tables per patient which are very wide and related . eg a subject ( patient ) will exists in many tables . each table is quite wide with many possible columns.. and once i recode the variables for categories it will really blow out . but that is not even the worst part , we also have encounter data with icd codes and dates , so many records for a patient , this table is also very wide and may have dozens or more records per patient . date order may be important..eg sequential occurrence . any suggestions for how to think about parsing/formatting the data for exploration ? is there a library for processing both deep and wide data ? or how should i be thinking about this ? i 'm currently working with a very small subset 500 persons , but the result will be applied to very large populations , 1,000 's if not millions . <eoq> a bit more research is pointing me to hmm and rnn approaches similar to those used for natural language processing , at least for the `` utilization , sequential '' portion of the problem.. <eoa> 
necessary math background ? considering taking a course in ml and trying to evaluate what specific math skills would be requisite . <eoq> calculus , linear algebra , probability , and statistics <eoa> 
necessary math background ? considering taking a course in ml and trying to evaluate what specific math skills would be requisite . <eoq> if this is in a university , try writing the professor and asking for a syllabus or at least what text they use . machine learning is a growing field and any one class will have different requirements than another . a lot will depend on whether it 's more theoretical or applied . <eoa> 
necessary math background ? considering taking a course in ml and trying to evaluate what specific math skills would be requisite . <eoq> i took an upper level undergrad course in ml and did well enough even though my grasp of math is n't great ( i was pretty much re-learning the linear algebra as i went ) . the most intense linear alegebra i ended up doing is deriving the psudo-inverse of a matrix , which is super easy . also you need calculus , but only derivatives ( in fact , as i recall , mostly only partial derivatives ) and they 're always super simple . in this case i think the most complex thing you 'll need to understand is the chain rule . be able to do it ( it 's easy once you understand it ) . for probability you need to know bayes theorem ( which every human being should know regardless ) , and understanding the concept of a probability distribution is good . but the best thing you can do is talk to your professor , tell him your math background , and ask for his opinion . <eoa> 
kmeans clustering library ( x-post from /r/python ) does anybody know of a library in python that allows the user to change the distance ( similarity ) function for kmeans clustering to a user defined function ? i 've been using sklearns but they do not allow the user to do that . thanks in advance ! <eoq> ideally the answer to this is to not use k-means ; there are potentially much better clustering algorithms ( depending on what you want to do exactly of course ) . any algorithm that accepts a distance matrix as input ( with metric='precomputed ' in sklearn ) allows arbitrary distance functions ; as long as you have a small enough dataset ( or enough memory ) that computing the full distance matrix is feasible then this is the easiest solution . in practice if you have enough data that this is n't a viable solution then a user defined function for distance computations is going to likely result in abysmal runtimes : the reason that many algorithms accept a limited set of distance functions is that those are the functions that have been appropriately optimized ( usually via cython , or c libraries ) ; the overhead of going out to a python user-defined function for every distance call is going to be huge in the long run . could you outline a little more about your problem and what you 're trying to do ( and why you chose k-means ) ? i might be able to recommend some better alternative algorithms . <eoa> 
how to single-node ml lab ? for text log classification . so i 've spent time this week on regex filters and field extractions for logstash to read my log files and insert the logs and extracted fields into elasticsearch . my application is very log-noisy and i was weeding out the `` normal '' errors to better identify actual issues , so i 've iteratively been identifying patterns of the most common remaining log entries to end up with the more rare ones . i showed the progress to coworkers , and one asked if machine learning could do the classification for me and free me up to better interpret the meaning . hmmm ... . so a few dozen internet pages later ... i 'm wanting to install mahout or spark/mlib to kick the tires , feed it some logs and see if i can figure out what to ask next . but much of the help material on installing on a cluster . i just want to set something up on a single machine and feed it up to a gigabyte of log files and see what it i can do with it . so am i on the right track ? can mahout or spark/mlib run on a single machine , or should i be looking at something else ? <eoq> crickets over the weekend so far . but i got apache spark installed and doing a couple of simple things on a single machine , and the actual steps are n't difficult at all : - have java with java_home set appropriately - [ download spark ] ( http : //spark.apache.org/downloads.html ) precompiled with hadoop client - untar the spark tarball into a directory - run things in the bin folder on windows there were some errors even though there are cmd/bat versions of the commands in bin . i think i need extra libraries . but on a bare ubuntu 14.04 container plus java 8 and spark it 's running with no extra steps so far . [ this page ] ( http : //spark.apache.org/docs/latest/ ) has some example commands . [ this section showing language classification of tweets ] ( https : //databricks.gitbooks.io/databricks-spark-reference-applications/content/twitter_classifier/index.html ) ( youtube presentation included ) is where i 'm going to start my tinkering . it demonstrates tokenizing and classifying tweets into clusters that end up being more or less language collections , but i think this can do what i 've been trying to manually do : classify log entry types into clusters , and then i can focus on the small clusters as rare log entry types . i think my steps are going to be : - use my existing logstash field extractions against a couple of non-problem days ' logs - store that in some intermediate data store ... the tweet exercise uses sql ; with my relatively small data set i 'll see if i can use text dumps or just pull it back out of elasticsearch . if those fail , mongodb ? - featurize the log text . i may omit the timestamp and thread pool info ; or try both with and without . i 'll probably start with the [ hashingtf ] ( http : //spark.apache.org/docs/latest/mllib-feature-extraction.html # tf-idf ) method as the example uses , but the [ word2vec ] ( http : //spark.apache.org/docs/latest/mllib-feature-extraction.html # word2vec ) method looks worth trying out for this purpose to my untrained eye . - do [ k-means clustering ] ( http : //spark.apache.org/docs/latest/mllib-clustering.html # k-means ) against the featurized log data - ... - profit - well , actually then i 'll see what size each cluster is and look for small-cluster or unmatched cluster ( if that 's a thing ) against the rest of the log data . <eoa> 
compressing image data to two channels for grasp detection i 'm utilizing transfer learning on an imagenet trained network to train a cnn for grasp location ( via regression ) . i 'm using image and depth data from a kinect . to use an imagenet trained model , the network needs to be 3 channel , and i 've done this initially by arbitrarily throwing away the blue channel to go from rgbd to rgd . i have a working network from this , but i think it was a hacky solution . i 've been thinking how for grasping locations , colour is irrelevant and in fact could lead to biases as my dataset is relatively small . in light of this , i was thinking i could convert rgb to hsv and throw away the h channel . this throws away colour information but preserves texture , which i think it more important for learning grasp features . is this thinking sound ? <eoq> if you have to choose 3 channels from color+depth , then it indeed sounds like hue is probably the least important for you . however , i 'm a little concerned about how well transfer learning is going to work if you change the features . so i could imagine rgd/rdb/dgb might work better than dsv , because those only `` screw up '' one channel . if you 're training the imagenet network yourself , then i suppose you could use hsv from the start , and avoid this a bit . it might also be possible to just augment the original network with a fourth channel . you can just add the connections for that if you wanted . if you think these could also benefit from transfer learning , you could initialize the weights based on those of the other channels . for instance , you could use their average . and then maybe multiply everything by 3/4 to keep the size of the inputs to the next layer roughly equal . basically i do n't know what will happen , so you may just have to try different options . that 's usually the answer in these cases , but i hope it wo n't be too much effort / training time here . <eoa> 
single feature learning useful ? hi , i 'm currently working on a machine learning project in university . my supervisor wants me to split out features into subcategories and single features to compare their performance . while i understand subcategories , i ca n't see the point in making all experiment series with 100s of single attributes . is this common ? do you think it 's scientifically necessary ? <eoq> i think it 's pretty common to make scatterplots for independent and dependent variables to visualize how they appear to correlate . i 'm not sure about full-fledged single feature learning . what i 've seen more often are ablation studies where you start with all of your features and then see how performance deteriorates when you remove each feature . <eoa> 
i do n't know what you 're doing exactly or why you 're using neural network specifically , but have you not heard of scikit-learn , specifically their [ regression page ] ( http : //scikit-learn.org/dev/supervised_learning.html # supervised-learning ) ? <eoa> 
i teach a bunch of courses on regression and classification using various linear and deep nets , and i find that the actual math you need to understand what 's going on is usually too much math for those who want to focus on the practical bits . ( at least from what i remember , you should have learned about maximum/log-likelihood , gradients , and the chain rule in undergrad ) . that said , why do n't you just try a more modern library like theano or tensorflow ? you would n't need to calculate gradients yourself , and it contains apis for more recently developed techniques . so using them proficiently is just a matter of reading and understanding the documentation . <eoa> 
inference stage in batch normalised network i came across this paper http : //arxiv.org/abs/1502.03167 and it state that normalising ( per batch ) the layer would resulted in faster learning step . batch normalising make sense in training however it 's not in inference step . as the matter of fact , section 3.1 suggest that it 's undesirable . however , i am confused as on how do we set the mean and variance during inference stage . the author suggest to simply use the mean of variance / mean during training . how many variance / mean should we collect during the training stage ? obviously using all of them wo n't make sense in this case . <eoq> your options are : * fix the weights , then calculate the mean/variance by running through all the training data again . use these values for inference . * track a moving window of mean/variance during training , then use these values during inference . the second option is the one most people go with . <eoa> 
classification with numerical labels ? i have a dataset in which each row has information about who , where , when and how much a customer has bought/spent in certain products . i was wondering if i could make a predictive model in which given the `` who '' , `` where '' and `` when '' i could predict how much money is this customer spending . <eoq> what is your question ? yes , you might be able to build a model for that . how much data do you have ? what is important to discover ? do you want to use a certain algorithm or just solve the problem ? <eoa> 
a confusion regarding kernels i fairly understand kernels in machine learning , how algorithms are kernalised and i also understand how kernels in image processing work , as they do in smoothing and in filters . but i ca n't help but wonder if they are related . for some time i began to relate the guassian kernel to be some function to transform the image vector into a new feature space and all , but i 'm unable to bring out any connection . could someone help me . <eoq> i have been asking myself what is a kernel for some time now . i thought i understood it when it came to image processing , but then i started doing ml and i too could n't see the connection . looking up the definition on google , maybe that will help us : kernel - `` the central or most important part of something '' . <eoa> 
machine learning vs. data mining , and recommendations for someone with no background in this ? hi machine learning . i 'm a doctoral student in an it-related field and have an idea for my dissertation . it will involve utilizing big data and automated analysis to make recommendations . my weak areas involve big data altogether . i do n't have any background with machine learning , statistics , or data mining . i am skilled and knowledgeable in my area of study , and am willing and excited to learn about machine learning or data mining . i 've been researching which subset of big data i should be using for my dissertation , but have ended up more confused than before . for example , [ this reddit post ] ( https : //www.reddit.com/r/machinelearning/comments/24sc5n/data_mining_vs_machine_learning/ ) attempts to explain it , but everyone 's interpretation is different . basically , i need to begin studying one of the big data focuses to get my dissertation started , but i 'm not sure which one i need . essentially , i 'm looking to incorporate a big data/machine learning/data mining approach that : * takes information about a large number of data i have collected * identifies patterns with the data * makes configuration recommendations to me based on the rules i set * optional : automates the implementation of the `` best '' recommended config is this data mining , machine learning , or something else ? any other recommendations for someone with no background in mathematics , statistics , or big data ( but can program , perform the other technical pieces , and learn/study about any topics i need to ) ? <eoq> the distinction between data mining and machine learning is pretty fuzzy and there is a ton of overlap so i would n't worry too much about it . your question is pretty light on details ( what kind of data do you have ? what sorts of patterns are you looking for ? what kind of rules will you create ? are you planning on using the discovered patterns to make rules ? ) so i 'm not sure how helpful the remainder of my answer will be . if you provide more detail about the specific problem you are trying to solve there 's a chance someone here could provide some more helpful advice . since you are n't knowledgeable about dm/ml trying to find the correct way of formulating the problem so that it can be handled by dm/ml will be impossible . there are many kinds of problems that have been studied quite a bit that are n't discussed in introductory materials so relying on the standard textbooks is n't necessarily a good idea . you may very well start learning about ml/dm and spend weeks of study only to find out it is n't applicable or to realize that you 've been studying the wrong portions of the field . some of what you are doing ( identifying patterns in data ) sounds like it would benefit from ml/dm techniques , while other things ( making recommendations based on user defined rules ) does not ( if you were trying to get a computer to learn potential rules from data you should look into association rule mining ) . **instead you should probably try to find previous work in the field solving similar problems and look at the sorts of techniques they employ and use that as a jumping off point for further background reading . ** if you are trying to automate the configuration of databases then search something like `` automated database configuration '' in google scholar . if you want to try to learn about dm/ml your best bet would be to [ checkout some of the introductory links on /r/machinelearning ] ( https : //www.reddit.com/r/machinelearning/wiki/index ) and read the first couple chapters of one of the textbooks listed there or watch the introductory lessons from a mooc so that you have a basic idea of what sorts of dm/ml tools are out there . but chances are good that your problem wo n't fit quite right into the basic paradigms discussed in introductory materials so it is important that you look at previous work on problems similar to the one you are trying to solve . <eoa> 
how to recognize fields in web pages ? hi , i have got into machine learning recently and i would like to ask advice on a couple of questions : 1. how do you recognize 'fields ' in unstructured web page data ? for example , i have 2,000 web pages about the same topic and i want to to recognize the top 5 fields contained on each page . lets say that my 2k pages dataset is about cars ( taken from the popular car reviewing websites ) . then the output for the 5 fields would be : > > * car manufacturer : ford > * color : blue > * car type : pickup > * engine : 3.0l > * cylinders : 6 > but , for example the field 'number of passengers ' would not enter the 'top 5 ' fields list because , lets say , only 100 pages are talking about it , so , statistically , it is not included . what is the sequence of steps/algorithms to achieve this and what open source package would you recommend me to use ? i have found tools for topic creation and classification , but they seem to be focusing on some specific fields , like name entities , or places , but what i want is to statistically detect the 'top 5 ' fields . 2 second question , if i may , of course : how do you take advantage of already existent knowledge implicitly contained in html tags and logical webpage structure ? for example , the car model , can be already embedded int the 'title ' tag of the html page describing the car . or maybe you do n't need to extract anything because there is already an html table with a lot of fields . how do you extract this knowledge from html ? but , you could not relay on it completely , because every web page will have different html template , so , i suppose , you must first scan the website fully , identify its template and then , extract the data from templates . i am correct ? do you know any tools that already parse html to prepare it as an input for machine learning algorithms ? thank you very much in advance <eoq> found this : https : //www.youtube.com/watch ? v=vincqghqrum maybe i could feed the html directly to the rnn , having such a complex internal logic it may understand the patterns and extract the data i need without any html preprocessing ? <eoa> 
small project on evolutionary algorithms / machine learning i have a small university project concerning evolutionary algorithms in which i will work on the [ ncaa dataset ] ( https : //www.kaggle.com/c/march-machine-learning-mania-2016/data ) that 's part of the current kaggle competition . the idea is : 1. train some ( simple ? ) learning algorithms on the dataset / subsets of the data 2. create an ensemble predictor by weighing the different trained models , so we get a prediction based on all the base models . 3. repeat a lot : use evolutionary methods to find good weights for step 2 . the focus of the project is to try different approaches in step 3 for selection , mutation , reproduction and evaluate which one works best . of course this might be easier on another dataset , but taking part in kaggle competitions is super fun : ) what i am still unsure about is **which base learners i should use** and this is my question that i hope you can help me with . i know that a random forest approach works well in many settings , so simply training a big amount of trees might work . however , since the dataset contains numerical and ordinal data , as well as binary nominal data , i think it should be beneficial to use different learning methods that are able to handle those respective types of data well and combine them . what do you think ? which models should i try to combine ? thanks for your help ! edit : the task is predicting the probability of which basketball team wins a given matchup , so i 'm looking for regression models . edit2 : after looking into this some more , i realised that what i want to do is called bagging . i am probably going to first try a big bag of regression trees ( which i guess would resemble random forest ) . secondly , i will try to combine different base models , so i will try to find good weights for a combination of a regression tree ( or multiple ones ? ) , an svm , maybe mars , maybe ann . if you have any more suggestions on this , i 'd be very happy to receive more suggestions : ) <eoq> if i understand correctly you have to predict some kind of probability , right ? that means you need to do regression ( as opposed to classification ) . regression trees should work fairly well . they should be able to handle both qualitative and quantitative data . maybe [ mars ] ( https : //en.wikipedia.org/wiki/multivariate_adaptive_regression_splines ) ... if you want to use something like a neural network , it deals fairly naturally with numerical data ( although you may want to normalize ) . you can use one-hot encoding for nominal data ( if you have one variable that can take on n classes , create n corresponding input nodes and turn all off except for the one that corresponds to the variable 's current value ) . for ( bounded ) ordinal data you can use a progressive encoding , so if there are n possible values , create n-1 nodes , and if the current value is the third , turn on the first two nodes . ( so if you have temperature which can be cold , warm , or hot , then make 2 nodes and turn them all off for cold , turn the first on for warm and turn both on for hot . ) these encodings might also work for other algorithms . <eoa> 
quick question about my facebook dataset classification trees/random forests/ ... take classification data and result in a binary dependent variable . for example : monday : cloudy , warm and windy . i go and play tennis tuesday : raining , warm and windy . i do n't play tennis wednesday : cloudy , cold and not windy : i go and play tennis . from this i predict whether i will play tennis on a given day . the data i was provided with is also data in classes . it is a facebook data set ( with all data from different facebook profiles ) . the dependent variable is whether a person will like a page or not . some of the variables in the dataset have **multiple** classification values per user per variable . for example : user 3 speaks english , french and dutch . so this gives me 3 values for the language variable ( 3 rows . ) how do i tackle this ? this is the case for a lot of variables in my dataset <eoq> instead of a single `` languages spoken '' variable which can have multiple values , you turn this into multiple variables ( `` english '' , `` french '' , `` dutch '' and more if other people in your data speak other languages ) with binary values . <eoa> 
reframing k-means using neural networks . by all indications , k-means is a numerical method for unsupervised clustering . i 'm looking at doing k-means using neural networks because i believe there would be a speedup in doing so . unfortunately , i 'm having trouble dealing with output representation . for this , self organizing maps were recommended . can someone run-me through self organizing maps ? ( explain it like im 10 ) <eoq> first of all , you should know that k-means and self organizing maps ( soms ) are different things and soms are not just a faster way to calculate a k-means clustering or something like that . the results for small soms will be similar to k-means though . in a som you have a number of nodes/neurons that are organized into a n-dimensional point lattice ( almost always a 2d grid ) . if you have m-dimensional data ( i.e . each data point has m features / values ) , then each node in the som has m weights or parameters . in that sense , these nodes could be compared to the cluster means in k-means . when the network is trained , each new data point is simply associated with the most similar node ( just like in k-means ) . to train the network , you take a data point d and calculate the distance to each node n. usually this just means the euclidian distance . remember those m values that the nodes and data points have ? just subtract the d 's values from n 's values , square each result , sum them together , and take the square root . we will call the closest / most similar node c . now we want to move c , and the nodes that are close to c ( on the grid ) , even closer to n. we want to pull c and nodes that are very close to it very hard in the direction of n , and pull nodes that are further away a bit softer . to do this , we define a similarity function called the `` neighborhood function '' . a similarity function is basically the opposite ( inverse ) of a distance function . if two nodes are the same , it should be 1 , and if they could not be more different , it should be 0. one way to do this is to calculate the euclidean distance ( on the grid , not of their feature vectors ) , plug it into a [ gaussian function ] ( https : //en.wikipedia.org/wiki/gaussian_function ) and take the absolute value ( if it 's negative , multiply by -1 ) . you can start out with a really wide gaussian ( high value of `c` on that wikipedia page ) and make it narrower when you 've been training the som for a long time . now we 're going to pull every\* node n towards the current data point d. the amount by which we change n 's parameters is determined by the difference with d 's features , multiplied by the neighborhood function , multiplied by the current learn rate . the learn rate is a number between 0 and 1 that determines how fast we should pull each node towards the current data point . you want this value to be high when you start training and then decrease over time . you do this for all of the data points in your data set , and then keep repeating that until you are satisfied with the result . what you end up with is not just a mapping from data points to a cluster as in k-means , but also a mapping to a lower-dimensional space ( the m-dimensional point lattice / grid ) . this means you could also use it for dimensionality reduction ( like e.g . principle component analysis ) . <eoa> 
trying to predict my manger 's arrival times . i 've collected data that records when my boss walks through the office e.g . 9am , 9:15am , 9:12am , 9:05am , 9:30am and i 'd like to predict what the next values may be to make sure i 'm at my desk as often as possible during his next walkthroughs . is this just a case of simple linear regression to detect if there is a trend and just extrapolate forward and make sure i 'm at my desk during the average of those times or at least within 1 standard deviation ? is there something more advanced in the machine learning area that can deduce something interesting about this data to help me ? <eoq> what are all factors you recorded ? <eoa> 
trying to predict my manger 's arrival times . i 've collected data that records when my boss walks through the office e.g . 9am , 9:15am , 9:12am , 9:05am , 9:30am and i 'd like to predict what the next values may be to make sure i 'm at my desk as often as possible during his next walkthroughs . is this just a case of simple linear regression to detect if there is a trend and just extrapolate forward and make sure i 'm at my desk during the average of those times or at least within 1 standard deviation ? is there something more advanced in the machine learning area that can deduce something interesting about this data to help me ? <eoq> check the auto correlation and see if there is a potential to use an arma model ( using the minutes after 9 for each day as a time series ) . to get more fancy try random forest regression and add some categorical variables for holidays , long weekends , sports events etc the night before . <eoa> 
trying to predict my manger 's arrival times . i 've collected data that records when my boss walks through the office e.g . 9am , 9:15am , 9:12am , 9:05am , 9:30am and i 'd like to predict what the next values may be to make sure i 'm at my desk as often as possible during his next walkthroughs . is this just a case of simple linear regression to detect if there is a trend and just extrapolate forward and make sure i 'm at my desk during the average of those times or at least within 1 standard deviation ? is there something more advanced in the machine learning area that can deduce something interesting about this data to help me ? <eoq> trying to not get too far into creepville you can use this as an added component . http : //googlegeodevelopers.blogspot.com/2015/11/predicting-future-with-google-maps-apis.html ? m=1 i second adding calendar info ( holidays , day of week , etc ) you can grab public transportation data if you think they might be using buses instead . you 'll then get discrete chunks when the buses general arrive coupled with their walk into your department . however , i think you already narrowed their morning down to a 30min chunk , is n't that good enough ? lol <eoa> 
using ml to determine whether a webpage is an article or not ? does anyone know if this sort of work has been done or not and if there are any good labeled data sets to work with ? any thoughts on where i can look to start approaching this problem would be great . <eoq> if there 's nothing existing , what scraping a news site 's archive , like bloomberg , to get a shit ton of example articles ? then maybe a site like reddit for non-article examples . i bet this takes less than a few hours depending on how much web scraping you 've done , since theres already existing reddit scraping apis <eoa> 
svm kernels are like heuristics ? in the machine learning coursera course , andrew says that different kernels are represented by different similarity functions . he then gives an example of the gaussian kernel which is just a similarity function . so basically different kernels are different similarity functions/heuristics ? <eoq> yes , in a sense . they are dot products that make computations easier and they do look for similarities for all intents and purposes . kernel functions are actually ( or should be ) based on the domain knowledge about how the data should appear . is it linear ? polynomial ? radial ? that 's basically the best option for choosing kernel functions , although there are some automated functions out there now . sharing 3 links i enjoy on kernel functions : [ quora ] ( https : //www.quora.com/what-are-kernels-in-machine-learning-and-svm ) [ great visual representation ] ( http : //www.eric-kim.net/eric-kim-net/posts/1/kernel_trick.html ) [ basically all kernel functions ] ( http : //crsouza.com/2010/03/kernel-functions-for-machine-learning-applications/ ) <eoa> 
how to learn user behavior ? hi , i 'm a absolute beginner in this field and i do n't have a plan to solve my problem ( it is n't a real problem , i 'm only interested in this ) . okay i have a android app with 3 buttons `` a '' , `` b '' and `` c '' . every weekday between 9am and 10am the user clicks on button a . but every weekday between 5pm and 8pm the user clicks on button b . at weekend the user clicks on button c . but on sundays between 10am and 11am the user clicks on a . what i want is that my android app learns this click behavior and reorders the buttons depending on the time . i played a litte bit with apache mahout example [ 1 ] but i 'm not sure if it is the best solution the recommend the right button . [ 1 ] ... https : //mahout.apache.org/users/recommender/userbased-5-minutes.html <eoq> it does n't sound like you need machine learning for this , why not just keep track of what times which buttons are most frequently pressed , and use that ? <eoa> 
question answering system where to start ? assuming i have a raw dataset with thousands of questions and answers , what would be the best way to tackle a system that is smart enough suggest possible answers for similar questions ? are there any libraries or systems that can already do this and that i can build on top ? cheers ! <eoq> you might take a look at the papers we covered in an ai/ml course last term at psu ( melanie mitchell 's class ) . we started by covering recurrent neural networks and lstms . the papers starting nov 12 specifically covered qa systems . http : //web.cecs.pdx.edu/~mm/ils/fall2015/fall2015.html <eoa> 
can someone explain to me or provide me with a practical application of the use of a perceptron learning algorithm ? <eoq> the pla is useful in that it reflects the basic structure of most learning algorithms ... predict , feedback , update ... <eoa> 
multi dimension inputs for nn how can i structure inputs to an nn so that trends within each day are not lost . my inputs are a 2d array for each weekday . y is a single real for each day that is ultimately the bottom right value from that days x. i want to get an updated y as each vector of x ticks by . just banging in the xs one after another , my nn is not learning the trend/function within each day . i 've been experimenting with rearranging features , lambda and the number of hidden units without success . seems i have a bias problem . i suspect i need an rnn or lstm to factor in the memory but i still dont know how to flag each day as a self contained training example ? <eoq> you could do a bi-directional rnn , then every hidden state of the rnn represents a feature in the both the forward and backward contexts , then use these as features for upper level stuff . think you 'd want to do this over your week dimension . this might be overkill though . <eoa> 
should i get into ml ? i 'm not entirely sure how i should proceed and looking for some advice . i 'm an artist who is looking to create a neural network and train it to my taste ( taste , from what i understand , is the way our neurons are wired which are wired based on experience ) and have it search for art for me . i 'd basically train it to art pieces i like and ones i do n't like . i realize this is not an accurate method for pinning down my taste , it 'd be better if it could scan my memories , or even be a neural network modeled on my neural circuit , but these days it seems that 's not possible . art discovery is something i try and do every single day , looking for fine art photography , music , movies , interior design , furniture , architecture , fashion , cooking , etc . i do this because , in a quote `` a child never writes his own alphabet '' - jacque fresco and `` you ca n't exceed your environment . if you give a cannibal a watch , he does n't look at it and say 'the gears are not precise , they are ten thousandth of an inch off ' he does n't say that . it 's impossible . that 's what i mean by you ca n't exceed your environment . you ca n't exceed what you 've been exposed to . '' - jacque fresco so i 'm basically trying to expand my environment by exposing myself to as many different systems as possible . i 've been doing this for 3 years and my work has reached levels i never thought it could as i consistently experienced the dunning-kruger effect . now i 'm wondering if it 's possible i could create a subjective neural network , feed it my favorite films , music , books , etc . with a rating , and my least favorite with a rating . i 'd be basically training it everyday as i discover more art . after that i want to program it to search for art for me , with the intent of finding *new information* on the internet ( this is critical , because it is the whole point ) , that aligns with my taste . art that maybe combines all areas of my taste . if i 'm being honest , the subject of machine learning does not give me the same ecstasy i experience when making a film . which is why i 've come here to ask for advice and guidance . i 'm not sure if it would be better to hire a developer ( after i 've saved up enough money ) to write these for me - or would it be worth the time investment to get into ml on my own and build neural networks from scratch on python ? for the kind of program i want to build , how long do you think it would take ? i looked for programs that build neural networks and only found one so far called simbrain , but it does n't seem ( although i 'm probably wrong ) , it can do what i want . it seems it can only analyze data/numbers . would love some help : / <eoq> hell yeah it 's possible , that 's what the 'recommended for you ' sections of amazon and netflix are doing ! you should totally teach yourself and build something on your own . who knows , maybe the ecstasy will come later . especially after you 've made something that works ! look into recommender systems , that 's what they 're called . also there 's an online course called cs231n on convolutional neural networks that 's used in image recognition . sounds like a fun project . <eoa> 
r or python , which is best for an ml beginner ? i would like to know which language i should use for starting machine learning . from what i can tell most people in the field use r , but i 've also heard that python is being used more and more . i have some experience with python , should i stick with what i know or should i learn r ? <eoq> if you already know python , continue using python . for all intents and purposes , they 're analogous . <eoa> 
r or python , which is best for an ml beginner ? i would like to know which language i should use for starting machine learning . from what i can tell most people in the field use r , but i 've also heard that python is being used more and more . i have some experience with python , should i stick with what i know or should i learn r ? <eoq> > from what i can tell most people in the field use r <eoa> 
r or python , which is best for an ml beginner ? i would like to know which language i should use for starting machine learning . from what i can tell most people in the field use r , but i 've also heard that python is being used more and more . i have some experience with python , should i stick with what i know or should i learn r ? <eoq> python , because it extends better to programming systems beyond where r covers . <eoa> 
r or python , which is best for an ml beginner ? i would like to know which language i should use for starting machine learning . from what i can tell most people in the field use r , but i 've also heard that python is being used more and more . i have some experience with python , should i stick with what i know or should i learn r ? <eoq> focus on the statistical techniques . use what 's easier . <eoa> 
best algorithms for learning on sparse data ? i have a dataset that i have collected with ~ 5000 binary features and less than 1 % of them are 1 's . i did a quick search to see if there are algorithms that are particularly good at working with sparse data and only found stochastic gradient descent . are there any others that are particularly good at working with sparse data ? <eoq> not sure of your goals but recommendation engines/recommender systems are built on this premise . <eoa> 
best algorithms for learning on sparse data ? i have a dataset that i have collected with ~ 5000 binary features and less than 1 % of them are 1 's . i did a quick search to see if there are algorithms that are particularly good at working with sparse data and only found stochastic gradient descent . are there any others that are particularly good at working with sparse data ? <eoq> what are you trying to do ? supervised classification ? if so , then linear models with l1 constraints/penalties ( called 'lasso ' ) have been developed with this problem in mind . comes up frequently in gene expression studies . <eoa> 
non-restricted boltzman machines how are non-restricted boltzman machines trained ? what are they used for ? are they used at all ? <eoq> the maximum likelihood gradient of any boltzmann machine learning breaks down into two terms , the `` positive phase '' and `` negative phase '' . part of the reason that rbms are comparatively tractable is that an unbiased estimate of the positive phase statistics can be had in closed form . the negative phase statistics need to be estimated via sampling ( it is an expectation under the distribution p ( v ) ) , and because you ca n't sample p ( v ) directly , you need to run a markov chain , the most popular method being to do block gibbs sampling of p ( h|v ) and p ( v|h ) alternating . [ deep boltzmann machines ] ( http : //www.cs.toronto.edu/~fritz/absps/dbm.pdf ) are another form of restricted topology where things break down into layers . they can be trained , by the method outlined in that paper ( involving rbm pretraining ) and also jointly all at once in a few different ways . neither the positive phase statistics nor the negative phase statistics are tractable but people have successfully used mean field approximations for the positive phase and monte carlo ( you can do block gibbs sampling by sampling the odd layers given the even layers and vice versa ) . training general , fully-connected boltzmann machines is hard because there 's no block structure you can exploit for efficient sampling . [ this tech report ] ( http : //www.cs.toronto.edu/~rsalakhu/papers/bm.pdf ) outlines a procedure that can apparently train at least some reasonably-sized general boltzmann machines . boltzmann machines ( including rbms ) are n't really used for much anymore ; rbms were used for pretraining layerwise deep neural networks but it 's become crystal clear that they are unnecessary most of the time ( there are a small number of tasks where rbm-pretrained networks still reign supreme for the time being , apparently , notably some large-vocabulary speech recognition tasks i think ) . the exception might be collaborative filtering , i know [ this variant ] ( http : //www.cs.toronto.edu/~rsalakhu/papers/rbmcf.pdf ) is supposedly one of the models in production at netflix . dbms were an active area of research but interest in undirected graphical models for deep learning seems to have dried up . <eoa> 
help understanding hiddeen markov models i 'm a bit confused about inputs and outputs when it comes to hidden markov models . say there is a general problem of some sequence x with some labels y. for example , x can be a sequence of words , and y can be their parts of speech tagging . let 's assume that the alphabet ( state space ) for x is size d and the alphabet of y is size l . say i want to create a hidden markov model ( hmm ) to infer y from x ( i.e. , guess parts of speech from words ) by training a transition matrix a and an observation matrix o. is the observation matrix the probability of seeing an x given a y or the other way around ? is the transition matrix a the normalized transition between the x states or is it between the y states ? do we normalize it for all previous states , or for all current states ? in other words , is the transition matrix a lxl matrix or a dxd matrix ? <eoq> you are talking about learning an hmm with fully observed data . can i refer you to a few pages of kevin murphy 's book ? it 's a simple read through . and the transition matrix will be lxl , the other matrix would be an emission matrix which would be either dxl or lxd . <eoa> 
detect/count persons in picture of a vehicle , what method ? hello ! i am looking for a method to detect/maybe count people in a grayscale-photo of a vehicle . example picture , but with backseat included . we could change the viewangle as we wish : http : //puu.sh/muah3/a5d0e86bcc.jpg there are a lot of available methods but we are unsure of which have the potential to work decently well and are quite “easy” to implement since we are quite new to the area . we are mainly working in c++ or python . we have been looking on some open source libraries like opencv etc . we got a tip that machine learning , random forest , neural networks might work in some extent . there are also a bunch of object detection algorithms like cascade classifiers “traincascade” ( haar , hog or lbp ) . and at last there are a few face detection algorithms ( fisher , eigen or lbh ) . we don’t have the time to test all of the methods and would like to know what you would choose or not choose for the task . <eoq> nice try copper . deep learning is best for image recognition . look at software ( they have turorials ) such as torch , theano , and cafe . <eoa> 
automatically changing regularization level during training ? so i 've been playing with neural nets in lasagne , which outputs the train error/validation error ratio during training . it seems like a very useful way to tell if your nn is suffering from high bias or suffering from overfitting . anyway , i noticed i would often see that the error ratio was too low , then regularize the net with a higher dropout p or weight decay term and start training over . this seems like it could be very easy/beneficial to automate . why not just average the train/val error ratio of the past 10 epochs and change the dropout probability/weight decay term size accordingly ? ( eg if the val error is too much higher than the training error , increase dropout p and l2 regularization size . if the val error is too much lower then training error , decrease dropout p . ) it seems like doing this would help by optimizing the regularization level so the net will never suffer too high bias or too high variance . the main thing i 'm worried about is it will somehow lead to the net sort of over fitting the validation set . i also do n't have nearly enough programming skills to actually implement and test this myself , so can you folks offer any insight ? -is this a good idea , or will it lead to poor results on the test set ? -if it will end up generalizing badly , how/why ? -has something like this already been done ? <eoq> i do n't it 's a bad idea , it might work out . this is somewhat similar to a learning rate schedule , e.g . halve the learning rate when validation loss stops improving . <eoa> 
sparkit-learn random forests ? is it possible to use sparkit-learn to build random forests on a spark cluster ? does anybody have an example of this ? <eoq> sounds like this group tweaked mllib ( unfortunately no syntax ) . https : //spark-summit.org/2014/wp-content/uploads/2014/07/sequoia-forest-random-forest-of-humongous-trees-sung-chung.pdf <eoa> 
can someone explain thresholding an image for me ? im doing an image recognizion task and i think thresholding would work . i need to make a program that detects bright spots on an image and circle them ( active neurons in a calcium probe image ) . im pretty new to these things . thanks . <eoq> at its most basic , if the luminance value of a pixel is above the defined threshold , then set it to white , otherwise set it to black . <eoa> 
can someone explain thresholding an image for me ? im doing an image recognizion task and i think thresholding would work . i need to make a program that detects bright spots on an image and circle them ( active neurons in a calcium probe image ) . im pretty new to these things . thanks . <eoq> typically in neuro , people want to keep activated pixels/voxels above a value or zero out some values . you might also use thresholding to make a mask from an image . if you had a brain or other region that is clearly defined , you can turn it into all 1 's with 0 everywhere else . then any other image that is in register ( aligned anatomically ) can be masked by multiplication . if this is mri or pet , you can use fsl tool 's fslmath to do the thresholding . fslmaths -h has all the options . <eoa> 
what is the difference between a ( dynamic ) bayes network and a hmm ? <eoq> hmm has the markov property of the first order . <eoa> 
format question using scikit attempting to put [ this ] ( https : //archive.ics.uci.edu/ml/machine-learning-databases/magic/magic04.data ) data into scikit & work with it . i am having a bit of an issue seeing how the data correlates to the features specified . can anyone help me ? [ here ] ( https : //archive.ics.uci.edu/ml/datasets/magic+gamma+telescope ) is a link about the data <eoq> i 'm not sure if i understand the question . do you have problems to see what column corresponds to which feature ? in this case the [ pandas ] ( http : //pandas.pydata.org/index.html ) library might be helpful : import pandas as pd features = [ `` flength '' , `` fwidth '' , `` fsize '' , `` fconc '' , `` fconc1 '' , `` fasym '' , `` fm3long '' , `` fm3trans '' , `` falpha '' , `` fdist '' , `` class '' ] df = pd.read_csv ( `` magic04.data '' , header=none , names=features ) df.head ( ) this will output the first few rows/datapoints of your data set with the header names as defined in the features list . i also recommend using [ jupyter notebook ] ( http : //jupyter.org/ ) for just trying around with datasets . the pandas library outputs datapoints in a neat little table , which makes it a bit easier to see what you are dealing with . if i misunderstood the question , please clarify : ) <eoa> 
x-post from /r/machinelearning : question about tensorflow/cifar10 hi guys , i 'm currently learning tensorflow . i 'm working on the cifar-10 tutorial . i 'm a bit confused and wanted to check whether i 'm understanding correctly how i would prepare data myself to train the model . here 's what i think i should do , i 'd appreciate any feedback : first , convert the images to a numpy array of shape height x length x channels ( 3 for rgb image ) . i 'm unsure what i 'd do with the labels , actually ... let 's say i call the final file foobar then , i could use read_cifar10 ( foobar ) to read in the image in my code . in the code , the individual images are then arranged in batches the way tensorflow wants all to receive them . is this correct ? if i want to read in more than 1 image , what will i write instead of foobar , let 's say the files are called foobar1 and 2 ? <eoq> for each batch , you 'll want to run tf.train.youroptimizerofchoice ( your-learning-rate ) .minimize ( your-cost ) the cost function google uses in their examples is cross entropy , in the form of tf.reduce_sum ( your_correct_labels * tf.log ( your_algorithm's_output ) ) <eoa> 
machine learning uses the same few equations over and over the more papers and ml books that i read , the more i see the same equations repeated over and over , only with slightly different names . i 'm wondering if the machine learning literature is overly complicated by what is essentially duplication with obsfucation ~~to keep out those who are afraid to wade through the math.~~ edit : maybe i was little steamed when i wrote that . : ) <eoq> cite a few examples ? <eoa> 
machine learning uses the same few equations over and over the more papers and ml books that i read , the more i see the same equations repeated over and over , only with slightly different names . i 'm wondering if the machine learning literature is overly complicated by what is essentially duplication with obsfucation ~~to keep out those who are afraid to wade through the math.~~ edit : maybe i was little steamed when i wrote that . : ) <eoq> i would also like to see some examples . i 'm imagining that you 're describing something like the fact that the linear regression equations appear over and over again - they do ! the basic concept of a linear model is the intuitive jumping off-point for just about every supervised learning model . the fact that this is a pattern we can exploit is wonderful and for some people ( myself included ) very counterintuitive . we use these over and over again because they are well-studied models . i find it difficult to believe any claims that it 's simply a matter of spiteful obfuscation on the part of authors , though . there are many alternative hypotheses that fit the data here , such as : 1 ) the same equations appear over and over again , but the variations between each usage are hard enough to systematize that we have yet to find an overarching system which totally describes all of them . 2 ) most papers are written by experts in the field , who have seen these equations and patterns before . when they write their papers , they purposefully reuse ideas and equations , both because it gives them a theoretical base to build on and a formalism that their audience will already be familiar with . note that in this case , it 's the opposite of your claim - the various authors actually go out of their way to present the material in a familiar way , knowing that it would aid understanding to do so . moreover , why on earth would someone spend months or years of their life researching something , only to publish a paper or a book which is intentionally obscure ? <eoa> 
if glm performs better than gbm or rf , what does that mean ? hi , i was asked this question in interview and i admit i 'm not sure about my answer . what would be yours ? > if glm performs better than gbm or rf , what does that mean ? i think the answer expected was something about the data , and maybe a `` solution '' . cheers note : - glm : logistic regression - gbm : gradient boosted trees - rf : random forest ( of trees ) <eoq> google is your friend my man ... https : //www.quora.com/what-are-the-advantages-of-logistic-regression-over-decision-trees but the short version is it means there is a clear linear separation in the logistic regression via slope , whereas the data is not conducive to a clear split for decision trees which ( for ease of explanation ) are parallel to each axis . <eoa> 
[ ufldl ] [ tensorflow ] can i consider my sparse autoencoder implementation a success ? trying to follow the ufldl tutorial with tensorflow as my tool . i 'm on lesson 1 . [ here is my nicest looking output . ] ( https : //i.imgur.com/zw9ept0.jpg ) i am concerned that i found splotches rather than edges . should i be ? or is the sample image of the tutorial an unrealistic expectation for how these filters go ? unfortunately , i had to do two things differently than andrew ng 's sample code to achieve these results . **i used 50k sample patches instead of 10k . ** trained with gradient descent instead of l-bfgs , so that could explain why i need more . by contrast , [ here is a 10k sample . ] ( https : //imgur.com/rftetgz ) it has several filters that look like random noise . **my learned filters are incredibly high contrast ! ** the above samples were first put through my normalizedata function ( which is exactly the same function used in sampleimages in the exercise ) before being sent to the visualizer . [ here is an unnormalized image ! ] ( https : //imgur.com/wrk8qnt ) gross ! my first instinct tells me this is an issue with my regularization of the cost function . the exercise has a lambda of .0001. increasing this one-thousand fold to .1 [ produces something a tad better ] ( https : //i.imgur.com/n2r7sjs.png ) . perhaps this means i am using the l2_loss function of tensorflow incorrectly ? here is that line of code : cost_reg = n_lambda * ( tf.nn.l2_loss ( weights [ 'hidden ' ] ) + tf.nn.l2_loss ( weights [ 'out ' ] ) ) finally , for tensorflow , adamoptimizer did absolutely nothing for me , leaving me with completely random filters in the end . is there a reason replacing gradientdescentoptimizer ( learningrate=learningrate ) with adamoptimizer ( ) should n't work for me ? [ here is the code on github ] ( https : //github.com/gosp/testingcenter/blob/b9b2cab17dcec8c90375b0bd29bc239719585111/testtensorflowsparceautoencoder.py ) **edit** one issue i 've found so far : i was not activating my output with sigmoid . now i do n't need to normalize my data ! but i 'm also no longer learning anything that looks like structure ... back to square one ... <eoq> i 've got a working solution based on your code . hope it helps . https : //github.com/trackbully/ufldl_tensorflow/blob/master/sae.py <eoa> 
which is your favorite tool for machine learning and predictive modelling when working with r/python ? some examples would be : r - caret , h2o , etc . python - scikit-learn , pybrain , etc . <eoq> numpy <eoa> 
interpreting the results of lstm-based recurrent networks does any of you know , if any research has been done interpreting the results of lstm-based recurrent networks and linking them back to the features that were the input to the model ? <eoq> visualizing and understanding recurrent networks , http : //arxiv.org/abs/1506.02078 <eoa> 
best starting place for beginners ? i 'm 15 and starting to learn about machine learning , i am currently read artificial intelligence a modern approach but after that what should i do ? <eoq> i 'm by no means an expert , but learning python , or another similar language , so that you can actually implement the stuff they talk about in the books is probably a safe bet . <eoa> 
best starting place for beginners ? i 'm 15 and starting to learn about machine learning , i am currently read artificial intelligence a modern approach but after that what should i do ? <eoq> you 're 15 and already understand stochastic calculus , linear algebra , and optimization theory ? impressive . <eoa> 
are n't features more important than any particular algorithm or ml method ? hi all , i just finished my first neural network ( ng 's coursera assignment , so as basic as possible , but still quite cool to me ) , and i was curious about how these can be scaled when it comes to object recognition , etc . this assignment is digit recognition on a 20x20 image , a mere 400-element input array . but real-world images are exponentially bigger than this . so my question is , if individual pixels are n't used as inputs , how do features get extracted from images for use in neural networks ? it seems to me that learning how to develop quality features is a better investment of my time while trying to learn ml than diving deep into the particular algorithms . is there any validity to that ? for example , i learned of josh tenenbaum 's name from the ml : a probabilistic perspective , and on his front page there 's an intro of how children learn to distinguish a horse from just a handful of examples ( which seems so common sense to me , but in the perspective of machine learning is quite remarkable ) . but is the problem that there 's no algorithm that can adequately generalize after such few training points , or is it that the features being fed to these algorithms do n't encapsulate enough information to allow such few training points ? i really appreciate your time and any insight you can give me . cheers <eoq> for image work and neural networks , they often use convolutional neural networks ( cnns ) to find features that are combinations of individual pixels . as the cnns are trained to find `` useful '' features , you get things like `` vertical edge '' or `` diagonal edge '' . these then become the features used by subsequent neural network layers . i 'm not an expert - i 've just read about this in one of my classes . <eoa> 
what are some good resources for someone looking to delve deeper into neural networks after having taken an introductory undergraduate course in ml . i am familiar with feed forward networks and how they work and am even taking a graduate course on neural nets this coming spring semester but would like to do some reading in the mean time . the thing is , a vast majority of the posts/papers i read over on /r/machinelearning about the topic are way over my head . does anyone have any suggestions ? thanks ! <eoq> if you 're explicitly interested in neural networks then i 'd recommend geoffrey hinton 's ( one of the pioneers in deep learning ) course on coursera which goes through a lot of the important topics relating to nns . <eoa> 
what are some good resources for someone looking to delve deeper into neural networks after having taken an introductory undergraduate course in ml . i am familiar with feed forward networks and how they work and am even taking a graduate course on neural nets this coming spring semester but would like to do some reading in the mean time . the thing is , a vast majority of the posts/papers i read over on /r/machinelearning about the topic are way over my head . does anyone have any suggestions ? thanks ! <eoq> https : //see.stanford.edu/course/cs229 <eoa> 
looking for free offline resource to learn machine learning and prerequisite knowledge can anyone point me to any ? i 'm new to ml , i have a basic understanding of stats or probability , and i wanted to be able to download it for offline reading . i 'd prefer something in the epub or mobi format , but pdf works as well . <eoq> scikit learn and ipython notebooks <eoa> 
looking for free offline resource to learn machine learning and prerequisite knowledge can anyone point me to any ? i 'm new to ml , i have a basic understanding of stats or probability , and i wanted to be able to download it for offline reading . i 'd prefer something in the epub or mobi format , but pdf works as well . <eoq> i already bookmarked [ this ] ( https : //redd.it/1jeawf ) reddit post to read at my leisure . great place to start ( i 'm new to ml as well ) , but i would also recommend coursera.org ml courses to any beginner . <eoa> 
looking for free offline resource to learn machine learning and prerequisite knowledge can anyone point me to any ? i 'm new to ml , i have a basic understanding of stats or probability , and i wanted to be able to download it for offline reading . i 'd prefer something in the epub or mobi format , but pdf works as well . <eoq> elements of statistical learning is a good book . <eoa> 
rnns as generative models i was going through [ ilya sutskever 's thesis ] ( http : //www.cs.utoronto.ca/~ilya/pubs/ilya_sutskever_phd_thesis.pdf ) and in section 2.5.2 he states `` an rnn defines a generative model over sequences **if** the loss function satisfies l ( z^^t ; y^^t ) = -log ( p ( y^^t ; z^^t ) ) for some parameterized family of distributions p ( · ; z ) and y^^t = v^^ ( t+1 ) `` ( emphasis mine ) does this mean that the rnn will not act as a generative model if we use , let 's say , the squared loss function ? <eoq> -log ( p ( y^t ; z^t ) ) is the `` negative log likelihood '' . it does not specify a specific loss function ( maybe you missread and thought that it is cross-entropy ? ) it is also a generative model if the loss function satisfies l ( z^t ; y^t ) = - p ( y^t ; z^t ) for ... ( adding the log does n't change anything , as log is monotonous and increasing function ) . maybe related to your question : we write the mean squared error without the log because it does n't help . cross entropy has exponential terms so there are computational issues because of limited precisions of floats , right ? but squared loss is a sum so there is n't such a problem . please , correct me if i 'm wrong . <eoa> 
how do you keep track of the progress of a machine learning project ? i am talking about what features and algorithms you use , their performance etc . <eoq> best way is often to use r markdown language ( or whatever software you use ) . you have the syntax and output but you can also write a paragraph opener and closer in each section that will describe what you did and why and the final results . <eoa> 
do i need a phd ? is a phd required to do work in machine learning ? <eoq> no . <eoa> 
do i need a phd ? is a phd required to do work in machine learning ? <eoq> absolutely not , if you do n't have an upper level degree indicating you learned the foundation and have a project to show for it ( thesis , dissertation ) , etc . your next best option is to create projects with definitive results to show your competence and understanding . i am part of a machine learning meetup type group . about every month or two there are industry hirees looking for people to bring into their businesses to solve a machine learning problem . they essentially ask who in the group had done a similar problem then dig into that project . if they feel the individual or individuals can tackle it they get hired for the project or the job . a good chunk of the financial guys are either still in undergrad , have graduates in a completely different field , or have nothing at all in paper degrees , but have a huge amount of high results in online ml financial challenges . unless your looking for grants or something that essentially has a clause indicating the pi needs to have a phd your good to go on your merits of previous results . i 've seen this same thing happen in python groups . someone will ask what you 're working on or present an open job/project available and as long as people know what you 're working you 'll be steered towards each other . i personally feel this is true in all of computer science . simply because a degree is n't always indicative of being a problem solver of novel questions . ( sure a degree helps , but this industry knows that it 's not the only way , unlike say practicing medicine on a human . ) hirees for hard questions simply want results not a person in a cubicle . <eoa> 
trying to predict user behavior . achieved some results , stuck on choosing a better model / input . total newbie here . using sklearn . so i have an web app with a restful server . this means i have data about the user approximately in this form , i 'm using reddit urls just to give you an idea : user 123 requested /r/machinelearning at 15:43:32 user 123 requested /r/machinelearning/top at 15:44:15 user 123 requested /r/mlquestions at 15:45:56 user 123 requested /r/mlquestions/submit ? selftext=true at 15:47:01 ... etc . basically , i have the user 's request log of the web app . what i 'm trying to achieve is classify whether the user adopts the app or not . there 's a 30 day trial period where you can use the app for free and at the end of this period you 'll have to start paying for the app in order to continue using it . on my first try i disregarded the urls completely and just fed in the number of requests per a day . so i might feed a vector like this to the ml algorithm : [ 24 , 10 , 0 , 15 ] so 24 actions taken on the first day , 10 actions on the second day , etc . with 14 days , i achieved very close to 90 % accuracy . this was with a balanced set , so the roc auc score was also very close to 0.9. i just tried a bunch of classifiers from sklearn and chose the best one . random forest and a simple linear logistic regression performed the best . i 'm now trying to do the same thing again , but only looking at the first hour of usage . i 'm also trying not to disregard the different categories fo actions . however , this has made the input matrix really huge and very sparse , etc for user 123 you 'll have something like this : [ [ 1 , 2 , 2 , 1 , 0 , 0 , ... , 0 ] , # actions performed within the 1st minute [ 0 , 0 , 0 , 0 , 0 , 2 , ... , 0 ] , # actions performed within the 2nd minute ... [ 0 , 0 , 0 , 0 , 0 , 0 , ... , 0 ] # actions performes within the 60th minute ] i thought i might feed each of the minutes to an unsupervised clustering algorithm for dimensionality rediction , to end up with a vector like this : [ 1 , 2 , 0 , 0 , 0 , 5 , ... , 0 ] where the numbers , hopefully , represent types of user behavior the clustering algorithm found . i just tried this using k-means and meanshift . meanshift did find lots of clusters , probably around 1500 different clusters . it 's a huge number , so maybe i 'm doing something wrong . i do n't thing new users can do 1500 different general things within the first hour of usage ... i could also try to reduce the number of different requests by just taking the first category , etc /r/me_irl and /r/me_irl/top would both be just /r/me_irl just so that i could reduce the dimensions of 1 minute . okay so my current plan is to feed these clusters to a classifier . the thing is , the whole thing is a time series , so would another algorithm be more suitable for this ? i 've heard lstm is good for time series . am i heading in the right direction or am i doing something stupid ? <eoq> just saw this paper `` session-based recommendations with recurrent neural networks '' : http : //arxiv.org/pdf/1511.06939v2.pdf . i have n't read it yet . maybe it helps to solve your problem . <eoa> 
hardware ? i 'm new to the ml game . starting on kaggle competitions . thinking about getting a new computer . what hardware will make the biggest difference ? not looking to break the bank , just some guidelines i.e . minimum sys requirements , amp up the speed , bang for my buck kinda stuff , etc . <eoq> the biggest speedup will come from a video card that supports cuda ( i.e . nvidia ) . i recently swapped my r9 280x for a 970 to utilize theano 's gpu support , and the speedup is considerable . <eoa> 
hardware ? i 'm new to the ml game . starting on kaggle competitions . thinking about getting a new computer . what hardware will make the biggest difference ? not looking to break the bank , just some guidelines i.e . minimum sys requirements , amp up the speed , bang for my buck kinda stuff , etc . <eoq> all major open source ml libraries support cuda hardware acceleration . so something that supports an nvidia card . correct implementation took some of my compute times down from 2 hours to 5 minutes . <eoa> 
reshaping in tensorflow mnist tutorial ? greetings , i 'm trying to follow [ tensorflow 's expert_mnist tutorial ] ( https : //www.tensorflow.org/versions/master/tutorials/mnist/pros/index.html ) , and i can not understand the role of reshaping . can someone briefly explain why and how is it done ? ( i 'm also a little confused by the negative value in the first place , what does that mean ? ) . the code in question is ... x_image = tf.reshape ( x , [ -1,28,28,1 ] ) <eoq> a matrix is a vector of vector of values . reshaping turns it into a single large vector of values . then turns it into a matrix of vectors again of whatever dimensions you want it in . simplest application for this is vec-ing a matrix to calculate a jacobian or a hessian <eoa> 
will be possible to develop a learning maching to try to predict tennis matches with rapidminer ? hi , i 've got tons of data with a lot of info about tennis matches of the last 5 years . i would like to create a machine learning and train it with the data from 2011-2014 and then test it with 2015 data . is this possible ? the info in the datasets is a little confusing . is this possible with rapidminer ? thanks . <eoq> yes and yes . split the data into training and test sets ( splitting by years may affect your tests predictability ) . run neural nets , random forests , etc . profit . <eoa> 
will be possible to develop a learning maching to try to predict tennis matches with rapidminer ? hi , i 've got tons of data with a lot of info about tennis matches of the last 5 years . i would like to create a machine learning and train it with the data from 2011-2014 and then test it with 2015 data . is this possible ? the info in the datasets is a little confusing . is this possible with rapidminer ? thanks . <eoq> you might look at `` analyzing baseball data with r '' by max marchi . different sport , but the principles are the same . <eoa> 
will be possible to develop a learning maching to try to predict tennis matches with rapidminer ? hi , i 've got tons of data with a lot of info about tennis matches of the last 5 years . i would like to create a machine learning and train it with the data from 2011-2014 and then test it with 2015 data . is this possible ? the info in the datasets is a little confusing . is this possible with rapidminer ? thanks . <eoq> at the most naive level , you might just look at each player and determine a mean and standard deviation for the number of points they score . then to predict the outcome of a game , for each player , randomly sample from a normal distribution using the mean and sd you found for each player . the one with the most points wins . maybe you do that an odd number of times , and choose the winner from who won the most simulated games . this is essentially a monte-carlo simulation . you could get more sophisticated by using more of your variables in your data to predict a number of points . you could even try to get more clever if your data can help you model when they 'll score in the game and their probabilities of scoring points given things like ... `` late in the game , and behind '' , etc . the possibilities are endless . you might also try to find ways to classify players based on your data ... good on clay vs grass , aggressive , consistent , good defender , etc . these would then be factors to help refine your prediction of their performance . but i 'd go with starting simple and adding complexity as you go . i 'd probably look at some simple regressions to see how well your variables predict points or outcomes . <eoa> 
advice request : using ml for neuroscience research , do i need to abandon matlab ... hi all ! i 'm a behavioral neuroscience researcher in the field of electrophysiology , meaning most of my work involves recording and anything signals from one or more neurons in order to determine their function . one of the common problems in my field is decoding the activity of neurons that are suspected to be encoding information about something , without any firm idea of how they 're converting that information . the information can be encoded in so many different ways ... by the timing of a spikes relative to an external triggering event , by the timecourse of the rate of spiking , by a precise temporal pattern of spikes , by the timing of spikes relative to the phase of a particular neural oscillation ... it is common in my field to use machine learning as a way to show that a particular type of neuron encodes information in a particular way , by using the neural activity as inputs to a classifier ( and the different types of events / stimuli preceding or following that activity as the target categories ) . the underlying implicit assumption is that if a ml classifier can identify the stimulus using that information alone , then there 's a decent chance that the brain areas that receive that information are doing so as well . i started off learning matlab for data analysis reasons . i had no real programming experience ( just writing scripts for data analysis software , or to control stimulus delivery systems ) , had no idea what convolution was , and had no experience working with matrices and vectors and whatnot . i 've become reasonably competent in matlab ( i think ) , and found the various ml tools in matlab to be helpful for data analysis . but i have run into a number of difficulties . my issues are : difficulty with circular data ( phase angles ) using feedforward nets and/or svms.. i have been just providing inputs as the sine and cosine of the angles , but i wish i could just use complex inputs . difficulty training classifiers when the differentiability of the classes is not known ( e.g . there are ten classes , but it is quite possible that the inputs only contain enough information to discriminate the samples into 1/2/3/4/5 , 6/7/8 , and 9/10 ) ; classifiers often get hung up trying to minimize errors instead of maximizing information . i have tried to get around this by using evolutionary algorithms to train anns , using mutual information ( or normalized variation of information ) between network outputs and targets as the fitness function . difficulty figuring out how to get matlab 's training algorithms to use my division of data into training and validation sets . uncertainty about how to determine the best type of classifier for my data.. this is more of a 'me problem ' than a matlab problem . uncertainty about how to expand my analyses to a broader range of input types : i have so far usually just narrowed my data down to some number of phase angles and then used those as inputs , but i might wish to do analyses in which the input is a continuous signal alongside a discrete event signal , with the goal being to identify periods in the signal during which certain events are occurring . things i do n't need to do : image recognition gigantic super-complicated models so , that 's my situation . i could use advice on whether it 's worth it for me to abandon matlab in favor of something more flexible . i could also use general advice about how to solve any of the issues i 've described . any help is appreciated ! and if you want to know more , just ask . <eoq> what do your fellow researchers use ? do you need to share code ? <eoa> 
what is the difference between convolution and correlation . why do cnn 's use convolution ? <eoq> correlation indicates a relationship between two variables . convolution is a product operation that combines two distributions ( or series ) to create a new distribution . in the context of a time series , the convolution operation combines the past information of two time series to create a new series . in the context of a cnn , at each time step the network contains the history of all past inputs and activations . in this way it is a generalized version of the convolution operation . *edit : i just realized that i confused cnn and rnn . : ( must have been on crazy pills . <eoa> 
guide to implementing rnn hi ! i want to build a simple rnn with matlab to learn reading text . i have tried to understand how to implement it in code , but i would love to see if there are good guides out there that gives exercises in implementing the code ! any tips would be great : ) <eoq> sorry if i 'm misinterpreting what you are saying , but , if you are n't implementing it to learn about nns but rather to do a specific task , i 'd really suggest not implementing your own nns . <eoa> 
guide to implementing rnn hi ! i want to build a simple rnn with matlab to learn reading text . i have tried to understand how to implement it in code , but i would love to see if there are good guides out there that gives exercises in implementing the code ! any tips would be great : ) <eoq> this is a very good post on rnn 's helped me understand them alot better and has some good code samples , though not in matlab . http : //karpathy.github.io/2015/05/21/rnn-effectiveness/ <eoa> 
want to focus on the financial application of machine learning , looking for good study material i 've already completed the coursera machine learning course offered by stanford so i believe i have a good understanding of the basics of machine learning . i want to now focus on the financial applications of machine learning , but i want to make sure i pick the right material to study . does anybody have any recommendations ? i am primarily interested in the prediction of stock prices . thank you . <eoq> it 's worth doing some finance related course to get more of an understanding of things from that angle . this coursera one is quite good https : //www.coursera.org/learn/financial-engineering-1/home/welcome <eoa> 
i want to make a program that will help me hunt for apartments . is ml the right tool and where should i start ? i have this project to write some kind of app ( i 'm more proficient in ruby and js ) that would eventually be able to sift through apartments for rent ads and send me the relevant ones . the way i see it is that i 'd feed the program ads and then tell it whether this particular listing is of interest to me or not and hope the machine will learn to eventually be able to do the sorting on its own . is this something that can be achieved with ml ? are there any frameworks that are better suited to this ? i am a developer but i do n't know much about statistics or maths and i 've never worked on anything related to ml . any pointers ? thanks ! <eoq> it just sounds like a job for regular expressions . anything more is overkill . <eoa> 
i want to make a program that will help me hunt for apartments . is ml the right tool and where should i start ? i have this project to write some kind of app ( i 'm more proficient in ruby and js ) that would eventually be able to sift through apartments for rent ads and send me the relevant ones . the way i see it is that i 'd feed the program ads and then tell it whether this particular listing is of interest to me or not and hope the machine will learn to eventually be able to do the sorting on its own . is this something that can be achieved with ml ? are there any frameworks that are better suited to this ? i am a developer but i do n't know much about statistics or maths and i 've never worked on anything related to ml . any pointers ? thanks ! <eoq> machine learning can help you figure out how good of a deal a particular listing is , and if you want to get even more advanced then go ahead and try to train an algorithm that could even figure out if a given posting is too good to be true and is probably a scam . <eoa> 
i want to make a program that will help me hunt for apartments . is ml the right tool and where should i start ? i have this project to write some kind of app ( i 'm more proficient in ruby and js ) that would eventually be able to sift through apartments for rent ads and send me the relevant ones . the way i see it is that i 'd feed the program ads and then tell it whether this particular listing is of interest to me or not and hope the machine will learn to eventually be able to do the sorting on its own . is this something that can be achieved with ml ? are there any frameworks that are better suited to this ? i am a developer but i do n't know much about statistics or maths and i 've never worked on anything related to ml . any pointers ? thanks ! <eoq> i am planning to do the same thing for job hunts.. let 's do it together ? i have ml and nlp knowledge , wan na work together ? <eoa> 
deep learning simplified : episode 5 - an old problem while deep neural nets are the state of the art in machine learning , the flipside is that they are really hard to train . up until 2006 , there was no way to train them satisfactorily . here is a clip that explains further . https : //www.youtube.com/watch ? v=skmpmaoua2q <eoq> this one is on the vanishing gradient . enjoy : - ) <eoa> 
trouble of using vgg for super-resolution i 'm trying to implement the net in [ accurate image super-resolution using very deep convolutional networks ] ( http : //arxiv.org/abs/1511.04587 ) . it 's inspired by vgg : the only difference is that there are no pooling layers , so all intermediate weight layers have 64 channels . i followed the details of the paper 1. gradient clipping 2. learning rate = 0.1 , decreases by 10 every `k` epochs 3. glorot-like weight initialization : `stddev= ( 2/ ( 3*3*64 ) ) ^0.5` , where filter width = 3 , number of input channels = 64 4. momentum = 0.9 , l2 regularization = 1e-4 in tensorboard , i 'm keeping track of the weights/biases histograms , and it seems after a certain time , the weights stop changing . this may mean the gradient vanished , or we 've settled on a bad local minimum . however , i am getting performance worse off than bicubic interpolation . in the paper , they get better performance even in the 1st epoch . i was wondering why is this happening ? <eoq> initialize the weight matrix of the last convolution , the one that results in the residuals , with zeros . it also helps to use ycbcr instead of rgb . <eoa> 
deep learning simplified : episode 4 - how to choose deep learning as a field has developed quite a bit in the last decade , and we now have a variety of models to pick from with new models and improvements arriving frequently . the flip side to this is , the burden of choice now falls on you to figure out which model to pick for what application . here is a clip that gives you some guidelines to help you decide . https : //www.youtube.com/watch ? v=jjzdoojyzxq <eoq> some simple rules of thumb on how to pick a deep net . enjoy : - ) <eoa> 
looking for em derivations exercises with solutions hi , i am looking for a good source of exercises of derivations of the em algorithm ( possibly also other inference methods , such as variational bayes ) with solutions . ideally this would contain an assortment of different graphical models on which to do the derivations of the loglikelihood , the e and m steps . any pointers ? <eoq> * [ em ] ( http : //www.david-andrzejewski.com/publications/em.pdf ) * [ variational inference in 5 minutes ] ( http : //davmre.github.io/inference/2015/11/13/elbo-in-5min/ ) * [ general purpose variantional inference ] ( http : //davmre.github.io/inference/2015/11/13/general_purpose_variational_inference/ ) * [ auto-encoding variational bayes ] ( http : //arxiv.org/abs/1312.6114v10 ) <eoa> 
deep learning simplified : youtube series hi everyone ! i am new to this sub-reddit and wanted to introduce myself . i have been working on a youtube series for deep learning that you may like . if you are ever need to explain deep learning to a newbie ( or are new to deep learning yourselves ) , you may like this series . content you 'll typically find online on the topic is highly mathematical/technical , which is great ! but if you 're like me , you probably want to just understand the models and the intuition . thats what this series is about ! here is the link to the series intro . please take a look and let me know what you think ! https : //www.youtube.com/watch ? v=b99uvkwzytq <eoq> this is the series intro - 6 total videos so far and many more to come . enjoy : - ) ! <eoa> 
deep learning simplified : youtube series hi everyone ! i am new to this sub-reddit and wanted to introduce myself . i have been working on a youtube series for deep learning that you may like . if you are ever need to explain deep learning to a newbie ( or are new to deep learning yourselves ) , you may like this series . content you 'll typically find online on the topic is highly mathematical/technical , which is great ! but if you 're like me , you probably want to just understand the models and the intuition . thats what this series is about ! here is the link to the series intro . please take a look and let me know what you think ! https : //www.youtube.com/watch ? v=b99uvkwzytq <eoq> very nice ! i watched the intro video . the production quality is great and i like the narrator 's voice . i look forward to watching the rest . thanks for doing this ! <eoa> 
deep learning simplified : youtube series hi everyone ! i am new to this sub-reddit and wanted to introduce myself . i have been working on a youtube series for deep learning that you may like . if you are ever need to explain deep learning to a newbie ( or are new to deep learning yourselves ) , you may like this series . content you 'll typically find online on the topic is highly mathematical/technical , which is great ! but if you 're like me , you probably want to just understand the models and the intuition . thats what this series is about ! here is the link to the series intro . please take a look and let me know what you think ! https : //www.youtube.com/watch ? v=b99uvkwzytq <eoq> i found that really useful ! thanks very much . <eoa> 
correct cost function to use with a softmax output layer with a continuous target distribution ? lets say i am training a neural network to play rock , paper , scissors . the network outputs a probability distribution over the three actions . i am using a softmax as the final layer of the network to ensure that p_rock + p_paper + p_scissors = 1.0 . i understand ( i think.. ) that if my networks target output was discrete , ie ( 1,0,0 ) , ( 0,1,0 ) or ( 0,0,1 ) , i should use cross-entropy as my cost function . my question is what cost function should i use to train the network if my required target output is continuous , for example ( 1./3 , 1./3 , 1./3 ) ? i have tried mean squared error , but it does not converge . i do n't know if i have a bug , or i am using the wrong cost function . is mean square error or cross-entropy appropriate in this case ? would anyone be kind enough to point me in the right direction ? thanks in advance ... <eoq> output of your network is still discrete , it is classification problem , but you will never get full one and zeros , because softmax is giving you probabilities that it should be that output . if you want result you need to sample from that distribution . so you should use cross-entropy . <eoa> 
weird error message when tuning svm with polynomial kernel : `` warning : reaching max number of iterations '' it is my first time working with support vector machines . i am trying to solve this homework , but am receiving the above mentioned error ... here is my code : library ( e1071 ) test_data = # upload test data here . training_data= read.table ( 'digits_training.csv ' , sep = ' , ' , header = true ) y = training_data $ y chosen_svm = function ( y , training_data , kernel_name ) { obj < - tune.svm ( y~. , data = training_data , gamma = 10^ ( -3:1 ) , cost = 10^ ( -3:1 ) , kernel = kernel_name ) gamma = obj $ best.parameters $ gamma cost = obj $ best.parameters $ cost model = svm ( y~. , data = training_data , gamma = gamma , cost = cost , kernel = kernel_name ) return ( model ) } radial_svm = chosen_svm ( y , training_data , 'radial ' ) lin_svm = chosen_svm ( y , training_data , 'linear ' ) pol_svm = chosen_svm ( y , training_data , 'polynomial ' ) any idea why this is happening ? <eoq> i am a bit busy to check but i would guess that svm requires convergence and it was failing to converge . <eoa> 
what effect does loss function have on training ? i understand it is only used to observe progress , ie . it does not really have any fingers in how weights are updated ? if so , why is the choice of loss function important ? the network will converge to what it will converge anyway ... a slightly related question about training criterion , ie . error for the output layer , from what i 've seen sometimes its just target - output sometimes its ( target - output ) *deriv ( output ) is it chosen independently of loss function ? <eoq> false alarm , figured it out <eoa> 
given a list of centroids , how to find optimal set of length k ? think this is an easy one i 'm having a brain fart about . given some dataset where each observation x has a pre-calculated distance to n centroids . if i know i want to separate the entire set into k centroids , how can i pick which k ? or more concretely , lets say i have 1000 rocks and each rock has a set of 1000 distances to some centroid representing a feature ( maybe color , likelihood to be found in a yard , etc ) and i know i want to cut the whole set of rocks into 5 centroids ( either a rock is closest to the yard centroid or to the color centroid ) . how do i find the 5 centroids which will minimize my distance to the whole set of rocks ? i feel like there 's some kind of reverse-knn i 'm just not thinking of . <eoq> [ k-means clustering ] ( https : //en.wikipedia.org/wiki/k-means_clustering ) . <eoa> 
need help with understanding how to compute the weight gradient in a convolutional layer . hi , so i have a have a hard time understanding how to compute the gradient for the convolutional layer . to test my understanding i have constructed a simple convolutional network featuring one convolutional layer , one maxpool layer and a fully connected output layer . the whole network looks like this : http : //i.imgur.com/gryuktu.jpg so when applying backpropegation computing the output delta is simple , but i have some uncertainties when computing the other deltas . for the maxpool layer most instructions will say to simply `` repeat '' the errors from the previous layer since this layer does not do any learning . so for delta^3 i compute a matrix with the error values from delta^4 for the pooled indices and 0 everywhere else . so for the delta for the convolutional layer ( delta^2 ) i guess one only computes the hadamard product between d/dz and delta^3 ? . when computing the gradient for the weights i furthermore compute the convolution between the input ( i ) and delta^2 . using this gradient does not yield usefull results ( i.e reduce the error ) when updating the weights , so i am obviously doing something wrong . how i compute the gradient is shown in this picture : http : //i.imgur.com/6lcr7v4.jpg basically my question is what is the correct way of obtaining the weight gradient in this example . <eoq> figured it out , to compute delta^2 you ofcourse compute it thusly = upsamle ( w^t delta^3 ) hadamard ( sigma ' ( z ) ) . where delta^3 = delta^4 . <eoa> 
[ help ] supervised classification implementation this is my frist ml project , so i am trying to build a really simple web analytics tool . i want to use apache web logs to determine whether or not to issue a coupon to a customer . the web logs contain ip address and browsing history , so i can determine what pages a customer went to and whether or not they bought the product . using this data as a training set , i hope to build a model that can report to me who i should issue new coupons to , in order to improve sales . my question is : how can i implement this ? any details i can get will be really helpful . i already have a java program that can take the web logs and split them into individual pieces of data . how do i get this data into a form that is acceptable , and how do i make the program ? its hard for me to find tutorials that are low-level and implementation focused . thanks ! <eoq> let 's get the data extraction sorted out . what you want is the pattern of a customer 's browsing before he/she makes a buy/does n't ever make a buy ( no buy for next 4 weeks ) . so your data is only limited to ppl making a buy . can you see how to extract this data ? now you have a time series data ( do n't be scared , fancy way of saying simple stuff ) of buying patterns . split into training and test . i guess a hidden markov network would do good here . the observed data is the page being browsed , and hidden state is probability to buy . learning this model is straightforward ( tractable ) . read kevin murphy 's section on learning a partially observed hmm . you can limit the length of the hmm to a value based on the amount of data and performance on test set . smaller lenght may give less test performance , longer hmm might not have enough data to train on . feel free to ask follow-ups : ) <eoa> 
the typical 25 horse problem with a twist the typical interview question : you have 25 horses and want to identify the 3 fastest horses . you have a track that can hold 5 horses at a time . what is the minimum number of races it would take for you to identify the 3 fastest horses . twist : same problem , except now you want to identify the 6 fastest horses . edit : i forgot to mention , the times are not recorded so you can not just find the speed of each horse individually and compare . ex : for the top 3 horses , the answer is 7 races to display this , i will show label the horses with a letter and a number . the letter refers to their first race set and the number refers to their order for the first race . race 1|race 2|race 3|race 4|race 5 : -- | : -- | : -- | : -- | : -- a1|b1|c1|d1|e1 a2|b2|c2|d2|e2 a3|b3|c3|de|e3 a4|b4|c4|d4|e4 a5|b5|c5|d5|e5 we can then find the fastest horse by comparing the best in each of the first 5 : race 6|na : -- | : -- a1|na b1|na c1|na d1|na e1|na lets say that horse a1 , b1 , and c1 are the fastest , showing up in their respective orders ( i.e . a1 is the fastest ) . then to identify the 2nd and 3rd fastest , we race the set of candidates : race 7|na : -- | : -- a2|na a3|na b1|na b2|na c1|na this will give us the 3 fastest horses . <eoq> 5 for both assuming the horses are in random order and you do not have any information on them . since 5 horses x 5 horses per track is 25 and you want to cover each horse exactly once because you do not have any prior information on them . then you sort the results by speed . any additional races can bias the results because horses used multiple times can be fatigued . <eoa> 
machine learning on paths ? hello everyone , i 'm currently a student working on a projet with a [ database ] ( https : //snap.stanford.edu/data/wikispeedia.html ) from wikispeedia > ( from their website ) > > you are given two wikipedia articles starting from the first article , your goal is to reach the second one , exclusively by following links in the articles you encounter . i 'd like to train an algorithm to guess the shortest path to go from one article to another , knowing only the categories ( e.g historical figure , geography , japanese videogames ... ) the idea is that computing every possible path from one point to another is simply impossible on such a large graph in a reasonable amount of time , so i want to use machine learning to have a satisfying solution in a relatively short amount of time . i 'd like to say that i 'm a novice un machine learning , although i already completed a few projects , but i never worked on graphs before : do you have an idea where i could find some ideas on what kind of algorithms to use for my project ? ( if it can help , i code in python and used scikit-learn for my previous projects ) thank for your help <eoq> this seems similar to ( not exactly ) how google maps would give you a route from one end of us to another.. ( edit ) the difference being that in maps you have a measure of closeness to the destination ( euclidean distance ) , in this problem it 's not that obvious . my take at it ( multiple ideas ) extract a graph from wikipedia , the articles are the nodes and the links are directed edges . then , this article is a good start : https : //en.wikipedia.org/wiki/pathfinding 1 ) a* search from source to destination . you 'll have to figure out the heuristics to use . we can discuss further on this . 2 ) by breaking down the problem into subparts ( sub-graphs ) based on major topic in wikipedia ( or other clustering metric ) . you can run single source/all pairs shortest path on all subparts and then join the outputs ( details need to be figured out ) . this would be useful if you want shortest paths from all articles to all other . the graph you construct would be sparse , that property could also be used . at the least storage should be done by an array of linkedlists . computational gains might also be needed to look into . <eoa> 
help with a deep convolution network i 'm building a deep convolution neural network on my own with c++ and am getting stuck . how does the convolution layers update its weights ? i understand how the hidden layer and the output layer 's weights are updated , but i ca n't think of how to update the convolution filters . can anyone explain how to update the convolution layer , provide an algorithm , or resources where i can learn more about this ? oh , for some context , the goal of my network is to learn to play breakout . with some hand coded filters i was able to get it to bounce the ball 2-3 times , but not that reliably . <eoq> it 's same as in the dnn . [ cs231n ] ( http : //cs231n.github.io ) would be a good start to check it out , or see how people have implemented it in frameworks e.g . keras . <eoa> 
designing a random forest from scratch , with no ml libraries allowed . hi all , not a programmer but i 'm taking a machine learning module i 've been asked to design a rf from scratch . i am lost . i have basic skills in python - very basic . can anyone help ? so far i 've scraped a decision tree from the internet but i 'm a bit lost on it as its telling me my features in the function argument are n't defined . my attempt at the code below . import numpy import csv with open ( 'banks.csv ' , 'rt ' , encoding='ascii ' ) as csvfile : # type name of datafile with extension in first set of apostrophes . data = csv.reader ( csvfile , delimiter= ' ' , quotechar='| ' ) # for row in data : # print ( ' , '.join ( row ) ) # prints out all data in file def newdt ( data , features , targetclass , fitness_func ) : # define a new decision tree data=data [ : ] values= [ record [ targetclass ] for record in data ] empty=majorvalue ( data , targetclass ) features= ( data [ :0 ] ) if not data or ( len ( features ) -1 ) < =0 : return empty elif values.count ( values [ 0 ] ) ==len ( values ) : return values ( 0 ) else : best=choose_features ( data , features , targetclass , fitness_func ) tree= { best : { } } for value in get_values ( data , best ) : subtree=newdt ( get_examples ( data , best , value ) , [ attr for attr in features if attr ! = best ] , targetclass , fitness_func ) tree= [ best ] [ value ] =subtree return tree newdt ( data , features , targetclass , fitness_func ) <eoq> i 'm a bot , *bleep* , *bloop* . someone has linked to this thread from another place on reddit : <eoa> 
designing a random forest from scratch , with no ml libraries allowed . hi all , not a programmer but i 'm taking a machine learning module i 've been asked to design a rf from scratch . i am lost . i have basic skills in python - very basic . can anyone help ? so far i 've scraped a decision tree from the internet but i 'm a bit lost on it as its telling me my features in the function argument are n't defined . my attempt at the code below . import numpy import csv with open ( 'banks.csv ' , 'rt ' , encoding='ascii ' ) as csvfile : # type name of datafile with extension in first set of apostrophes . data = csv.reader ( csvfile , delimiter= ' ' , quotechar='| ' ) # for row in data : # print ( ' , '.join ( row ) ) # prints out all data in file def newdt ( data , features , targetclass , fitness_func ) : # define a new decision tree data=data [ : ] values= [ record [ targetclass ] for record in data ] empty=majorvalue ( data , targetclass ) features= ( data [ :0 ] ) if not data or ( len ( features ) -1 ) < =0 : return empty elif values.count ( values [ 0 ] ) ==len ( values ) : return values ( 0 ) else : best=choose_features ( data , features , targetclass , fitness_func ) tree= { best : { } } for value in get_values ( data , best ) : subtree=newdt ( get_examples ( data , best , value ) , [ attr for attr in features if attr ! = best ] , targetclass , fitness_func ) tree= [ best ] [ value ] =subtree return tree newdt ( data , features , targetclass , fitness_func ) <eoq> hi ! once you implement a decision tree you are almost there , just build n of them , predict your validation set through all of them and for each input vector take the mean ( regression ) or the mode ( classification ) . if you can share your banks.csv file i will take a look at it tomorrow . plus a link to where you found the script . <eoa> 
how do i describe a perceptron , comparing two inputs x/y triggering when x > y . i tried to setup a perceptron witch outputs an 1 when output x is bigger than y. how can i realise this ? like wtf ? <eoq> if i recall perceptron well , this python code should describe such perceptron : <eoa> 
how do i describe a perceptron , comparing two inputs x/y triggering when x > y . i tried to setup a perceptron witch outputs an 1 when output x is bigger than y. how can i realise this ? like wtf ? <eoq> y is an input ? are there only 2 inputs ? are you going to use back propagation for training ? <eoa> 
does changing contrast , brightness , etc in data set increase the accuracy of the trained net ? i have a data set of around 15000 images , if i modify these images by changing their properties such as contrast , brightness , rotation ect to create a larger data set will the accuracy of my net be greater ? very new to this ! <eoq> depending on what your images are , it can . i remember someone who won a kaggle competition on identifying galaxies ( ? ) did a write-up on his processes , and part of it was stretching the images in different ways to create a more generalized dataset edit : http : //benanne.github.io/2014/04/05/galaxy-zoo.html <eoa> 
simple projects to begin with ? hello ! i 'm in a research program at my school and choose to learn about ml . currently i 'm learning algebra ii/trig , so my math education is not nearly enough to easily understand ml . i 've been working at a decent pace and i currently understand gradient descent thanks to coursera and andrew ng , however suddenly my teacher requested 10 pages of original research done in a very short amount of time ( monday , 11/27/15 ) . are there any basic data sets and techniques i can use to at least get credit for my research thus far ? ml seems to be a very deep topic you learn about over time , and i currently do n't have time . i was thinking about trying to use some modified gradient descent algorithms on data sets , but my issues currently are : i do n't know what kind of data i need/where to get it , i do n't know how to make a gradient descent algorithm work for more than 2 parameters ( linear regression ) . any advice would be appreciated . i 'm sorry for asking this question , i know it 's very `` how do i learn ml quick '' style , but really i just need a simple project to pass and continue learning at a normal rate . <eoq> andrew ng 's intro to ml is really good and i 'm not sure how you would go about learning ml much faster than that . i think i would recommend binging a bit on his lectures so that you get a bit beyond linear regression with 2 parameters . if i recall correctly , there are assignments in this course , right ? maybe you can use that in your report ( maybe extend the exercises a bit ) . i do n't really know anything interesting to do with 2 parameter linear regression . you can fit a line through a bunch of data points . if you learn to use more parameters , you can make things slightly more interesting by comparing the lines you get with various numbers of parameters . if you learn about logistic regression , you can start doing classification/detection . you could make `` networks '' that simulate and and or logic gates . for xor you need to use a nonlinear technique since it 's not linearly separable . for instance , a neural network with a hidden layer ( i.e . a multilayer perceptron ) . one fairly small but interesting problem is classifying the [ mnist ] ( http : //yann.lecun.com/exdb/mnist/ ) dataset of handwritten digits . however , this is quite a bit beyond linear regression with 2 parameters . you could also take a look at some of the challenges on [ hackerrack ] ( https : //www.hackerrank.com/domains/ai/machine-learning ) . good luck ! btw , i hope you meant monday the 30th and not friday the 27th ( today/yesterday depending on your time zone ) . ( monday the 27th does n't exist . ) <eoa> 
can two machine-learning computers be identital ? hi , i have a question that 's been eating me for some time and do n't know where else to post it . so here it is : assuming that two computers ( m1 & m2 ) are fed the exact-same training data , would they behave identically , to the point where we can accurately assess/predict the behaviour of m2 based on m1 . i guess i 'm trying to find out whether it is impossible , due to inherent differences in the hardware ( not the exact same chip , even if from same manufacturer ) akin to a genetical difference . it 's a bit like the rhetorical `` if two humans grew up in the *exact* same environmnent , would they behave in the same way '' , though i would assume it 's technically impossible to do such a test . if anyone could shed some answer i would appreciate it , thank you : ) peace <eoq> i 'm not sure if i understood the question correctly . <eoa> 
can two machine-learning computers be identital ? hi , i have a question that 's been eating me for some time and do n't know where else to post it . so here it is : assuming that two computers ( m1 & m2 ) are fed the exact-same training data , would they behave identically , to the point where we can accurately assess/predict the behaviour of m2 based on m1 . i guess i 'm trying to find out whether it is impossible , due to inherent differences in the hardware ( not the exact same chip , even if from same manufacturer ) akin to a genetical difference . it 's a bit like the rhetorical `` if two humans grew up in the *exact* same environmnent , would they behave in the same way '' , though i would assume it 's technically impossible to do such a test . if anyone could shed some answer i would appreciate it , thank you : ) peace <eoq> you mean would their roundoff error be the same for the same algorithms ? <eoa> 
can two machine-learning computers be identital ? hi , i have a question that 's been eating me for some time and do n't know where else to post it . so here it is : assuming that two computers ( m1 & m2 ) are fed the exact-same training data , would they behave identically , to the point where we can accurately assess/predict the behaviour of m2 based on m1 . i guess i 'm trying to find out whether it is impossible , due to inherent differences in the hardware ( not the exact same chip , even if from same manufacturer ) akin to a genetical difference . it 's a bit like the rhetorical `` if two humans grew up in the *exact* same environmnent , would they behave in the same way '' , though i would assume it 's technically impossible to do such a test . if anyone could shed some answer i would appreciate it , thank you : ) peace <eoq> the hardware differences will not affect your algorithm in any way ( other than how quickly it trains of course ) . <eoa> 
can two machine-learning computers be identital ? hi , i have a question that 's been eating me for some time and do n't know where else to post it . so here it is : assuming that two computers ( m1 & m2 ) are fed the exact-same training data , would they behave identically , to the point where we can accurately assess/predict the behaviour of m2 based on m1 . i guess i 'm trying to find out whether it is impossible , due to inherent differences in the hardware ( not the exact same chip , even if from same manufacturer ) akin to a genetical difference . it 's a bit like the rhetorical `` if two humans grew up in the *exact* same environmnent , would they behave in the same way '' , though i would assume it 's technically impossible to do such a test . if anyone could shed some answer i would appreciate it , thank you : ) peace <eoq> i think it depends if the algorithm used is deterministic or not . if it uses randomness in some way , then there 's no way to guarantee that the results will be exactly same . <eoa> 
tf as individual chatbot what happens if you feed in your whole chat-history of facebook to the tensorflow-framework as sequence to sequence model whereas the target-output it the text i wrote and the input the text my facebookfriends wrote . will the result of the training be a chatbot which is using language like me ? <eoq> i doubt it . the biggest challenge in nlp is actually understanding the context , not just following the grammatical rules of humans . also , i imagine the training data is not going to be large enough since i assume most of the conversations are unique and largely depends on the situation . of course it will output in the language of yours , but it wo n't effectively answer the question . <eoa> 
question about types of neural nets hey all ! so i have one main question about the different types of neural nets that exist . it seems like a lot of different types of nns fall under the umbrella term of neural net and i was wondering what kinds there are . i 'm familiar with a cnn , an rnn and so forth . do those employ backpropogation ? the only one i 've had experience with has been the one as done in the andrew ng coursera lectures ( feedforward with backprop- not sure if this is its own type of neural net or if this is just a general term ) i 'm also aware that many different types of cost functions exist ? does this actually change the type of net or is this really just a different way to characterize `` cost '' ? <eoq> those are the 3 main types of nets that you will usually see . also , there are variations of these ( lstm , gru , etc are types of rnns ) . <eoa> 
question about types of neural nets hey all ! so i have one main question about the different types of neural nets that exist . it seems like a lot of different types of nns fall under the umbrella term of neural net and i was wondering what kinds there are . i 'm familiar with a cnn , an rnn and so forth . do those employ backpropogation ? the only one i 've had experience with has been the one as done in the andrew ng coursera lectures ( feedforward with backprop- not sure if this is its own type of neural net or if this is just a general term ) i 'm also aware that many different types of cost functions exist ? does this actually change the type of net or is this really just a different way to characterize `` cost '' ? <eoq> there are certainly many types of neural networks . check out the wikipedia page on [ recurrent neural networks ] ( https : //en.wikipedia.org/wiki/recurrent_neural_network ) . the complement of rnns are [ feedforward nns ] ( https : //en.wikipedia.org/wiki/feedforward_neural_network ) . this is one of the more clear-cut distinctions since ffnns are directed acyclic graphs and rnns contain at least one cycle . however , i would say that a recurrent mlp is more similar to a feedforward multilayer perceptron than it is to a hopfield network ( which is also recurrent ) . basically you can come up with all kinds of variations by adding , omitting , sharing and fixing connections or changing some activation functions . it 's not always clear when a change constitutes a new `` type '' of network . what most of these networks have in common is the the node 's activation is given by an activation function applied to the weighted sum of other nodes ' activations , although some nodes in lstms compute the product . this is different in [ spiking neural networks ] ( https : //en.wikipedia.org/wiki/spiking_neural_network ) , but they are rarely used since they are hard to train . i would say [ boltzmann machines ] ( https : //en.wikipedia.org/wiki/boltzmann_machine ) are also quite different . most of the time some form of backpropagation is used . i think backpropagation technically refers to a fairly specific algorithm , and people often use variants like rprop or quickprop instead , but it 's still all about propagating an error back throughout the network . things like momentum and dropout and other training/regularization tricks can be added . it 's also possible to use other optimization techniques ( like genetic algorithms ) . hebbian learning ( increase weight between nodes that are simultaneously active ) is not used much anymore i think . ( restricted ) boltzmann machines have their own training procedure that uses gibbs sampling and gradient descent . i would say that the used cost function is probably more part of the training procedure than it is of the network . finally , i should mention that there are also computational neuroscience projects that basically aim to actually simulate parts of the brain ( like ibm 's blue brain project ) . these are technically also neural networks , but they are also pretty different . <eoa> 
sklearn pca not making sense to me . i 'm under the impression that if you have a 2d data set and you perform pca on it , without any dimensionality reduction it will essentially rotate your data to a new coordinate system . why then when i attempt this in sklearn does the data seem like its being transformed in some way ? here is my code : import numpy as np from sklearn.decomposition import pca import matplotlib.pyplot as plt x = np.linspace ( 0,10,101 ) y = x + 2*np.random.randn ( 1 , 101 ) y = y [ 0 ] data = np.asarray ( zip ( x , y ) ) pca = pca ( n_components=2 ) new_data = pca.fit_transform ( data ) plt.figure ( 0 ) plt.scatter ( data [ : ,0 ] , data [ : , 1 ] ) plt.figure ( 1 ) plt.scatter ( new_data [ : ,0 ] , new_data [ : , 1 ] ) plt.show ( ) <eoq> i think it may just be appearing that way because of the different scales . when i look at it the corresponding points seem to be in the correct spot . add colors = range ( 101 ) , and pass in c=colors to scatter ( ) and you 'll see it more easily . <eoa> 
reinforcement learning help ! im learning reinforcement learning . can someone provide me with some example problems which i can try out and improve my understanding . thank you ! <eoq> [ rlpy ] ( http : //mloss.org/software/view/514/ ) is an open source framework for performing sequential decision making experiments in python . it has a bunch of [ example domains ] ( https : //github.com/rlpy/rlpy/tree/master/examples ) . <eoa> 
reinforcement learning help ! im learning reinforcement learning . can someone provide me with some example problems which i can try out and improve my understanding . thank you ! <eoq> in [ the book by sutton & barto ] ( https : //webdocs.cs.ualberta.ca/~sutton/book/ebook/the-book.html ) there are sometimes scenarios used in the chapters , as well as some case studies in the end , which you can use to create some own problems . did you mean something like this ? <eoa> 
ml newbie : how to approach this problem , given a screenshot , find areas of interest or interaction . suggestions ? **the problem ( in detail ) : ** given a screenshot , i want to be able to detect areas where a user would typically interact - whether that is through clicking or typing . i would want my output to be somewhat similar to an ocr pipeline . **approach 1 - ocr like document analysis : ** use some kind of modified version of [ document layout analysis ] ( https : //en.wikipedia.org/wiki/document_layout_analysis ) more suited to this problem . **the problem : ** i really have no idea how to even begin here . **approach 2 - attempt to learn areas of interest : ** my idea would be to download a bunch of web pages , screenshot them , and use that as training data because areas of interaction can be identified fairly easily in a webpage ( as either an input , anchor , or button tag ... it does n't get everything , but may be good enough ) . **the problem : ** my hypothesis is that something like a neural net would not be able to distinguish interactive and non-interactive elements with a high enough accuracy . another problem , even if i think it could be accurate , how could i leverage this as a classification problem to be useful to me ? i.e . my output would just tell me whether or not that screenshot contains an interactive element ? i 'm looking at this the wrong way but i ca n't see another approach . i 'm definitely lost , so any pointers would be greatly appreciated ! <eoq> you are like 90 % of the way to a complete solution . it looks like what you really need is a classifier that , given an image , produces bounding boxes around the interactive elements . happily algorithms to both detect ( decide if an entity exists in the image ) and locate ( draw bounding boxes around the entity ) are quite common in computer vision . its called `` object detection '' or `` object recognition '' , and its a common enough thing that you could find tutorials about online . see the pascal voc challenge for more cutting edge algorithms . i am not an expert in this field but if you dig around you should be able find implementations of such algorithms , including ones that use deep learning and ones that do n't . as you say , you could then imagine downloading webpages , identifying the interactive elements by parsing the html ( in practice this might be the hard part ) , and then using that as training data for your classifier . since you have potentially unlimited training data , and i would guess that this is a difficult but not extremely difficult classification problem , my guess is that that approach could achieve very good accuracy . note , of course , it is liable to only work for screen shots of webpages , not screen shots in general , since that is where what your training data coming from . <eoa> 
unsupervised learning with theano/cgt/tensorflow i have a huge amount of respect for computational graph frameworks , and would love to start working with them more , but most of what i want to do is unsupervised or generative . i 've not been able to figure out what kind of loss function to use or how to utilize them to accomplish what i want , so i 've always been relegated to writing my own libraries . a google search yields a few pages from deeplearning.net wherein they define an rbm loss function , but that 's really about it . the rest are just links to frameworks . i realize the universal trend is to do unsupervised pre-training on the bottom layers of the network and slap a fc network on top , but i really want to do unsupervised all the way to the top . are there good resources or tutorials on using any of these frameworks ? any advice for writing loss functions there ? edit : or am i relegated to only using rbms/auto-encoders with graph frameworks ? <eoq> [ deleted ] <eoa> 
are nonlinearities needed if you add bias at each layer ? from what i understand the stacked weight matrices are no longer reducible to one matrix if you have bias at each layer ... yes or no , what am i missing here ? btw , does the same apply if you have normalization at each layer during training and forward pass ? <eoq> let 's say you have a mlp with two inputs ( x1 and x2 ) and two neurons : neuron 1 : w11*x1 + w12*x2 + b1 neuron 2 : w21*x1 + w22*x2 + b2 output : w1*neuron1 + w2*neuron2 + b = w1* ( w11*x1 + w12*x2 + b1 ) + w2* ( w21*x1 + w22*x2 + b2 ) + b = a*x1 + b*x2 + c ( the formulas for a , b and c are left for the reader , but they do not depend on the inputs x1 and x2 ) . clearly the output is linear with regard to the inputs , the only thing we 've accomplished is a convoluted way to determine the linear coefficients a , b and c , which would make the *training* nonlinear . so the answer is no , biases do not add any nonlinearity to the network , activation functions are definitely required . what do you mean by normalization at each layer ? batch normalization ? <eoa> 
[ career question ? ] help with preparing for an interview ? `` desired qualifications '' . *experience with machine learning or pattern recognition algorithms *strong experience with conducting machine learning experiments . this includes writing and evaluating machine learning algorithms *excellent programming skills in either python ( preferred ) , java or c++ *enrolled in undergraduate or graduate studies in cs , ee or related disciplines *experience with data analytics 1 ) any recommendations on books/ sections i should know in and out ? 2 ) should i focus on those things or focus on typical `` software engineering '' questions ? like big-o of search/ sort algorithms , writing data structures and all that . <eoq> > any recommendations on books/ sections i should know in and out ? you should probably read the [ docs ] ( http : //numenta.org/ # docs ) that numenta has made available about nupic and htm . i would probably focus here , because apparently they use this . all of the other desired qualifications are `` experience with xyz '' , which you ca n't really get in a short amount of time . i mean , you could take some intro to ml courses on coursera , edx or udacity , but it sounds like that will not be advanced enough for this job . i like christopher bishop 's [ pattern recognition and machine learning ] ( http : //research.microsoft.com/en-us/um/people/cmbishop/prml/ ) book , but it 's also not something you breeze through . it 's probably a lot more doable to read the [ wikipedia page ] ( https : //en.wikipedia.org/wiki/machine_learning ) and make sure that you at least know what they 're talking about with all of the approaches . i would probably try to figure out some specific things about the company . are they doing computer vision ? then read a little bit about that . are they focusing on facial recognition ( i 'm just making stuff up here ) ? then so should you . what kind of algorithms do they use aside from nupic ? > should i focus on those things or focus on typical `` software engineering '' questions ? like big-o of search/ sort algorithms , writing data structures and all that . judging from the desired qualifications i probably would n't *focus* on this . they seem to care much more about ml in particular than they do about more general software engineering and computer science stuff . i 'd worry more about knowing various ml algorithms than the traditional cs fare . it 's good to know the basics though . learning the complexity ( big-o ) of these algorithms and data structure operations should not take too much time , so it might be worthwhile . <eoa> 
need help in audio signal processing so i am implementing a project on determining a birds species from its song ( wav file ) i figured [ this method ] ( http : //hearinghealthmatters.org/waynesworld/2012/animal-vocalization-analysis-the-gplp/ ) can be tried out ... i need help on implementing it on python ... i 'd give anything to anyone who gets it xd <eoq> you might want to check out [ librosa ] ( https : //github.com/bmcfee/librosa ) . it 's a python library that takes care of the audio processing ( by way of another library [ audioread ] ( https : //github.com/sampsyo/audioread ) ) and also implements many of the common audio features . check out the tutorial . librosa makes it trivial to get a [ spectrogram ] ( http : //bmcfee.github.io/librosa/generated/librosa.core.stft.html ) and transform it into the [ cepstral domain ] ( http : //bmcfee.github.io/librosa/generated/librosa.feature.melspectrogram.html ) . the filter bank mentioned sounds similar to a mel filter bank which is included in librosa . i did n't know we had equal-loudness curves for birds , that 's awesome . if you have that it should n't be too hard to apply it to the spectrogram , once you 're familiar with the matrix representation . hopeufully this helps . good luck ! <eoa> 
after 30 hours , i still ca n't figure out how to properly implement a backpropagation algorithm . help please . i spent nearly 30 hours thinking , reading , and trying to implement this . everything is correct . i checked them a hundred times , so there is no problem in the logic as far as i understand it . below are the issues i 'm facing . 1- so i want to approximate a function ( x^2 , sinx , ... ) using a neural network , but how do i make this work ? i have one input neuron , variable hidden neurons , and one output . how would my network ever produce the correct outputs if the sigmoid function has a range from 0 to 1 ? 2- my network ca n't even figure out the xor problem with 2 hidden neurons , two inputs , and one output . but the forward pass works if i manually set the weights and use a threshold activation function for both hidden and output layers . 3- it always produces the same output for any input . <eoq> code for weight correction in the hidden layer ( modified for clarity ) , where i suspect it 's wrong : <eoa> 
after 30 hours , i still ca n't figure out how to properly implement a backpropagation algorithm . help please . i spent nearly 30 hours thinking , reading , and trying to implement this . everything is correct . i checked them a hundred times , so there is no problem in the logic as far as i understand it . below are the issues i 'm facing . 1- so i want to approximate a function ( x^2 , sinx , ... ) using a neural network , but how do i make this work ? i have one input neuron , variable hidden neurons , and one output . how would my network ever produce the correct outputs if the sigmoid function has a range from 0 to 1 ? 2- my network ca n't even figure out the xor problem with 2 hidden neurons , two inputs , and one output . but the forward pass works if i manually set the weights and use a threshold activation function for both hidden and output layers . 3- it always produces the same output for any input . <eoq> please , can someone at least give me the correct weights and biases for a 2-2-1 network that uses sigmoid ( 1/ ( 1+e^ ( -x ) ) for all neurons ? it 's driving me insane . i just want to know where the error is . <eoa> 
after 30 hours , i still ca n't figure out how to properly implement a backpropagation algorithm . help please . i spent nearly 30 hours thinking , reading , and trying to implement this . everything is correct . i checked them a hundred times , so there is no problem in the logic as far as i understand it . below are the issues i 'm facing . 1- so i want to approximate a function ( x^2 , sinx , ... ) using a neural network , but how do i make this work ? i have one input neuron , variable hidden neurons , and one output . how would my network ever produce the correct outputs if the sigmoid function has a range from 0 to 1 ? 2- my network ca n't even figure out the xor problem with 2 hidden neurons , two inputs , and one output . but the forward pass works if i manually set the weights and use a threshold activation function for both hidden and output layers . 3- it always produces the same output for any input . <eoq> 1 - you just give up sigmoid for the output layer . keep sigmoid for hidden units only . <eoa> 
after 30 hours , i still ca n't figure out how to properly implement a backpropagation algorithm . help please . i spent nearly 30 hours thinking , reading , and trying to implement this . everything is correct . i checked them a hundred times , so there is no problem in the logic as far as i understand it . below are the issues i 'm facing . 1- so i want to approximate a function ( x^2 , sinx , ... ) using a neural network , but how do i make this work ? i have one input neuron , variable hidden neurons , and one output . how would my network ever produce the correct outputs if the sigmoid function has a range from 0 to 1 ? 2- my network ca n't even figure out the xor problem with 2 hidden neurons , two inputs , and one output . but the forward pass works if i manually set the weights and use a threshold activation function for both hidden and output layers . 3- it always produces the same output for any input . <eoq> on mobile so please forgive smelling errors . i 'll try to give a more thorough answer when i get to my laptop . in the meantime : 1 : you can either try linear output nodes instead of sigmoid , or just scale your answers by some constant that allows signal outputs to generate answers in the range you want . stick to approximately 0.3 to 0.8 , not 0 ... 1 . 2 : a single hidden layer ffnn with two hidden nodes can solve xor , but backpropagation is n't guaranteed to find that answer . try more nodes and different initialization . do not initialize weights to 0 . <eoa> 
[ work question ] what does the workday of someone working in machine learning look like ? what do you love the most about your job ? what is your least favorite thing ? what steps have you taken to get to your position ? what are personal qualities that are necessary to succeed in machine learning ? if you could give a piece of advice to a young person seeking a career in machine learning , what advice would you give ? <eoq> i would n't say i every really worked in ml , but i have had work where i made heavy use of ml . currently i 'm a phd student in ai , which means that i 'm mostly just reading a lot of articles and then occasionally implementing some algorithms to try them out , but ml is n't my focus . i used to work for a small computer vision company where i worked on custom software for clients , which included stuff like facial expression recognition , video surveillance , body pose estimation , object recognition/detection/tracking and behavior analysis . when a project started we would have some meetings about what the client wanted , what we expected to be able to deliver , and how we might be able to tackle the challenges . we would usually have some initial ideas of what techniques could be used , after which i would dive into the scientific literature about those techniques ( if it was my project ) . then at several points in time i would present my ideas and progress to the rest of the team , who would then give feedback . also at different points in time the project would get redefined in several ways because the client wanted something different , or some algorithm turned out to really not work , or we adjusted our expectations of what was possible , etc . i probably spent most of my time programming . after deciding what techniques to use , they needed to be implemented . you can read about these things in scientific papers , but it 's ( almost ) never quite what you need , so you have to adapt it to your specific situation and problem , and usually this would involve cobbling together multiple ml algorithms . that also means that you have multiple points of failure , and that you need to train a bunch of separate algorithms . a lot of time was also spent gathering or producing data , training the algorithms and then testing them . this often involved creating separate annotation and training and training tools , and then actually doing the annotations . sometimes we could tell the client what kind of setup they should use ( i.e . number and kind of cameras and computers ) , which would mean researching what was best/cheap and building the setup . otherwise , i occasionally visited some clients in order to get data from their actual setups , or i 'd try to recreate locally it to get data that was somewhat similar . once i had all of that , i could bring it all together in some piece of software for the client . even if you have all of the ideas , this still takes quite a bit of time . if you have a whole pipeline of algorithms , it can be difficult to get them to work together , and there is always something that is inaccurate or too slow , which means you might have to replace it or gather more data/retrain it or change the setup ( if that 's an option ) . finally , you need some nice way of visualizing the results for the client , and the program needs to actually be usable by other people who are not necessarily experts . this is usually pretty easy compared to the other things , but it can still take a lot of time . aside from that it 's just regular company and miscellaneous stuff . meetings , some administration and reporting , creating tools for internal use , trying to upgrade the dev stack , updating/upgrading the website , learning new stuff , organizing and attending events , and generally just having a nice time with coworkers ... <eoa> 
in machine learning is it necessary to take software engineering ? there 's a statistical and machine learning major which i 'm going into but i would like to pair it with either a language technology minor or a software engineering minor . right now i 'm not sure which would come in more useful in the field . <eoq> when you say `` the field '' , what field do you mean ? software engineering ( or at least programming ) is going to be relevant for all machine learning , no matter what direction you go . in fact , it is so relevant that i expect your major will already include quite a bit of it . i do n't know too much about language technology minors , but it sounds like it might be useful if you want to do natural language processing . if you want to do nlp , or want to learn something more `` unique '' , then you should probably that language technology minor ( you 'll have to learn to program anyhow ) . if you just want to do ml in general , then software engineering is the most obvious choice . <eoa> 
can i use machine learning to recognize professional photography ( vs amateur photography ) ? i 'm a complete beginner and have no experience with machine learning , i was wondering if it would be possible for me to train some kind of neural network to recognize professional photography reliably . an example of photos that should rate very high as professional : https : //www.flickr.com/photos/steamster/sets/72157656521743602 what kind of a dataset would i need to achieve this ? and how much machine learning do i need to learn to implement it ? ( i currently know a decent amount of basic programming in python and java ) <eoq> so actually , you 're going to need to use a neural network because you 're dealing with complex non-linear hypothesis . the theory is similar , but the the implementation is different . i ca n't help there yet . <eoa> 
can i use machine learning to recognize professional photography ( vs amateur photography ) ? i 'm a complete beginner and have no experience with machine learning , i was wondering if it would be possible for me to train some kind of neural network to recognize professional photography reliably . an example of photos that should rate very high as professional : https : //www.flickr.com/photos/steamster/sets/72157656521743602 what kind of a dataset would i need to achieve this ? and how much machine learning do i need to learn to implement it ? ( i currently know a decent amount of basic programming in python and java ) <eoq> i 'd say this is a pretty big problem . should be possible but might need some month or even years of research . <eoa> 
can i use machine learning to recognize professional photography ( vs amateur photography ) ? i 'm a complete beginner and have no experience with machine learning , i was wondering if it would be possible for me to train some kind of neural network to recognize professional photography reliably . an example of photos that should rate very high as professional : https : //www.flickr.com/photos/steamster/sets/72157656521743602 what kind of a dataset would i need to achieve this ? and how much machine learning do i need to learn to implement it ? ( i currently know a decent amount of basic programming in python and java ) <eoq> what you have to first understand is modelability vs model choice . <eoa> 
can i use machine learning to recognize professional photography ( vs amateur photography ) ? i 'm a complete beginner and have no experience with machine learning , i was wondering if it would be possible for me to train some kind of neural network to recognize professional photography reliably . an example of photos that should rate very high as professional : https : //www.flickr.com/photos/steamster/sets/72157656521743602 what kind of a dataset would i need to achieve this ? and how much machine learning do i need to learn to implement it ? ( i currently know a decent amount of basic programming in python and java ) <eoq> yes you could . there 's something called a classification algorithm , or logistic regression . fancy words that just mean `` is it probably this or is it probably that '' ? the more training examples you provide , the more accurate your results will get usually , and for this example you would benefit from having a bunch of amateur photos as well . since there will be a large number of features , you wo n't really be aware of what the algorithm knows , but you would be able to tell if one matches your training set , or does n't . maybe someone else can provide more details , because i do n't know exactly how to work with photos yet . i just know how to work with data , but with these photos the algorithms will simply look for patterns in the pixels . for increased comprehension , read slowly : basically , what you do is provide a set of training examples with several features that the algorithm can focus on , then you provide it a definite answer . so you say , this is professional . this is not . the algorithm then identifies something called a hypothesis function that tries to match as many of the features observed in the training examples as possible , based on a set of numbers called parameters or weights , that when plugged into the hypothesis , can predict if the data you 're feeding it falls in the matching side or the not matching side . it involves something called a cost function which figures out how different your hypothesis is from the actual data . this cost function is set to be minimized , or to find the partial derivative of the curve with respect to the parameters ( basically , when the hypothesis differs from the actual data the least ) , and this is done through another function called gradient descent . gradient descent is a stepwise calculation that takes arbitrary initial values for your weights , calculates the tangent of the cost function , and repeats until it reaches a minimum , meaning the difference between the hypothesis and the actual training examples is minimized . once you have the right parameters or weights , you can then feed the hypothesis new information , and using those parameters it figured out through gradient descent , it runs a comparison saying , yes this image likely matches the professional set , or no this image does n't match the professional set . like i said though someone else may be able to help a little bit with how to exactly do this with images . but all in all , it 's just programming a few very simple functions . nothing too algorithmically complex . edit : grammar fixes and clarifications <eoa> 
basketball ( or any sport ) predictions are there generally accepted algorithms used for predicting the outcomes of sports games ? i 'm getting ready to scrape historic data for ncaa bb and want to actually use it for something . i 've come across the pythagorean formula , but there has to be something better than a predictor for baseball . <eoq> check out this blog : http : //netprophetblog.blogspot.com/ what a wonderful breakdown of many different prediction algorithms ! definitely one of the best blogs out there on basketball prediction . <eoa> 
ml n00b here . starting to learn ml on my own . classification technique guidance required . im starting to learn ml by myself . i have a course starting ml from next year , but im too eager to learn it before the course begins . i always learn by starting to solve a problem and then read up the theory behind it . so in a way , when i solve the problem id have read the theory behind it and applied it at the same time . so now , im trying to classify images into 5 categories . to make my life easier i have even got the feature vectors . i just need to classify them into multiple categories . how do i go about doing this ? what are the various techniques ? how accurate are they ? how do i tweak the performance ? which programming language should i use ? please help me out with this . thanks in advance ! <eoq> for a complete beginner i 'd recommend playing around with [ scikit-learn ] ( http : //scikit-learn.org ) in python . the site has a lot of tutorials and explains the difference between classification ( binary and multi-class ) and regression . it also talks about feature selection and normalization a bit . more than anything , it has the shallowest learning curve of any ml toolkit i 've worked with , and it has great pointers should you want to learn more . also , read up on evaluation . it 's really key to getting good results and comparing models : accuracy is not everything , and sometimes it 's not informative . for example , if you were trying to predict whether a car has a boot on it from an image , you could probably get 99 % accuracy by always predicting `` no boot '' , since most cars are not booted . <eoa> 
class of concepts i have a class of concepts c of the form ( a ≤ x ≤ b ) ^ ( c ≤ y ≤ d ) where a , b , c , d ∈ { 0 , 1 , 2 } . i have two questions . 1. how many distinct concepts c can be formed ? 2. what happens if we allow a , b to take also negative integer values , that is , a , b ∈ { −2 , −1 , 0 , 1 , 2 } while c and d remain with values in { 0 , 1 , 2 } ? how many distinct concepts as described above are there in this case ? <eoq> your notation is a little bit unclear to me . what does ^ mean , and why ca n't you just say that x and y are elements of { 0,1,2 } or { −2 , −1 , 0 , 1 , 2 } ? what are you trying to do here ? <eoa> 
general question about data sets with large number of boolean columns . so let 's say i have a data set where each row is a very long list of booleans and i want to predict the value of certain columns given other columns , but not always necessarily the entire row ( minus the column i 'm predicting ) . so basically i will have a subset of a complete row and would like to predict the target column 's value . what methods should i be focusing on to accurately tackle this problem ? <eoq> a [ bayesian network ] ( https : //en.wikipedia.org/wiki/bayesian_network ) would probably be ideal , since it lets you input the values you know and then gives you probability distributions over the rest . this can be a fairly good option if you know the structure of your domain so that you can construct the network manually so that only probability distributions need to be learned . otherwise you need to do structure learning as well , which is more difficult . neural networks may also be an option . i 've never seen them used in the wild , but it sounds like maybe you could use interactive activation networks ( see [ chapter 2 here ] ( http : //web.stanford.edu/group/pdplab/pdphandbook/ ) ) , which actually end up functioning a lot like bayes nets . otherwise you could try using some kind of [ autoencoder ] ( https : //en.wikipedia.org/wiki/autoencoder ) . these are basically networks that are trained to output their input . if you then specify incomplete input , it should autocomplete . make your true/false inputs 1 and -1 so that you can use 0 for missing data . <eoa> 
just started with introduction to statistical learning and i have some basic questions . equation 2.1 states : y = f ( x ) + e quoting : `` here f is some fixed but unknown function of x1 , ... , xp , and e is a random error term , which is independent of x and has mean zero . in this formulation , f represents the systematic information that x provides about y . '' questions : - what do they mean here by systematic information ? - if e has mean zero , then why even include it in the equation ? - are there any scenarios where e is non-zero ? if there are , what are some examples ? - is e a vector ? a set ? a scalar ? thank you . <eoq> let me see if i can help : <eoa> 
just started with introduction to statistical learning and i have some basic questions . equation 2.1 states : y = f ( x ) + e quoting : `` here f is some fixed but unknown function of x1 , ... , xp , and e is a random error term , which is independent of x and has mean zero . in this formulation , f represents the systematic information that x provides about y . '' questions : - what do they mean here by systematic information ? - if e has mean zero , then why even include it in the equation ? - are there any scenarios where e is non-zero ? if there are , what are some examples ? - is e a vector ? a set ? a scalar ? thank you . <eoq> if e has mean zero it means that the error can go on both sides of a regression line more or less with equal probability . i.e . positive and negative error ( positive error as in your f + positive value e , negative error as in your f + negative value e ) will occur normally . <eoa> 
just started with introduction to statistical learning and i have some basic questions . equation 2.1 states : y = f ( x ) + e quoting : `` here f is some fixed but unknown function of x1 , ... , xp , and e is a random error term , which is independent of x and has mean zero . in this formulation , f represents the systematic information that x provides about y . '' questions : - what do they mean here by systematic information ? - if e has mean zero , then why even include it in the equation ? - are there any scenarios where e is non-zero ? if there are , what are some examples ? - is e a vector ? a set ? a scalar ? thank you . <eoq> also , is there a subreddit dedicated to the discussion of this book ? <eoa> 
good data preprocessing python module ? i have to do a lot of data preprocessing for my machine learning task . specifically scaling , inserting missing values , splitting large multidimensional numpy arrays into training , validation and testing sets , handling timestamps and timeseries data . i have been building small methods to handle the data with mostly sklearn and various numpy functions . is there a good library i can use that will help me with preprocessing data ? is sklearn the best option ? <eoq> for scaling , inserting missing values and handling timestamps and timeseries data the answer is [ pandas ] ( http : //pandas.pydata.org/ ) . it has labelled dataframes as basic objects , a wide variety of tools and methods for handling null or missing data ( filling , dropping , back filling , forward filling , interpolating , etc . ) , and unparalleled timestamp and timeseries facilities . it also has great tools for sampling , munging , merging , filtering , summarising and plotting data , but those are just gravy at this point . if you do n't know pandas you should learn it . there 's a [ great tutorial ] ( https : //www.youtube.com/watch ? v=5jnmutdy6fw ) by brandon rhodes that will get you started , and after that the docs are excellent . <eoa> 
sklearn pca with pandas dataframes when passing along a pandas dataframe to a pca ( ) object , how can i tell which of the input vectors are being chosen when using n_components='mle ' ? pca = sklearn.decomposition.pca ( n_components='mle ' ) pca.fit ( df ) <eoq> pca does n't choose input vectors . it ( lineary ) combines them all . <eoa> 
anyone care to take a stab at this non-descript classification task ? trying to validate my results . <eoq> i get 80,4 % on a 4 x 20 x 1 relu network <eoa> 
anyone care to take a stab at this non-descript classification task ? trying to validate my results . <eoq> i 'm trying to validate the results i 'm getting . i replicated ( or so i think ) an algorithm from a paper that achieved ~78 % accuracy using a deep neural network ( [ 4 , 3 , 2 ] hidden layer ) , but i can not seem to get more than 63 % . svm gives me 55 % , a generic mlp model from deeplearning.net gave me 48 % . the dataset has 4 inputs with domain [ 0,1 ] . the single output is a binary 0 or 1. there are 1000 samples in the dataset . i recommend you come up with your own train/test split to validate your model . i used 5x2 cross-validation , with 33 % of the training data set aside for validation . feel free to use any model you wish , but please explain how you got it : o added difficulty : do n't transform the inputs ( i.e . use them as is ) . <eoa> 
[ beginner ] what is a good ml textbook that contains pseudo code of algorithms ? i have been looking at books . i like tom mitchell 's book on machine learning as it has some pseudo code for c4.5 . does anyone else know of other similar books that have pseudo code ? if not , does anybody know where decent documented source code is for a ml library ? i took a look at weka and it was n't the easiest code to follow . <eoq> [ introduction to statistical learning with applications in r ] ( http : //www-bcf.usc.edu/~gareth/isl/ ) when i help companies interview data scientists , i advise that candidates should be able to open this book up to any page and explain the concepts . <eoa> 
fraud detection with unsupervised learning i 'll preface by saying i am brand new to ml . i have a data set of ~10,000,000,000 ( fake ) transactions where each row is the username , time of transaction , credit card number , device used ( ios , android , laptop , etc ) and amount . i need to determine which transactions are likely malicious/fraudulent and which are n't . i would prefer to approach this with unsupervised learning of some kind , as going through these and manually labeling ones i *think* are bad would be super tedious and not necessarily correct . is there a good approach to this problem with unsupervised learning ? i 've read about random forests and decision trees ( high level overviews ) and maybe they would do the trick ? thanks <eoq> fraudulent transactions are likely different from 'normal ' transactions , so you are basically looking for outliers . you could start looking at some histograms / scatterplot of your data to begin with . you could use clustering ( unsupervised learning ) to determine what are 'normal ' transactions , and then you could find which transactions are not near clusters , and are thus likely fraud . you could start with some simple clustering algorithms such as kmeans or hierarchical clustering approaches . if you really want to do something fancy you could try using tsne , but i would try easy approaches first . you will have to find some algorithms that will scale to your dataset though ( look for large scale clustering toolbox or something like this ) ... if you have no information about which transactions are fraud ( it sounds like you do n't have access to this information ) you can not train any supervised models such as random forests or decision trees ( those are usually supervised models , unless you are talking about some special models ) . <eoa> 
what type of machine learning or decision making should i start looking into for this task ? thank you in advance for reading this - i know enough to know that i do n't really know anything . but here is what i am trying to accomplish : i have two populations of individuals , let 's call them a and b . a is a mostly static group of people with the normal traits a person would have , age , gender , locale , specific degrees in some form of education , a number of years involved in our project , etc . they also have a history we could review of encounters and outcomes we could teach a system with . b is an ever changing population of people with the same traits but with the addition that we want them to take an action . let 's say we want them to read more books , or walk more . the `` game '' is that a person from a is `` up '' to talk to someone and try to convince them to take an action . i am wondering what sort of machine learning or approach to ai would watch and learn the successes and failures in population a and make the best decision on who from population b they should be matched up with . what would most likely result in an action taken . i simply have no idea on what direction to head with this ? a few word answer would get me going for some time on my own . maybe i am dreaming but i have always been fascinated by ai since i read marvin minskey 's society of the mind decades ago . i think that was the name . i would love to pursue leveraging some sort of machine learning in this task . sadly aside from a fascination and some more light reading i trended more into mainstream programming : ( thanks for any help , even if it is `` your nutz , this is not something for machine learning '' thank you , bill <eoq> if you have data on 'successful ' interactions . that is when a member of particular a succeeds in getting a particular member of b to perform an action then i would model this as classification problem for edge prediction on a graph . you have a bipartite graph of a 's and b 's and you have your known edges , when an a has successfully persuaded a b for a task . you then use this data to train a model to look for additional connections . you can either model each edge as 'successful ' ( binary classification ) or each edge as 'successful for action k ' then a multiclass problem . either way this differs from normal classification in that you are using attributes of both a and b to make the prediction . this may make an advantage over just modelling a to predict if they can persuade any b . have a look at this paper : http : //arxiv.org/pdf/0806.0215v2.pdf its a biological paper so you need to read through the biology but could be useful . <eoa> 
[ beginner ] error rate stays same from beginning to end i started learning nn . after some reading of theory i stumbled upon this [ blog ] ( http : //iamtrask.github.io/ ) and decided to follow it and implement by myself . i am trying to use iris dataset ( iris-setosa and iris-vericolor ) but it seems that my nn stays on 0.5 error rate . i tried to change alpha , hidden layer size but it seems that in the end error rate is equal 0.5. code is [ here ] ( http : //pastebin.com/1aamvqg9 ) . output : error ( iterations : 0 ) : 0.436383143672 error ( iterations : 10000 ) : 0.500000789314 error ( iterations : 20000 ) : 0.500000195026 error ( iterations : 30000 ) : 0.50000011538 error ( iterations : 40000 ) : 0.500000692513 error ( iterations : 50000 ) : 0.500000173484 error ( iterations : 60000 ) : 0.500000148476 error ( iterations : 70000 ) : 0.500000059061 error ( iterations : 80000 ) : 0.500000134286 error ( iterations : 90000 ) : 0.500000466787 any help or suggestions ? <eoq> if its a balanced binary classification problem then the expected error rate of a random guess should be 0.5 . <eoa> 
[ beginner ] error rate stays same from beginning to end i started learning nn . after some reading of theory i stumbled upon this [ blog ] ( http : //iamtrask.github.io/ ) and decided to follow it and implement by myself . i am trying to use iris dataset ( iris-setosa and iris-vericolor ) but it seems that my nn stays on 0.5 error rate . i tried to change alpha , hidden layer size but it seems that in the end error rate is equal 0.5. code is [ here ] ( http : //pastebin.com/1aamvqg9 ) . output : error ( iterations : 0 ) : 0.436383143672 error ( iterations : 10000 ) : 0.500000789314 error ( iterations : 20000 ) : 0.500000195026 error ( iterations : 30000 ) : 0.50000011538 error ( iterations : 40000 ) : 0.500000692513 error ( iterations : 50000 ) : 0.500000173484 error ( iterations : 60000 ) : 0.500000148476 error ( iterations : 70000 ) : 0.500000059061 error ( iterations : 80000 ) : 0.500000134286 error ( iterations : 90000 ) : 0.500000466787 any help or suggestions ? <eoq> could n't spot an obvious error in the code . i do n't speak alot of python though . <eoa> 
[ beginner ] error rate stays same from beginning to end i started learning nn . after some reading of theory i stumbled upon this [ blog ] ( http : //iamtrask.github.io/ ) and decided to follow it and implement by myself . i am trying to use iris dataset ( iris-setosa and iris-vericolor ) but it seems that my nn stays on 0.5 error rate . i tried to change alpha , hidden layer size but it seems that in the end error rate is equal 0.5. code is [ here ] ( http : //pastebin.com/1aamvqg9 ) . output : error ( iterations : 0 ) : 0.436383143672 error ( iterations : 10000 ) : 0.500000789314 error ( iterations : 20000 ) : 0.500000195026 error ( iterations : 30000 ) : 0.50000011538 error ( iterations : 40000 ) : 0.500000692513 error ( iterations : 50000 ) : 0.500000173484 error ( iterations : 60000 ) : 0.500000148476 error ( iterations : 70000 ) : 0.500000059061 error ( iterations : 80000 ) : 0.500000134286 error ( iterations : 90000 ) : 0.500000466787 any help or suggestions ? <eoq> try l2_error = l2 - y ? <eoa> 
can a nn be taught to multiply two numbers ? hi , i 've been playing with regular neural networks with relu input , hidden and output units . i have successfully trained it to add two real numbers , just approximating binary functions for fun , of the form f ( x , y ) = z . the input is 4 numbers [ negpartofx pospartofx negpartofy pospartofy ] ex . f ( -5.1 , 8 ) becomes f ( [ 5.1 0 0 8 ] ) . output is similar [ negz posz ] addition is then simply to learn direct transfomration matrix [ 1 -1 1 -1 ] [ -1 1 -1 1 ] i 've tried similar approach to learn general multiplication , but with no luck . ie . i expect it to work for any real numbers , it sort of works for numbers within a limited range , but the results become too low for large numbers it has not seen before . something tells me it can not be done to learn general multiplication ... yes or no ? <eoq> i 'm afraid your model does not fit the problem . you might want to think about what kind of functions you can model with a three layer relu feedforward net . as a first step : what is the smallest neural network that can be trained to execute general addition of real numbers ? ( you might need to use another activation function ) <eoa> 
what is the best way to address the vanishing gradient problem in neural networks ? i 'm relatively new to neural nets and machine learning in general , and recently stumbled onto this problem when attempting to add more layers to a network used to classify digits . as i added more layers , my accuracy fell . i have heard of a few ways this is addressed , but was wondering if someone could highlight what the best techniques were , and what the pros and cons of each of them were . <eoq> relu units are the most common solution . basically the idea of these is to have a non-linear function whose gradient is either on or off , that way it pushes the error signal all the way down without diluting it at each layer . i do n't know enough myself to talk about pros and cons of different methods . <eoa> 
what 's the hardest part about training ml algorithms ? is it getting the massive datasets and cleaning them ? or is it waiting for the algorithms to finish ? or is it guarding against overfitting ? <eoq> cleaning > overfitting > waiting <eoa> 
cnn with engineered features if i am working with a data set of faces , 32x32 pixels each , and i want add an engineered feature like 'is the user wearing glasses ' . would it be reasonable to add a new row , 32x33 pixels , with a binary value to signify glasses or not . what 's the pro / con of this approach ? thanks <eoq> i suppose that an advantage of this approach is that it is easy to implement . it 's usually a good idea to try the simplest approach first and see if the result is good enough for you before you invest more time and effort developing something more complicated . <eoa> 
cnn with engineered features if i am working with a data set of faces , 32x32 pixels each , and i want add an engineered feature like 'is the user wearing glasses ' . would it be reasonable to add a new row , 32x33 pixels , with a binary value to signify glasses or not . what 's the pro / con of this approach ? thanks <eoq> ( mostly just echoing what /u/cyberbyte said ) adding just row is a bit odd because only some convolutional filter applications will 'see ' the feature . so , if you add the indicator as the 33rd row , convolutional filters applied to the 1st row will take only pixels as input while convolutional filters applied to the last rows will take both pixels and the indicators as input , which is probably a bad thing considering those filters will have their weights tied . instead i would add the an extra channel to the input , so instead of 1x32x32 input switch to a 2x32x32 input , where the second channel is binary value indicators . this paper did exactly that to encode information about player skill level when it comes to playing go : http : //arxiv.org/pdf/1412.6564.pdf a more efficient but maybe harder to implement method could be to add the indicator with a weight to the output of the first convolutional layer before the non-linear functions is applied . so you could perform convolution on the input , add w*b where `` w '' is a new parameter and `` b '' is an indicator to each output unit , then apply your activation function and proceed as normal . <eoa> 
is over-training a risk when a person marks lots of messages in their preferred email client/site as spam/not spam ? let 's say that i receive on the order of 50 messages a day . if i diligently make sure that every one every day is correctly flagged as spam vs. not spam , is over-training a risk ? if so , how do i know at what point to stop training ? my knowledge level : i 'm can describe a bit how naive bayes works , i can define the term over-training , and i know that i should be saying `` ham '' instead of `` not spam '' - but the latter seems more appropriate for a `` questions '' subreddit . : ) ( i use thunderbird specifically , but i think this is a more general issue that would be applicable to any email client/app/site ? ) <eoq> i have n't really used naive bayes much , but i very much doubt that you 're going to `` overtrain '' your spam filter . the problem with overtraining a classifier is that it can lead to overfitting , which is when random error/details/noise ( from the training set ) is modelled instead of the real underlying relationship . this requires that the classifier is flexible enough to model all of those tiny details ; this is usually called high variance/low bias . variance typically increases when the classifier has more learnable parameters , so e.g . a big neural network has higher variance than a smaller one . naive bayes classifiers tend to have fairly low bias and are therefor less susceptible to overfitting . overtraining tends to only be a problem when a classifier 's training procedure goes over the training set multiple times to incrementally refine its model . one example is ( again ) a neural network ( multi-layer perceptron trained with backpropagation ) . if you have 500 training items and you can afford to run 1,000,000 training iterations , then you can train on each item 2,000 times and you might overfit . if your training set is larger ( e.g . 5,000 ) then you can train less on each item ( e.g . 200 times ) and you are less likely to overfit . in fact , even if you do train each item 2,000 times as well , overfitting is still less likely because it 's more difficult to capture all the details of 5,000 items than for 500. expanding your training set virtually always leads to less overfitting . with naive bayes you really only consider each training item once . the only way to increase the number of training iterations ( and potentially run the risk of overtraining ) is to expand the data set . it 's probably not literally impossible to screw up a classifier by expanding the training set . for instance , if you add ( almost ) the same item a million times ( and few other items ) , that will probably do the trick . but you should be fine if the e-mails that you flag form a sample that fairly represents the whole body of e-mail that you 're getting . in ml terms you want your sample to be independent and identically distributed ( i.i.d . ) . tl ; dr : flagging more e-mails probably makes your spam filter more accurate , not less . <eoa> 
what do you use for gradient descent ? do you implement your own or use pre-built software ? as i understand it , there is software that implements cross-validation and gradient descent for you , and all you have to do is supply the cost function and its derivative . i was wondering , do most people implement their own , or use off-the-shelf software/library for it ? <eoq> scikit-learn allday errday . <eoa> 
what do you use for gradient descent ? do you implement your own or use pre-built software ? as i understand it , there is software that implements cross-validation and gradient descent for you , and all you have to do is supply the cost function and its derivative . i was wondering , do most people implement their own , or use off-the-shelf software/library for it ? <eoq> i use fmincg inside of matlab . <eoa> 
what is the best classifier from the ones tested here ? for starters , i 'm a complete beginner in this field and i 'm just dipping my toes into this sea of knowledge . and now , what i 'm trying to do is to test and understand which is the `` best '' classifier and the one who performed best from the below list of classifiers ( [ **click the link** ] ( https : //drive.google.com/open ? id=0byaaxtj8chf7flrez05bmzrtdmhuvjznylrrrfrmew1pvfz2tnlytdlfnhjxmgpur3fxewm ) ) - i 've explained what the graphs mean , in detail , down below . to put this into context , i 'm trying to write a supervised-learning application with python and sklearn and what i am trying to do is find the right classifier for correctly classifying a resume from a list of resumes . so far , my learning algorithm has these 2 phases : * pre-processing * model training and then prediction based on the trained model . the pre-processing phase is where i made all of the adjustments and tried different methods before generating both a countvectorizer matrix and a tfidfvectorizer matrix and compared the `` performance '' of the classifiers trained with them . the distinct parts of my pre-processing phase are simply using a tfidfvectorizer and countvectorizer , or in combination with the following : * stemming ( with lancaster / snowball stemmer from nltk ) * word correction using [ peter norvig 's approach ] ( http : //norvig.com/spell-correct.html ) so , for training my model i 've tried combinations of all of these ( which can be found in the link ) : * a simple countvectorizer over my training text * a simple tfidfvectorizer ... over my training text * lancaster stemming + countvectorizer ( ... over my training text ) etc . * snowball stemming + countvectorizer * lancaster stemming + tfidfvectorizer * snowball stemming + tfidfvectorizer * lancaster stemming + peter norvig 's word correction algorithm + countvectorizer * snowball stemming + peter norvig 's word correction algorithm + countvectorizer * lancaster stemming + peter norvig 's word correction algorithm + tfidfvectorizer * snowball stemming + peter norvig 's word correction algorithm + tfidfvectorizer and in order to have a better overview of how my classifiers would perform in a dynamic context ( resumes can have more or less the same number of words ) , i 've used a variable value for min_df ( minimum document frequency ) to range from 18 - the number of documents in the test scenario to max ( word document frequency ) - which is what is displayed on the graphs . so , actually i 'm testing the `` performance '' of my models and how they work with different numbers of training features . as mentioned at the beginning , i 'm a complete beginner and i 've picked the classifiers to test these based on suggestions from people both here on reddit and on other forums related to ml . one of my main questions , which i hope to find an answer to here is if some classifiers would be considered overfitting , since this is not currently clear to me . for example , i can understand that the multinomial naive bayes is overfitting for most of the countvectorizer approaches . * nusvc is too `` unstable '' , which i 'm guessing makes it a poor choice of a classifier for this scenario . * but how about bernoulli naive bayes ? would it be considered a `` good '' classifier ? does it overfit at the beginning but then become a more `` real '' and better performing classifier towards the end , even if it 's prediction rate is not 100 % ? * same thing for gaussian naive bayes.. also , some people suggested that lda ( latent dirichlet allocation ) is a good classifer for this kind of scenario . i 'm hoping that someone could help me make some sense out of my results , since i 'm a bit confused on how i should interpret them . and if there 's anyone interested in how i actually did this , with code , you can find it [ **on github** ] ( https : //github.com/radu-gheorghiu/recommendersystem/blob/master/implementations/bag_of_words_sklearn/bag_of_words_sk.py ) . disclaimer : i 'm a terrible programmer with a very non-pythonic way of writing code . and in the end , thank you in advance for reading this `` story '' and your help is greatly and immensely appreciated ! <eoq> http : //scikit-learn.org/stable/tutorial/machine_learning_map/index.html <eoa> 
what is the best classifier from the ones tested here ? for starters , i 'm a complete beginner in this field and i 'm just dipping my toes into this sea of knowledge . and now , what i 'm trying to do is to test and understand which is the `` best '' classifier and the one who performed best from the below list of classifiers ( [ **click the link** ] ( https : //drive.google.com/open ? id=0byaaxtj8chf7flrez05bmzrtdmhuvjznylrrrfrmew1pvfz2tnlytdlfnhjxmgpur3fxewm ) ) - i 've explained what the graphs mean , in detail , down below . to put this into context , i 'm trying to write a supervised-learning application with python and sklearn and what i am trying to do is find the right classifier for correctly classifying a resume from a list of resumes . so far , my learning algorithm has these 2 phases : * pre-processing * model training and then prediction based on the trained model . the pre-processing phase is where i made all of the adjustments and tried different methods before generating both a countvectorizer matrix and a tfidfvectorizer matrix and compared the `` performance '' of the classifiers trained with them . the distinct parts of my pre-processing phase are simply using a tfidfvectorizer and countvectorizer , or in combination with the following : * stemming ( with lancaster / snowball stemmer from nltk ) * word correction using [ peter norvig 's approach ] ( http : //norvig.com/spell-correct.html ) so , for training my model i 've tried combinations of all of these ( which can be found in the link ) : * a simple countvectorizer over my training text * a simple tfidfvectorizer ... over my training text * lancaster stemming + countvectorizer ( ... over my training text ) etc . * snowball stemming + countvectorizer * lancaster stemming + tfidfvectorizer * snowball stemming + tfidfvectorizer * lancaster stemming + peter norvig 's word correction algorithm + countvectorizer * snowball stemming + peter norvig 's word correction algorithm + countvectorizer * lancaster stemming + peter norvig 's word correction algorithm + tfidfvectorizer * snowball stemming + peter norvig 's word correction algorithm + tfidfvectorizer and in order to have a better overview of how my classifiers would perform in a dynamic context ( resumes can have more or less the same number of words ) , i 've used a variable value for min_df ( minimum document frequency ) to range from 18 - the number of documents in the test scenario to max ( word document frequency ) - which is what is displayed on the graphs . so , actually i 'm testing the `` performance '' of my models and how they work with different numbers of training features . as mentioned at the beginning , i 'm a complete beginner and i 've picked the classifiers to test these based on suggestions from people both here on reddit and on other forums related to ml . one of my main questions , which i hope to find an answer to here is if some classifiers would be considered overfitting , since this is not currently clear to me . for example , i can understand that the multinomial naive bayes is overfitting for most of the countvectorizer approaches . * nusvc is too `` unstable '' , which i 'm guessing makes it a poor choice of a classifier for this scenario . * but how about bernoulli naive bayes ? would it be considered a `` good '' classifier ? does it overfit at the beginning but then become a more `` real '' and better performing classifier towards the end , even if it 's prediction rate is not 100 % ? * same thing for gaussian naive bayes.. also , some people suggested that lda ( latent dirichlet allocation ) is a good classifer for this kind of scenario . i 'm hoping that someone could help me make some sense out of my results , since i 'm a bit confused on how i should interpret them . and if there 's anyone interested in how i actually did this , with code , you can find it [ **on github** ] ( https : //github.com/radu-gheorghiu/recommendersystem/blob/master/implementations/bag_of_words_sklearn/bag_of_words_sk.py ) . disclaimer : i 'm a terrible programmer with a very non-pythonic way of writing code . and in the end , thank you in advance for reading this `` story '' and your help is greatly and immensely appreciated ! <eoq> i 'm a bot , *bleep* , *bloop* . someone has linked to this thread from another place on reddit : - [ /r/machinelearning ] [ what is the best classifier from the ones tested here ? : mlquestions ] ( https : //np.reddit.com/r/machinelearning/comments/3inerg/what_is_the_best_classifier_from_the_ones_tested/ ) [ ] ( # footer ) *^ ( if you follow any of the above links , please respect the rules of reddit and do n't vote in the other threads . ) ^\ ( [ info ] ( /r/totesmessenger ) ^/ ^ [ contact ] ( /message/compose ? to=/r/totesmessenger ) ) * [ ] ( # bot ) <eoa> 
a question about softmax layer neurons . how are the partial derivatives ∂a^k ' / ∂a^k = 0 for all k ' ! = k , where a^k is the output from the kth neuron of the softmax layer . should n't a change in the output of one neuron of the softmax layer cause a change in the output of other neurons as well ? i 'm having a lot of trouble wrapping my head around this . any help is appreciated , thanks ! <eoq> you should do the derivatives with respect to the node 's *input* , which i 'll call `z` . ∂a^k ' / ∂z^k = a^k ' \* ( 1 - a^k ) if k=k ' , and -a^k ' \* a^k if k ' ! = k . <eoa> 
i have a square matrix of data ( from simulations of 2d pde 's ) what kind of fun things can i do ? basically as title says , i have a lot of data from solving some basically stochastic/noisy diffusion fields in 2d and i am interested in just playing around with the data with some ml tools ( probably with pythons sklearn toolkit ) i just do n't want to head in blindly at the moment , so i wanted to ask for suggestions first on what could/can/should be done ! ( quite vague i guess , apologies for that but this entire field is very very new to me but fascinating ) thank you : ) quick edit : forgot to add these solutions give some sorta nice pattern formations/fractals-like structures hence my interest in ml application <eoq> plot that shit <eoa> 
can you please help with a perceptron ( neural networks ) problem ? i have the following neural networks problem and couldnt find any answer on the web . any hints would help . i am not looking for a complete solution , just some pointing in the right direction . problem : write the upper bound ko of the number of iterations needed to a perceptron to learn a linear separable set with the rosenblatt rule : w ( 1 ) = w* / 10^6 what category of points makes the learning hard ? justify the answer . thank you , dan . <eoq> it 's basically asking you to prove the perceptron convergence theorem ( easy to google for writeups ) . gvien that perceptrons are linear classifiers , points with non-linear boundaries are impossible to learn . the canonical example is the xor function . minsky 's perceptron book proved this and started the first ai winter . <eoa> 
training methods other than by example hey team , pretty general question for you . every time we talk about training a machine learning algorithm , we are referring to training by example . are there any other methods to training ? take image classification via neural network for example . you can show a convoluted deep net 10,000 images of airplanes , and eventually it learns to find patterns in the edges and so forth . is it not possible to help the algorithm along by letting it know that most bi-planes have 2 sets of stacked wings , or something similar ? <eoq> that is an idea older then ml . however , `` explaining '' to algorithm facts about bi-plane is far from trivial . so ml is exactly trying to avoid most of explaining and lets algorithm learn facts from examples . <eoa> 
what is `` deep learning '' as opposed to just `` machine learning '' ? <eoq> `` deep learning '' is a technique developed by the field of machine learning . deep learning is a rebranding of neural networks , which historically did n't perform very well on many problems . more recently , neural networks have been performing well because machine learning researchers figured out a set of techniques that make it easier to learn deep ( complex ) neural networks . machine learning does not necessarily involve neural networks . <eoa> 
what is `` deep learning '' as opposed to just `` machine learning '' ? <eoq> my understanding of this topic is a bit limited , so i 'd be interested if this response is off target , but here 's what i think it means : <eoa> 
what is `` deep learning '' as opposed to just `` machine learning '' ? <eoq> `` deep '' learning refers to having a hierarchical stack of models on top of each other , each of which uses the output of the previous `` layer '' to produce another intermediate result that is a little closer to what we want to get . this is in contrast to `` shallow '' models that only do a single ( albeit very complicated ) transformation of input to output . hierarchical/layered approaches work very well in computer vision , because images are made up of objects , which are made up of parts , which are made up of edges ... so having a succession of highly specialized models detect ever more complicated features is a much more practical approach than trying to have one model detect everything from raw pixel values . currently this is almost always done with many-layered neural networks , but there is also growing interest in `` deep '' graphical models . <eoa> 
neural network to learn to play video games is it possible to use a deep belief network to learn and play a video game ( e.g tetric , pacman , snake ) ? i 've read about deep reinforcement learning being used for it , but i 'm unsure how far this is from a dbn . apologies for how nonsensical this may be , i 'm still new to neural networks as a whole as i 'm in the planning stage for an undergrad project involving it . <eoq> hey op ! so i 'm not very familiar with a reinforcement method but you should look up mario . not sure if it 's related but hopefully it can help give you some inspiration . i 'd link it now but i 'm on mobile . <eoa> 
i 'm an r user but know zip about ml . help me out ... alright guys , i know at some point i 'll need ml in r for future projects at work . i want to be ahead of the game so the transition is as smooth as possible . dear experts , consider the following : 1 ) i have no idea about ml ; 2 ) i know r ; 3 ) i know statistics/econometrics well enough to understand how to model some real world issues ; given that , would you answer me the following questions : 1 ) where can i learn the absolute basic about ml ? 2 ) given that i 've learned the foundations , is there any books focusing on r and ml ? i 've downloaded a few ones but would n't mind a few extra tips . thank you ! <eoq> [ `` introduction to statistical learning '' ] ( http : //www-bcf.usc.edu/~gareth/isl/ ) <eoa> 
tutorials for svm ? where is a tutorial to use/implement svm using a programming language such as r , python , java , or c # ? i would like to both gain a deeper theoretical understanding and how to implement it in a programming language please . i tried google , but have come up empty . please help . <eoq> machine learning with r by brett lantz and python data science essentials by alberto boschetti ; luca massaron might be the answer to my own question <eoa> 
relu derivative in 0 hi , recently i found a mistake in my code where i defined the derivative of relu as 1 if node unit is > = 0 this does n't make much sense because it would be 1 for all nodes in a relu layer . i corrected it to 1 if node unit is > 0 however , i 've been running some tests switching back and forth , and it seems i have slightly faster convergence with the old derivative ( > = 0 ) . what do you guys use ? it 's undefined in 0 http : //www.wolframalpha.com/input/ ? i=derivative+of+max % 280 % 2c+x % 29 http : //www.wolframalpha.com/input/ ? i=derivative+of+ % 28x+ % 2b+abs % 28x % 29 % 29 % 2f2 <eoq> false alarm , i think i might be wrong on this one <eoa> 
have a data set of more than 1gb , how to read it in r ? the 'read.csv ' command leads to a hang . and r doens't allow opening files more than 5mb . what can i do in this case ? any help will be highly appreciated . <eoq> [ found this bit of reading ] ( https : //theodi.org/blog/fig-data-11-tips-how-handle-big-data-r-and-1-bad-pun ) <eoa> 
have a data set of more than 1gb , how to read it in r ? the 'read.csv ' command leads to a hang . and r doens't allow opening files more than 5mb . what can i do in this case ? any help will be highly appreciated . <eoq> what do you ultimately need to do with the data ? ? <eoa> 
any recommended papers or books on time-series/sequence classification ? hello everyone , so i tried solving a previous research problem ( first one i had the pleasure of working on alone ) like a noob , i.e . i did n't really understand things like hmm and dynamic time warping so i did n't use it . in retrospect , it would have seriously helped out with my problems , and it would have definitely given me a magnitude of difference in results . anyway , that was my first attempt at research as an undergraduate . am i alone in this 'ooooh that would have solved all of my problems but i did n't use it ' feeling ? i hope not . anyway , i 'm curious now , can you all recommend papers that would help with sequence classification for time-series data ? my goal is to classify sensor data into one of several classes . <eoq> i am working on anomaly detection from dynamic strain or accelerometer measurements . in my case i try to extract some damage sensitive features ( modal parameter ) before the anomaly detection step . the following book may be helpful for you . the authors describe multiple methods for damage sensitive feature extraction from time-series . farrar , charles r. , and keith worden . structural health monitoring : a machine learning perspective . john wiley & sons , 2012 . <eoa> 
free course : amazon machine learning <eoq> i saw the roadmap for the course , seems pretty good . the instructor , unfortunately sounded a bit monotonous to me at the preview video . in case anyone here has taken the course , would you recommend it ? <eoa> 
can artificial neural networks be programmed to 'mutate ' ? i 'm a regular scientist curious about machine learning . so , please be patient with me . can artificial neural networks be programmed to 'mutate ' ( quasi ) randomly characterised nodes that may predict or act as hereustics to reach desired output nodes better than those features which may be hypothesised in advance ? <eoq> yes , they can . the [ neuroevolution of augmenting topologies ( neat ) ] ( http : //www.cs.ucf.edu/~kstanley/neat.html ) algorithm is one method to do it . such an approach works well when the optimal network topology is n't known ( or surmised ) in advanced , and when there is a good way to formulate incremental improvements . <eoa> 
can artificial neural networks be programmed to 'mutate ' ? i 'm a regular scientist curious about machine learning . so , please be patient with me . can artificial neural networks be programmed to 'mutate ' ( quasi ) randomly characterised nodes that may predict or act as hereustics to reach desired output nodes better than those features which may be hypothesised in advance ? <eoq> i do n't know enough to fully answer this question , but there are genetic algorithms in machine learning which perform similar to what you described . it is at least a starting point for you to research . <eoa> 
can artificial neural networks be programmed to 'mutate ' ? i 'm a regular scientist curious about machine learning . so , please be patient with me . can artificial neural networks be programmed to 'mutate ' ( quasi ) randomly characterised nodes that may predict or act as hereustics to reach desired output nodes better than those features which may be hypothesised in advance ? <eoq> yes , most all machine learning algorithms do use a type of `` artificial evolution '' . even in something as simple as linear regression , or finding a line of best fit , a program will probably iterate through `` generations '' , continuously adapting a hypothesis until cost between known values is minimal . <eoa> 
interpreting improvements to logistic regression model in r i have an existing logistic regression model that works fairly well ( 100 % precision and 95 % recall in initial training ) , but i 'm looking to make improvements to it after about 6 months time . i 've seen some false positives in that time and i 've added some features ( from about 7 to 15 ) to improve the accuracy . the initial model was fit on about 1200 observations ( 800 train , 400 test ) , and was fit using stepwise regression to determine the optimal feature set based on aic . i used r to fit this model using glm . i 'm looking into a couple options for improving , primarily pca ( prcomp ) and regularized regression ( glmnet ) . in addition to my goal to improve accuracy , i 'm also interested in keeping the model interpret-able to someone without r access ( i.e . if someone were to query new data , can they easily apply my model to predict the result without needing r ? ) my problem is that i do n't know whether to use pca and then apply logistic regression on those pcs ( i.e . principal component regression ) or work toward regularized regression instead . i 'm leaning toward regularized logistic regression using glmnet , but having a hard time interpreting the plots from glmnet . i 've been following the [ glmnet vignette ] ( http : //web.stanford.edu/~hastie/glmnet/glmnet_alpha.html # log ) , but i 'm pretty lost , to be honest . i understand some of the use of the alpha and lambda parameters , but how do i use the output from glmnet to determine : a ) the appropriate feature set , and b ) the appropriate values for lambda and alpha ? when would i use lambda.1se over lambda.min ? it 's been a while since i was in university learning about regression , so i only remember the basics ... i get lost quickly in the mathematical notation . i 'm also not able to share anything related to my data . can anyone link to a resource showing a detailed walkthrough of using glmnet , or explain to me more simply than the vignette ? finally , i fear i 'll end up with a model result that i 'm not able to fully grasp . let 's say i were to use this model result and later find prediction errors in production ... how do i go back and determine where my model is failing ( and therefore determine how to improve it ) ? i know the answers to these questions are heavily reliant on my specific data set , but i 'm looking more for general answers or links to good resources that could help me figure these answers out for myself . <eoq> [ deleted ] <eoa> 
minimizer wo n't converge for logistic regression program i 'm trying to do the logistic regression assignment from andrew ng 's course in python . i got it to work in octave , but for some reason when i try migrating it over to python the mininimzation procedure fails . the cost function just diverges to nan . i ca n't figure out what 's causing it . to me everything looks exactly the same . here 's my python [ program ] ( http : //pastebin.com/zgxkb7kf ) . here 's the [ dataset ] ( http : //pastebin.com/yjrxw4dw ) . here 's a description of the [ assignment ] ( http : //s3.amazonaws.com/spark-public/ml/exercises/on-demand/machine-learning-ex2.zip ) . can anyone see an obvious bug ? <eoq> you should try using a debugger or print statements to inspect values <eoa> 
how many learning curves should i plot for a multi-class logistic regression classifier ? if we have k classes , do i have to plot k learning curves ? because it seems impossible to me to calculate the train/validation error against all k theta vectors at once . to clarify , the learning curve is a plot of the training & cross validation/test set error/cost vs training set size . this plot should allow you to see if increasing the training set size improves performance . more generally , the learning curve allows you to identify whether your algorithm suffers from a bias ( under fitting ) or variance ( over fitting ) problem . <eoq> ||ypredict-yactual||2 <eoa> 
cost function confusion in many of the guides i have read online , ( in particular [ this one ] ( http : //www.holehouse.org/mlclass/09_neural_networks_learning.html ) ) , there is a step at the beginning of back propogation where you calculate the discrepancy between your observed output and your target output . the part that i am very confused about is that sometimes this discrepancy is calculated as 𝛿 = a - y ( where `` a '' is the observed output and `` y '' is the target output ) . later on , the guides will discuss a `` cost function , '' which seems to me to be the same thing as the difference formula above ? however , a totally different equation ( http : //imgur.com/guz1oxp ) is then given . is the difference formula just an abstraction of the more complicated formula ? thanks ! <eoq> in regression , the cost function is usually the mean or sum of squared errors -- each individual term in the sum is ( a-y ) ^2 . what you called the discrepancy is actually the derivative of this error wrt to a as used in backpropagation : http : //www.wolframalpha.com/input/ ? i=derivative+.5* % 28a-y % 29 % 5e2+wrt+a when you do binary classification the cost function is [ the cross-entropy ] ( http : //deeplearning.stanford.edu/wiki/images/math/f/a/6/fa6565f1e7b91831e306ec404ccc1156.png ) between the wanted distribution and what the network predicts . this is a special case of the [ negative log-likelihood under a multinomial distribution ] ( http : //deeplearning.stanford.edu/wiki/images/math/7/6/3/7634eb3b08dc003aa4591a95824d4fbd.png ) \* that is used in multiclass problems . \* for neural networks this formula is slightly different <eoa> 
my classifier has 100 % accuracy , is something wrong or is this good data ? hello everyone , i ran several models on a binary classification problem , each performing at around 70-80 % classification accuracy . i then used [ stacking ] ( https : //en.wikipedia.org/wiki/ensemble_learning # stacking ) to build a new model on top of those and i got near 100 % accuracy . my process : * run several models to see how well they performed for accuracy * pick the top 4 models ( lda , random forest - 15 trees , logistic regression , and random forest - 150 trees ) * create new features from those models - taking the predictions of each model ( using cross validation so that i would train 4/5 of the data to predict the last 1/5 , then swapping out so that there would be no data leakage ) . * run a binary classifier on the new dataset which includes those extra features * get 100 % accuracy . i tried 5-fold cross-validation on the new model and it performed at around 96 % + accuracy . do i need to check something else to make sure that this model is legit ? <eoq> make sure you are evaluating the ensemble on totally untouched data . by untouched i mean that this data has n't been used for fitting the base or `` stacker '' models . edit : and also not used for selecting hyperparameters for your stacker model <eoa> 
need ml algorithm suggesstion ? hi , i have read ml long time ago . i 've crunch time and in need to choose the algorithm to complete my following task : traveller , is visiting my website . i make them fill the form and have alll the necessary signal ( attributes ) with me like whether they have booked flight or not , whether email is guenine is not , phone no is given or not , trip date is fixed , destination location is fixed or not . but along with that i have many visitor who do n't fill the form completely or just uses fake phone number . i again re-iterate , i have lot of signal available with me , and i need to filter out the traveller who is certain to go for travelling so that i can personally contact them . i also need some score as well on the scale of 10 . which ml algorithm is best suited for this job and why ? <eoq> regularized logistic regression . because you need a starting point , you ca n't expect to find the perfect method on the first attempt . <eoa> 
day of the week with generic backward prop nn 's i have been with temporal data and have been representing monday as `` 00001 '' and friday as `` 10000 '' . in general is this a good idea or is it better to represent the data as the day of week in binary ? ( 000,001,010,011,100,101 ) i realize this is a pretty generic question . i am just looking for a `` rule of thumb '' . <eoq> from what i know , first method is common for categorical data ( days of the week ) , named `` one-hot encoding '' . i do n't see any advantages in second method besides slightly smaller number of inputs . <eoa> 
what is the most common model for acoustic classification ? i 'm writing a few labs on the subject and i 'd like to get some opinions on what i should teach . <eoq> this paper here might be useful : ) http : //cs229.stanford.edu/proj2013/camenzindgoel-jazz_automatic_music_genre_detection.pdf <eoa> 
how important is performing cross validation on your algorithm 's parameters ? <eoq> a lot . if you have a parameter you are changing it is just like another feature . if you run your algorithm with 100 's or 1000 's of parameter combinations.. you will have results which are 'significant ' just due to chance . <eoa> 
would you benefit from deep learning examples & tutorials ? i 'm wanting to build a sort of blog that dives into different aspects of deep learning , a project based/case study style learning experience . assuming the content was good , would you subscribe to a blog like that ? would you be interested in working through the problems ? <eoq> definitely ! it sounds like it could be fun <eoa> 
would you benefit from deep learning examples & tutorials ? i 'm wanting to build a sort of blog that dives into different aspects of deep learning , a project based/case study style learning experience . assuming the content was good , would you subscribe to a blog like that ? would you be interested in working through the problems ? <eoq> i would definitely subscribe . go ahead please do it <eoa> 
would you benefit from deep learning examples & tutorials ? i 'm wanting to build a sort of blog that dives into different aspects of deep learning , a project based/case study style learning experience . assuming the content was good , would you subscribe to a blog like that ? would you be interested in working through the problems ? <eoq> if it 's python , if it provides some math - not too hard - not too dumbed down - something that a probability/linear-algebra background can figure out - pretty pictures - links to further reading - links to foundational reading , if the projects have varied lengths so some are short and some are medium ( 100 lines ) and few are large ( 200+ ) . <eoa> 
would you benefit from deep learning examples & tutorials ? i 'm wanting to build a sort of blog that dives into different aspects of deep learning , a project based/case study style learning experience . assuming the content was good , would you subscribe to a blog like that ? would you be interested in working through the problems ? <eoq> pseudocode with clear description of every variable would be a goldmine <eoa> 
would you benefit from deep learning examples & tutorials ? i 'm wanting to build a sort of blog that dives into different aspects of deep learning , a project based/case study style learning experience . assuming the content was good , would you subscribe to a blog like that ? would you be interested in working through the problems ? <eoq> yes please , i would definitely benefit from tutorials/guided learning ! <eoa> 
long boolean vectors as data set - looking for suitable model / algorithm hey ! i am new to this topic , therefore any help is greatly appreciated ! my data set consists of two pieces : * one factor ranging roughly from -1 to +1 * a rather long ( say 100-dim . ) boolean array in the training process , i want to find correlations between the factor and what values in the boolean array are set . runtime , the algorithm should be able to predict a value for the factor when analyzing an array . the previous solution used some kind of svm . unfortunately i do n't have access to the source code and ca n't really think of a way to re-implement this . thank you for any kind of help ! <eoq> my uninformed opinion : one-hot encoding for the array boolean values , assuming you have enough data for that dimension size . then run a logistic regression on it for some interpretable results ( by reading the coefficients ) , or run a random forest for high accuracy . maybe you can determine the importance of features with tree-feature selection ... ah i ca n't link to it because sourceforge is down at the moment ( they host scikit-learn ) . then if you cut down on the features , your logistic regression model may become more interpretable . <eoa> 
i 've already implemented a naive bayes classifier to help me assess the sentiment of tweets . what other algorithms can i use ? i have a data set of tweets , and i need to classify them as either pro , anti , or neutral with respect to some topic . i 've already used the mulitnomial naive bayes classifier on sklearn , and would like a second algorithm to compare the classification against . my first thought would be k-nearest neighbours ( i can use `countvectorizer` to turn the tweet into a vector of characteristics , and then run it through the alg ) . any other suggestions ? <eoq> i 've tried using a nb classifier like that for sentiment analysis on larger passages , but it did n't work too well . you could build a long short-term neural network and then classify it 's output , which is how sentiment analysis is often done with neural-networks . take a look at : http : //deeplearning.net/tutorial/lstm.html <eoa> 
trying to decide on algorithm for my data hi . i 've began reading a lot about machine learning , but having no advanced background in statistics/maths , i find it hard to relate to most of the online information for selecting the proper algorithm on my data . i do programming for a living . this is a pet project as will be clear from my example , so i do n't need an absolute best fit or optimal result . with this said , here is my problem : i play path of exile , a free game . it 's extremely similar to diablo . it 's got one of the most complex set of items attributes . the game allows players to have shops online ( so list items with prices ) as well as give the information about what the players are wearing . i spent months of time writing code to extract this data , so i have historical data about what items were worn , what they were replaced with , what items were listed for sale , which ones sold , how long it took to sell , etc . this data is rather massive , were talking a few hundred gigabytes . my data is split in 2 big data sets : * inventories * ( what players wear ) * * items for sale * ( pricing data is very noisy , as there are no consistency in pricing ) * heres a breakdown of how items work , as far as stats are concerned : * there are ~250 stats to chose from . those stats never change . * ( ex : +40 % fire resistance ) * * an item is a combination of 4-6 of those stats . * every stat has one category . when an item has one stat , it can not get another stat within the same category . * ( ex : if the item has +40 % fire resistance , it can not have the 2nd stat '+20 % fire resistance ' . there are ~30 categories ) * * every stat has a known chance to occur . * ( ex : 0.25 % chance to get the stat +40 % fire resistance ) * * if the item is listed for sale , it *may* have a listed price . the system is n't much more complicated than that . i simplified a lot the details , but this is what matters . i have 2 questions i wish to answer with machine learning : * **predict an item pricing** * **try to find how strongly related are stats together ( ie : out of the stats that players wear , estimate the demand ) ** i tried the andrew ng online course , as well as reading a lot of documentation online , the weka tool , and none seem to make it clear what algorithm i should pick . this is what i am planning to use as input neurons : * every category ( ie : set of stats ) is an input neuron , with the currently selected stat within the category being the value . * the 'weight ' of every stat in the category is simply its probability to occur . * the stats within the category are ordered from least probable to most probable ( ie : best to worst ) . * proportionally adjust the categories ranges within 0-1 . ex : ( one category ) stat | range ( of fire resistance ) | probability -- -| -- -| -- -- of the magma | 42-45 | 0.10 % of the volcano | 36-41 | 0.25 % of the furnace | 30-35 | 0.75 % of the kiln | 24-29 | 1 % of the drake | 18-23 | 1 % of the salamander | 12-17 | 1 % of the whelpling | 6-11 | 1 % so if my item has `` of the magma '' , for that neuron , i would give it 0.10 value to that input in the nn . if it had `` of the volcano '' , i would give it 0.35 . every category is given an input accordingly , with 0 denoting none was chosen . so with this said , i am not sure which algorithm to pick to feed it the output neuron ( pricing ? ) . my data is very noisy for prices . should i use a classifier with the output neurons being slices of prices ( ie : neuron 1 = 0-1 $ , neuron 2 = 1-5 $ , neuron 3 = 5-10 $ , etc . ) ? or should i have only one output neuron ? i thought the slices would effectively 'fix ' the issue of very variable pricing and at least give a good idea of the price range to expect . and as far as detecting the strength of connections between stats for items that are worn , i do not see what output neuron i could map , so i was wondering if that was even possible . i though i should rank negatively stats that are being removed ( ie : player stopped equipping item with stats x , y , z , so input those values but as negatives ) , but that still leaves me wondering how to extract the correlation between the stats . i would welcome any help as such . i do n't expect any hand-holding , i just want a nudge in the proper direction ! thanks again <eoq> let me preface this by saying that i 'm no ml expert , i 've only part gone through hinton 's coursera class . i do play poe though , and this is an interesting project . <eoa> 
trying to decide on algorithm for my data hi . i 've began reading a lot about machine learning , but having no advanced background in statistics/maths , i find it hard to relate to most of the online information for selecting the proper algorithm on my data . i do programming for a living . this is a pet project as will be clear from my example , so i do n't need an absolute best fit or optimal result . with this said , here is my problem : i play path of exile , a free game . it 's extremely similar to diablo . it 's got one of the most complex set of items attributes . the game allows players to have shops online ( so list items with prices ) as well as give the information about what the players are wearing . i spent months of time writing code to extract this data , so i have historical data about what items were worn , what they were replaced with , what items were listed for sale , which ones sold , how long it took to sell , etc . this data is rather massive , were talking a few hundred gigabytes . my data is split in 2 big data sets : * inventories * ( what players wear ) * * items for sale * ( pricing data is very noisy , as there are no consistency in pricing ) * heres a breakdown of how items work , as far as stats are concerned : * there are ~250 stats to chose from . those stats never change . * ( ex : +40 % fire resistance ) * * an item is a combination of 4-6 of those stats . * every stat has one category . when an item has one stat , it can not get another stat within the same category . * ( ex : if the item has +40 % fire resistance , it can not have the 2nd stat '+20 % fire resistance ' . there are ~30 categories ) * * every stat has a known chance to occur . * ( ex : 0.25 % chance to get the stat +40 % fire resistance ) * * if the item is listed for sale , it *may* have a listed price . the system is n't much more complicated than that . i simplified a lot the details , but this is what matters . i have 2 questions i wish to answer with machine learning : * **predict an item pricing** * **try to find how strongly related are stats together ( ie : out of the stats that players wear , estimate the demand ) ** i tried the andrew ng online course , as well as reading a lot of documentation online , the weka tool , and none seem to make it clear what algorithm i should pick . this is what i am planning to use as input neurons : * every category ( ie : set of stats ) is an input neuron , with the currently selected stat within the category being the value . * the 'weight ' of every stat in the category is simply its probability to occur . * the stats within the category are ordered from least probable to most probable ( ie : best to worst ) . * proportionally adjust the categories ranges within 0-1 . ex : ( one category ) stat | range ( of fire resistance ) | probability -- -| -- -| -- -- of the magma | 42-45 | 0.10 % of the volcano | 36-41 | 0.25 % of the furnace | 30-35 | 0.75 % of the kiln | 24-29 | 1 % of the drake | 18-23 | 1 % of the salamander | 12-17 | 1 % of the whelpling | 6-11 | 1 % so if my item has `` of the magma '' , for that neuron , i would give it 0.10 value to that input in the nn . if it had `` of the volcano '' , i would give it 0.35 . every category is given an input accordingly , with 0 denoting none was chosen . so with this said , i am not sure which algorithm to pick to feed it the output neuron ( pricing ? ) . my data is very noisy for prices . should i use a classifier with the output neurons being slices of prices ( ie : neuron 1 = 0-1 $ , neuron 2 = 1-5 $ , neuron 3 = 5-10 $ , etc . ) ? or should i have only one output neuron ? i thought the slices would effectively 'fix ' the issue of very variable pricing and at least give a good idea of the price range to expect . and as far as detecting the strength of connections between stats for items that are worn , i do not see what output neuron i could map , so i was wondering if that was even possible . i though i should rank negatively stats that are being removed ( ie : player stopped equipping item with stats x , y , z , so input those values but as negatives ) , but that still leaves me wondering how to extract the correlation between the stats . i would welcome any help as such . i do n't expect any hand-holding , i just want a nudge in the proper direction ! thanks again <eoq> first , have you tried just taking something like scikit-learn and just fitting a simpler regression model , like linear regression or random forest regression , to the data first ? often those will get you 90 % + of the accuracy of a more complex model in about 1 % of the time , and help you figure out which sets of attributes and outputs will work well . if you just use the 'raw ' price as the output , you can use a regression model , and if you want to lump the prices into ranges , then use a classifier ( where each price range is a separate class ) . if you 're determined to use a neural network , then for managing the output there 's two simple ways you can do it . the easiest is to have a single output neuron for price that uses a rectified linear activation function and squared error loss ( in other words , treat it as a regression problem ) . this will drive your model to try to estimate the average price across that set of inputs . the second way to do it is to break the price into ranges and treat it as a classification problem : stick a softmax layer at the end with categorical cross-entropy as the loss function . this will make it estimate the probability that an item will sell in a given range . you do have to be a bit careful to 1 ) have a 'bin ' for every possible price , and 2 ) not have too many outputs , since this can make the model harder to fit . advantages of the linear output is that you only need one output neuron , it should fit quickly , and it will give you a flexible output . disadvantage is that it wo n't give you much in the way understanding the variance of the price ( i.e . the average might be $ 1 , but is it $ 1 +/- $ 0.01 , or $ 1 +/- $ 0.99 ? ) advantages of the 'binned ' output is that it will give you some measure of uncertainty , so it might tell you that it 's 95 % sure that it will sell for $ 1-2 , or it might tell you that it 's 25 % likely to sell for $ 1-2 , 25 % for $ 2-3 , etc . disadvantages are that it will probably be slower to train , and if you slice the prices too small , then you could get odd results if you have very rare combinations of attributes that sell for unusual prices . for analyzing the combinations of items , you might be able to use something like t-sne on your trained network to get some idea of if or how particular combinations of traits cluster , but i 'm less familiar with that sort of thing . it might be easier to fit a separate model , either by filtering to only examine items that were actually purchased and then doing a ( non neural network ) clustering on those , or maybe through building a model that tries to predict the nth attribute given the other n-1 on the item . <eoa> 
how do i handle large csv datasets ? i 'm a beginner messing around with some basic application of ml , applying simple classifiers and stuff , and i stumbled along a ctr challenge avazu posted a while back on kaggle : https : //www.kaggle.com/c/avazu-ctr-prediction . my problem is that the training set here is a single csv ~2gb in size , and possibly a few hundred million entries . i tried opening this dataset in the weka viewer , and even after increasing the stack size to 4gb , it stopped responding after some time , forcing me to close it . this brings me back to the question , what tools/libraries do you guys use to handle such large datasets ? <eoq> let 's denote the problem straight : it does n't matter to you if the dataset size is 4gb/20gb/3tb if you can hold only 1gb dataset . there is no other way to shrink down your dataset other than to take a subset of it . every line-by-line approach to get this subset would work for you . <eoa> 
how do i handle large csv datasets ? i 'm a beginner messing around with some basic application of ml , applying simple classifiers and stuff , and i stumbled along a ctr challenge avazu posted a while back on kaggle : https : //www.kaggle.com/c/avazu-ctr-prediction . my problem is that the training set here is a single csv ~2gb in size , and possibly a few hundred million entries . i tried opening this dataset in the weka viewer , and even after increasing the stack size to 4gb , it stopped responding after some time , forcing me to close it . this brings me back to the question , what tools/libraries do you guys use to handle such large datasets ? <eoq> my uninformed opinion : <eoa> 
how do i handle large csv datasets ? i 'm a beginner messing around with some basic application of ml , applying simple classifiers and stuff , and i stumbled along a ctr challenge avazu posted a while back on kaggle : https : //www.kaggle.com/c/avazu-ctr-prediction . my problem is that the training set here is a single csv ~2gb in size , and possibly a few hundred million entries . i tried opening this dataset in the weka viewer , and even after increasing the stack size to 4gb , it stopped responding after some time , forcing me to close it . this brings me back to the question , what tools/libraries do you guys use to handle such large datasets ? <eoq> you could simply read it line by line with csv reader , put it in list then convert it to numpy array . <eoa> 
help a beginner out ? hi ! i am a college student trying to diversify my resume with some projects that are a little different than a standard crud application and have always been interested in ml . i am going through 'mahout in action ' right now . i am just curious if anyone could suggest a decent data clustering project ? i am completely unable to think of anything interest right now : / any advice or suggestions will be appreciated . thanks <eoq> anyone here ? ? <eoa> 
are there any datasets freely available on which i can test the autocomplete algorithm i have developed <eoq> there is /r/datasets -- also ask there . <eoa> 
i need help with reinforcement learning i 'm trying to do an ai with machine learning , so i started to learn how to do machine learning , but i do n't think i understand enough of it to do one . i tried learning by watching some courses , but what they say does n't seem to be related with what i 'm trying to do . if anyone could help me , i 'm trying to do it with pybrain and python 3.4 at the moment . i 'm currently trying to code the environment class . i am doing this just for fun and to learn how it works . <eoq> so what *are* you trying to do ? without a concrete question , i doubt anyone can really help you . what are your problems with the environment class ( from [ this tutorial ] ( http : //pybrain.org/docs/tutorial/reinforcement-learning.html ) i assume ) ? for a general introduction to rl , sutton and barto 's famous rl book is freely available [ here ] ( http : //webdocs.cs.ualberta.ca/~sutton/book/ebook/the-book.html ) . you might also like [ this udacity course ] ( https : //www.udacity.com/course/machine-learning-reinforcement-learning -- ud820 ) by charles isbell and michael littman , or olivier georgeon 's [ ideal mooc ] ( http : //liris.cnrs.fr/ideal/mooc/ ) although it 's a bit more advanced . <eoa> 
markov decision process in r for a song suggestion software ? okay , so i 'm not exactly sure if this belongs here , but this is my problem : we have a music player that has different playlists and automatically suggests songs from the current playlist i 'm in . what i want the program to learn is , that if i skip the song , it should decrease the probability to be played in this playlist again . i think this is what 's called reinforcement learning and i 've read a bit about the algorithms , decidin that mdp seems to be exactly what we have here . i know that in mdp there are more than one state , so i figured for this case it would mean the different playlists . like depending on the state ( playlist ) i 'm in , it chooses the songs that it thinks fits the best and get `` punished '' ( by skipping ) if it has chosen wrongly . so what i 'm asking is , if you guys think this is the right approach ? or would you suggest a different algorithm ? does all of this even make any sense , should i provide more information ? if it does sound right , i 'd like to ask for some tutorials or starting points getting about mdp in r. i 've searched online but have only found the mdp toolbox in r and it kind of does n't really make sense to me . do you have any suggestions ? i 'm really helpful for any kind of advice . : ) <eoq> mdp researcher here . i would avoid the mdp formulation for this problem since what you would have here is a partially observable ( in the state of the person selecting songs ) markov decision process , which are *really* difficult . depending on your requirements , i think this could be a good way to proceed that does n't take a lot of advanced ml education : * confine your worldview to a single playlist . switching playlists is a separate and likely more complicated problem that you do n't need to deal with . * if you have the resources to build song features that describe the attributes of each song ( think of these like pandora 's genres ) then you can build a model where an action ( punishing ) for one song can inform whether you want another song to be played . if you do n't have the ability to create song features , then your modeling choices become very simple . * assuming you have song features , you should start with a logistic regression . from there , many tweaks and improvements are possible . <eoa> 
can i use sklearn 's naive bayes to classify tweets ? i have a data set of tweets with keywords relating to vaccine perception . i want to be able to classify a new tweet as either pro-vaccine , anti-vaccine , or neither . i hear naive bayes is a way to do this , but i can not find any documentation on how can implement the classifier with words as feautres rather than numbers . can anyone provide some insight ? <eoq> yes you can , but you have to figure out a way of representing tweets so that the classifier can do its work . [ these slides ] ( https : //web.stanford.edu/class/cs124/lec/naivebayes.pdf ) go over the bag-of-words representation and how you use it to fit the multinomial naive bayes model . other groups have done this kind of stuff with so-called word vectors , where individual words are given a representation in an arbitrary vector space learnt from a huge text corpus via neural network models ( see [ word2vec ] ( https : //code.google.com/p/word2vec/ ) ) . if you 're using python , the sklearn library has automatic functions for this kind of feature extraction , see [ here ] ( http : //scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.countvectorizer.html ) . <eoa> 
trouble pre-processing large volumes of image data this [ kaggle competition ] ( https : //www.kaggle.com/c/diabetic-retinopathy-detection/data ) is my first time applying my limited understanding of ml . my knowledge is mainly from andrew ng 's coursera class . any how , i 've drafted up a [ class ] ( https : //gist.github.com/aweeeezy/164ee8be0464ec79df04 ) to get started with the preprocessing of these images . the basic function of the code is to create a data abstraction for each image that has attributes like its id and rgb values for each pixel ( found using `skimage.io.imread ( ) ` ) . now , my computer is pretty old and only has 2gb of ram ... i 'll be getting a new computer soon with 16gb and nearly double the processing speed , but in the mean time , i 'd like to find a way to more efficiently generate , store , and represent this data so my ml scripts can work with it reasonably at a later stage of the project . the method you see in [ the code ] ( https : //gist.github.com/aweeeezy/164ee8be0464ec79df04 ) bogs up my computer and does n't finish execution . i thought of throwing a condition statement in the suite of `process_images ( ) ` that checks if i 'm at , let 's just say , a 5 % interval of the total data set -- > if so , then write the binary of the dictionary , `image_data` , to file and then set it to none so i can free up memory -- > continue where i left off , repeatedly appending to the same binary file until i 've gone through all 44,000 training examples . the problem with this is that after testing this out on just 5 images , my binary file is about 161m ( 32.2m each , which accounting for all 44,000 images scales up to a 1.5tb ! ) . how the hell can i possibly work with this much data efficiently ? ! what do ml experts usually do to pre-process high res image data ? how do i normalize/standardize the rgb values of each pixel in an image for every image in this huge of a data set ? i know that 's more than a couple questions , but i 'm not really sure where to go from here . advice , please and thank you ! <eoq> i 'm sorry to say it more or less boils down to resources . a dataset that big really requires that faster pc you 're talking about . 2gb of ram just is n't enough to do what you need . i just took a look at your code and the competition . for a start you want to use [ cpickle ] ( https : //docs.python.org/2/library/pickle.html # module-cpickle ) over pickle . it is many , many , many times more performant than standard pickle . that alone may solve a lot of your issues . <eoa> 
classification algorithms - a discussion hi there , i 've been tasked with writing my own implementation of any classification algorithm for an assignment . i was going to pick the c4.5 algorithm but i thought i 'd ask you guys what you think ( as i thought it 'd be interesting to see all the different opinions ) . i chose the c4.5 because i am quite familiar with how it works and feel that it would be ok for a beginner to attempt it ( i have never tried to write my own implementation of a classification algorithm before ) . my data deals with a bunch of measurements and i have to decide if the next entry is a certain type of animal . just in case you were wondering ! : ) so if you were a beginner , what algorithm would you choose ? better yet , if you had to decide on an algorithm with your current knowledge , would it be different than if you were just starting out ? what pitfalls did you encounter when you were first starting ? also , if you could think of any resources that may be beneficial to my *learning* ( not direct answers as i 'd like to learn from my assignment ! ) ... ..post away ! : p sorry if this is the wrong place , i got a notion and i thought it 'd be pretty cool to hear form an expert or two ! thanks ! <eoq> the first classifier i implemented was logistic regression . maybe that 's too simple for your assignment though . <eoa> 
classification algorithms - a discussion hi there , i 've been tasked with writing my own implementation of any classification algorithm for an assignment . i was going to pick the c4.5 algorithm but i thought i 'd ask you guys what you think ( as i thought it 'd be interesting to see all the different opinions ) . i chose the c4.5 because i am quite familiar with how it works and feel that it would be ok for a beginner to attempt it ( i have never tried to write my own implementation of a classification algorithm before ) . my data deals with a bunch of measurements and i have to decide if the next entry is a certain type of animal . just in case you were wondering ! : ) so if you were a beginner , what algorithm would you choose ? better yet , if you had to decide on an algorithm with your current knowledge , would it be different than if you were just starting out ? what pitfalls did you encounter when you were first starting ? also , if you could think of any resources that may be beneficial to my *learning* ( not direct answers as i 'd like to learn from my assignment ! ) ... ..post away ! : p sorry if this is the wrong place , i got a notion and i thought it 'd be pretty cool to hear form an expert or two ! thanks ! <eoq> i think c4.5 will be just fine . i guess you could also start with id3 and then extend it to learn more about the differences . to prevent overfitting , you may want to look into some pruning algorithms . then if you want to go further , you could look into random forests were really popular a few years ago i think ( maybe they still are , but ml is n't my main field and i got the feeling their popularity has decreased now that deep learning works so well ) . in my earlier days i mostly implemented neural networks and genetic algorithms . you could do those too , although genetic algorithms may be slightly harder to apply to your problem . <eoa> 
