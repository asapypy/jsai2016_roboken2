Question=000000  Generative Models:From an engineering standpoint, how much more difficult is programming a chatbot to use a generative model than a retrieval-based model?
-------------
Question=000001  Lasagne/Theano parameter saving problem
-------------
Question=000002  Can this problem be solved using ML?
I do descriptive analytics and reporting at a company that sells a wide range of products. We record sales transactions, and everytime a item is sold, the following is recorded:

* Customer ID (each customer has a unique ID)
* Product ID (each product has a unique ID)
* Sale date

(Other fields are recorded too - location of purchase, quantity, payment type, etc.)

We sell a few big ticket items, and what I'm wondering is if it's possible to predict whether a customer will buy one of the big ticket items based on their purchase history, using transactional data as described above. We have about 2 million rows of sales data spanning seven years, and in that time maybe 14,000 big ticket items have been sold to 5,000 out of 50,000 customers.

Is this a problem that ML applies to? I realize ML isn't something you learn overnight but I would love to know more about how something like this can be tackled.
comment_no.=000--> You might want to look into Affinity Analysis:  https://en.wikipedia.org/wiki/Affinity_analysis

You might also look at "recommender systems".  As an example, if most people who buy diapers also buy sippy-cups, you might see someone buying diapers who hasn't yet bought a sippy-cup.  In that case, they might be more likely to buy a sippy-cup, especially if you recommend it to them.  Something like:  http://link.springer.com/article/10.1007/s11334-016-0274-x
-------------
Question=000003  Started to implement SVM from scratch just to see if I can... turns out it does not go so well, could anyone care to take a look?
I'm following cs231n.github.io and decided to implement Support Vector Machines from scratch in Python. Turns out, it does not work so well - I made up sets that are clearly separable, yet my algorithm has a lot of trouble finding accurate line. Am I doing something wrong or my failure was imminent since algorithm itself requires much more powerful math to even work correctly?

I uploaded source code to github, please forgive polish variable names here and there. 

https://github.com/czlowiekrakieta/machine-learning-from-scratch/blob/master/svm.py
comment_no.=000--> I just glanced at it and noticed you were not explicitly plotting the support vectors. That should be the first thing you try.
comment_no.=001--> make sure the objective you are minimizing decreases.
-------------
Question=000004  What are the various way to improve accuracy
-------------
Question=000005  What is the most popular file type for datasets?ARFF? Why they don't use database?
as my question, I doubt that what is popular file type for datasets?
ARFF?
comment_no.=000--> I've been wondering this, too? Anyone know the fastest file format for reading from disk into memory?
comment_no.=001--> Should I ask this on Machine Learning subreddit?

-------------
Question=000006  Is there any way to implement Neuro-Fuzzy systems in Python?
-------------
Question=000007  How to make a conv network for classification using custom data?
-------------
Question=000008  Basic Random Variable question
-------------
Question=000009  Transition from econometrics to machine learning?
Hello everyone 
I'm currently an economics major and would like to know if there is a way to transition from economics, particularly econometrics to a carreer in machine learning. I've found out that I don't like economic theory and want to look for an alternative without changing majors. The school Im in has fame of being very good in econometrics, compared to the other schools in my country, but I haven't reached those classes yet so I don't really know what are they about. 
Thank you for your answers in advance (And sorry for bad english)
comment_no.=000--> Machine learning is a mix of math and programming, so if you haven't started already, I would recommend starting to learn R. It's a statistical language and an easier transition from other math languages like MATLAB. We teach it at our [bootcamp](http://datasciencedojo.com) because it is fast to learn.

This book is also recommended: [R for Everyone](http://www.amazon.com/Everyone-Advanced-Analytics-Graphics-Addison-Wesley/)
-------------
Question=000010  How is weight initialization done today?
I've just read [Understanding the difficulty of training deep feedforward neural networks](http://jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf). It seems to me that no regularization is used. Today, now that we have [batch normalization](http://arxiv.org/abs/1502.03167), [Dropout](https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf) and [deep networks with stochastic depth](https://arxiv.org/abs/1603.09382) as well as typically ReLU activation functions, I wonder how weights are initialized. Was there a follow-up paper which checked the importance of initialization / how much of a difference the initialization still makes? What are common ways to initialize weights in 2016?

For example, is RBM initialization still done? (Why / why not?)
comment_no.=000--> Yes, there was indeed a follow up Paper tackling initialization when using Relu non-linearities: [Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification](https://arxiv.org/abs/1502.01852) [He]. 

In the paper they proposed an initialization Sheme for Relu neurons. The formula which was derived in Glorot to obtain constant variance initialization is not valid for Relus, as Glorot assumes symmetry. However a similar analysis for Relu will yield to a new initialization which is Glorot \times \sqrt(2). Heuristically the \sqrt(2) is there, because Relu non-linearities have a 50% change of being zero, which reduces the std by a factor of two.

He seems to be one of the most common initializing method used for Relus. Almost all high level frameworks have a He implementation (Lasagna, Keras, Torch).

Dropout and deep networks with stochastic depth should not affect the initialization. Batch Normalization makes careful initialization much less important, (as they have analyzed in there paper). However I still find it reasonable to stick to He for initialization. It is derived using reasonable assumptions (constant variance) and works well in practice.

comment_no.=001--> (More questions to this paper are in [my summary](http://www.shortscience.org/paper?bibtexKey=journals/jmlr/GlorotB10#MartinThoma))
-------------
Question=000011  Basic Python Machine Learning - why use vstack and hstack?
I am going through a book called Python Machine Learning. I am working through how to use scikit and there is an example in the book where we write a method called plot_decision_regions.

I am having a hard time understanding the reasoning behind some of the code and would love some help.

Here is my notebook from github: https://github.com/gohuygo/machine_learning/blob/master/iris/scikit-learn.ipynb

There are two lines that I am particularly interested in:

> X_combined_std = np.vstack((X_train_std, X_test_std))

and
 
> y_combined = np.hstack((y_train, y_test))

Why do we want a vertically stacked array in one case and a horizontally stacked array in another case? Any guidance on supplementary material is also appreciated.

comment_no.=000--> X is input, usually a matrix of shape (batch_size, n_features). X_train_std and X_test_std are two batches of data, so you want to combine them to be a shape of (batch_size*2, n_features).


Y is target/label, usually a vector of shape (batch_size,). You want to combine y_train and y_test to a shape of (batch_size *2,)
-------------
Question=000012  Vector Space Models and Support Vector Machines
I'm applying vector space modelling and support vector machines to my thesis (specifically to see how frequently certain terms appear in documents within a particular corpus). I was wondering whether I could ask some specific questions about the relationship between VSM and SVM. Following Salton, Wong and Yang's paper, I'm guessing that first each of the terms within the document are plotted as vectors using the tf-idf (so that terms are weighted higher if they appear more frequently in a document and weighted lower if they appear less frequently in the entire collection). SVM could then be applied to each of these vectors in order to categorise them depending on the terms which appear most frequently in each. Is this the correct way of understanding the relationship between the two? Thank you.
comment_no.=000--> The vector space model is simply a way to represent your document, while a support vector machine is a classification algorithm that is going to work with that representation. They operate on different ends of the machine learning process, and you could swap any of them with an other method.

If it can help, imagine that you are working with documents which only have 2 words (for visualisation purposes). That means that each document can be plotted as a point on a plane (that's your vector space). Your TF-IDF weighting is going to impact where you are going to put those points on the plane. Your SVM is going to try and draw a line (the separating hyperplane) to separate (as well as possible) the points in the plane.
-------------
Question=000013  What do I need to know to get started in ML?
-------------
Question=000014  ELI5 multi class SVM machine learning
-------------
Question=000015  How to use create simple Linear Regression model with sklearn.dataset using PyMC3?

comment_no.=000--> "How to create simple Linear Regression model with sklearn.dataset using PyMC3?" is what I meant to say lol. 
comment_no.=001--> I REALLY want to learn how to create predictive Bayesian models so I can move away from frequentism and broaden my understanding of statistics. I am having trouble taking the theoretical concepts and applying them real data (i.e. bayesian linear model w/ supervised data).
-------------
Question=000016  What am I missing with word embeddings?
-------------
Question=000017  thousands of humans, dozens of decisions each, best clustering method to use for recommending increasing certain decisions
I have thousands of records (of humans) that each have dozens of percentage attributes (of their decisions) where for each human all their decision percentages add up to 1.

The assumption is that these records should be able to be clustered based on their percentages, and then I should be able to get a new record that matches closely to one of these clusters and I would provide recommendations on how to better *conform* to one or more of the clusters by increasing the percentages of one or more (but hopefully not too many) decisions. Also I'd prefer it didn't recommend decreasing a decision percentage, because the only way that is possible is by increasing many other percentages.

I'm currently learning methods and scikit with the Udacity course but haven't gotten to the section on clustering yet, and I need someone with a high level understanding to make sure I'm going down the right path.

I tried to abstract away the actual problem, and hope I've done a good job, but I would be happy to explain that if curious.
comment_no.=000--> Sounds like you are on the right path.

To get nearer the nearest cluster you may need to increase / decrease many percentages like you said - this depends on your data and clusters and clustering method. Possibly you could only suggest 3 percentages to change in your 'frontend' of the application, and then you only suggest the most important changes.

Finally, you will have to think about how many clusters you want, and you will need to somehow gain some knowledge what these clusters are about. You will need to somehow verify a bit that your clusters you find using some algorithm (for example kmeans) are meaningful.

Good luck! 
-------------
Question=000018  Why does LSTM need 4 inputs per node, compared to sigmoid nodes which only use 1 var a sum? Could something simpler get the job done?
https://en.wikipedia.org/wiki/Long_short-term_memory defines a node as...

    sumState <- inA*inB + sumState*remember 
    out = sumState*outMult

inA, inB, outMult, and remember are ordinary sigmoid neural nodes whose input is a weighted sum of many other nodes.

The purpose is for a network to self modify its patterns of how long to remember things.

But couldnt it be done with something simpler like...

    out <- out*(1-decay) + decay*in

in, out, and decay are all fractions.

in and decay are normal sigmoid nodes, and out is derived by that equation and can be used the same way. For example, any existing neuralnet could have a decay var added so it changes its state slower as chosen by some other node.

Training any of these is a harder problem than defining a workable model of data flow of a node type.

How are RNNs normally trained?

How are LSTM normally trained?

How could a RNN with decay var per node be trained?
comment_no.=000--> Hey. So the LSTM can remember, forget and output a custom value (that it memorized previously). All these operations are dynamic (decided based on the inputs, with trainable weights). Think of it as a programmable memory unit.

Your net (if I understand it correctly) would output just a weighted average (or unnormalized sum) of the previous output and current input transformed (you forgot to wrap input with a weight matrix, bias and nonlinearity).

What would that give? Well, it would be quite similar to Deep Residual Networks conceptually. They work well, but check out their detailed architecture in the original paper. They do not use a trainable decay parameter though, just sum the output.

How to train both an LSTM or your RNN? You'd use backpropagation through time - you unroll your RNN in time by K timesteps, giving a deep feedforward net, and then apply backprop. If you're unfamiliar with this, I would suggest trying to work with an unrolled RNN treated as a feed-forward net. Just assume you have a constant number of inputs and zero-pad your sequences if they're shorter.

Happy experimenting!

Edit: Deep Residual Networks http://arxiv.org/abs/1512.03385 (code on github)
-------------
Question=000019  What is a reasonable way to compare the improvement in accuracy?
-------------
Question=000020  Spectral Networks and Deep Locally Connected Networks on Graphs
Has anybody read this paper.I have been reading it and with all honesty i am pretty lost .If anybody has read this paper please can you please let me know how the network construction is taking place.
comment_no.=000--> [Article](http://arxiv.org/abs/1312.6203v2)

I haven't read this but it looks really hard... what's your background with ML?
-------------
Question=000021  Working with credit scoring data I want to find a potentially better classifier than what I have right now
So I'm working with this data set in R

http://www.statistik.lmu.de/service/datenarchiv/kredit/kreditvar_e.html

I want to predict credit (approved or unapproved) using the other variables. Most of them are categorical or binary, only about 3 are quantitative. Anyways, I've built quite a few models for my class project and the best one that I made used 800 training points and 200 test points and achieved about 15% error rate, it was a decision tree that incorporated all the variables. Most of the other models were in the 20% error rate. I wanna know if you guys have any suggestions for lowering this as much as possible? Is it possible that there is a limit as to what can be extracted from the data? 

Things I've tried: All the variables, Best & Mixed subset selection applied to -> Logistic regression, KNN, LDA, QDA, LASSO methods, SVM and Decision trees, simple, pruned & random forest. I also tried a neural network in R but the results were not as good as my simple decision tree. What model do you think will work best for this type of data?


comment_no.=000--> Be sure to try some featuremappings, such as PCA, before trying a classifier, that might improve results.
-------------
Question=000022  Experience with Amazon AWS?
Just wondering if anyone here has any experience working with Amazon AWS? Looking through the website it felt like it was written more to convince management, and less geared to someone who would actually be "doing" it. Anyone have any thoughts/experience on how it is?

Its multiclass, binary and regression description seems a little sparse to call it ML in my (unexperienced) opinion
comment_no.=000--> Set up a Spark cluster w/AWS!
comment_no.=001--> I use Torch on their gpus. Once you get it setup it's really nice. There are other services that offer much faster gpu clusters though
comment_no.=002--> AWS is set of more than 70 services. Are you asking about Amazon Machine Learning? If yes, that service is more aimed at ML beginners, I don't think you will get the same results as a hand tuned model. If no the main benefit is that you run whatever software you want on their servers and only pay per hour.
comment_no.=003--> At [Data Science Dojo](http://datasciencedojo.com), we've started teaching AWS ML, but there are many less options than in Azure ML. Azure has more algorithms and parameters to select (and the way it's set up is easier for beginners to understand). AWS ML is still pretty new - but it works for simple modeling if you already have all your data in Redshift or S3. 
-------------
Question=000023  Using Jupyter for Machine Learning?
Hey all!

I was just doing some reading and came across Jupyter. It looks like it's pretty useful for data analysis but I was wondering if it's used in industry for machine learning? 

Or is a more traditional editor what is commonly used?
comment_no.=000--> I've never really known for editors to be standardized.  most of my lab mates use sublime.  most of them also mainly use c++.  

as far as shareable demonstrations,  jupyter/ipython notebooks rock.  There's no real competition to demonstrating an idea and having someone be able to play with or recreate it.
comment_no.=001--> I work in an R&D shop and we use Jupyter a lot for any python work -- when you're prototyping and idea, or demonstrating an analysis its excellent. Of course once everything has been hammered down we move to IDEs for the heavy lifting (Eclipse, Emacs, and PyCharm are all popular) of actually coding up a complete solution in anger.
-------------
Question=000024  Why does OPTICS use the core-distance as a minimum for the reachability distance?
-------------
Question=000025  Newbie LSTM question: What are "hidden units" & "LSTM cell size" in LSTM?
-------------
Question=000026  When is probabilistic clustering applied?
The EM algorithm seems to be the only example of probabilistic clustering. It seems to me that it is very similar to k-means, but deals better with noise. However, what is the advantage of probabilisitc clustering compared to density based clustering like DBSCAN / OPTICS?
comment_no.=000--> If you have some expectation that your data has clusters with a particular parameterised distribution then obviously an EM approach is going to be the right answer. DBSCAN and OPTICS and HDBSCAN* (a further improvement over OPTICS) can actually be viewed as a non-parametric probabilistic clustering where these exists some PDF that generated the data, and the goal is to find suitable level sets of that PDF -- this is most clear in HDBSCAN* and its obvious relationship with Robust Single Linkage (which takes precisely this probabilistic view and even proves convergence to the level set tree of the PDF with sufficient data).
-------------
Question=000027  Customer profiling after applying K-Means
-------------
Question=000028  How to calculate bits per character (BPC)?
-------------
Question=000029  Developer Focused Questions about Machine Learning
-------------
Question=000030  Sequence to Sequence/Word/Character
-------------
Question=000031  How to calculate the performance of a neural network?
I made a neural network using python and numpy, and was wondering how I could calculate the performance of my classifier on a given test set. 
Any explanation, or link would be appreciated.
comment_no.=000--> By performance do you mean accuracy or speed? In either case you should build two identical NNs, one with your code and the other with an existing NN solution. Train them both on the same dataset (perhaps MNIST) and see what happens. It's very likely that the NN you've built will be significantly slower and if there are bugs in your code you could get diff results. Solutions like tensor flow or theano do a lot of optimizations to make their computation very fast. 
-------------
Question=000032  Newbie Tensorflow CNN doubt
-------------
Question=000033  Need help picking an algorithm in AzureML
-------------
Question=000034  learning conceptual hierarchy? ex: math -> [algebra, calculus, ...], algebra -> [addition, subtraction, multiplication, ...]
-------------
Question=000035  Book or talk that deals with the history of ML?
-------------
Question=000036  Bayesian treatment of outliers
-------------
Question=000037  Intuition for using matrices instead of vectors as gate parameter in an LSTM when dealing with word embeddings.
If I am correct then the gates of an LSTM always take the same shape as the object, on which they are used to either forget, update or output the cell state. But I am wondering whether it is a stupid idea to just use for example a scalar to forget about a cell state which is a vector. Since I didn't find any such implementations if there is a intuition related to word embeddings why this is not recommended.
comment_no.=000--> So you might want a different scalar to decide which dimensions of the cell state you want to forget about.. so we need a vector of scalars to predict which will be forgotten. Okay.. but we want this vector to depend on the RNN input, previous time point output, (and possibly cell state)... so how can we transform those things into this vector of forget scalars? We use a matrix and a nonlinearity
-------------
Question=000038  Point me in the right direction for simple machine learning project?
Hello, I have a simple project where people can text a phone number and i need to determine what category of sentence they are saying. The sentences do follow a pattern, so much so that i could just write a logic tree or if statement.

However, I do this this falls into the category of classifcation? And I saw this library: https://github.com/monkeylearn/monkeylearn-ruby

I'm a good developer, but new to machine learning. Id like to use my project as a way of going a bit deeper, but not too deep :)

Thoughts? Am I on the right track?
comment_no.=000--> If the sentences follow a fixed pattern you are likely better of writing the if statement. Machine Learning only makes sense for problems that can't be solved with a simple computer program or algorithm. In your case it would be a bad approximation of the if statement.
-------------
Question=000039  Is using a ReLU activation function always better then using using sigmoid/tanh ? (Neural Networks)
From what I've read online and from my limited experience training neural networks, it seems like ReLU is always a better choice than using a sigmoid/tanh activation function. 

I know one advantage ReLU has is that it does not suffer from the vanishing gradient problem (both sigmoid and tanh are affected by this). 

On http://playground.tensorflow.org/ , Using ReLU always seems to get to a better result faster than the other functions. 

Are there cases where sigmoid/tanh work better then ReLU ? If so, what are those cases and how would I identify them? 


comment_no.=000--> If you wanna to learn an input representation, like sparse coding, then sigmoid activation is a good choice.
-------------
Question=000040  Prior distribution for HMM transition matrix.
-------------
Question=000041  Handling Multiple Feature Types in an Adaboosted-Decision Stump Classifier [MATLAB or Python]
Hi,

I'm working on a project where I'm using an adaboosted decision stump classifier to predict a binary label(1 or -1). I have working code that works on feature vectors that are exclusively binary, and am now trying to handle certain elements of the feature vector that are not binary. For example, some of them are numeric(i.e. 0-9), or categorical(i.e. "strings"). 

They are interspersed throughout the feature vector. I'm considering reformatting the data set provided to put categorical variables at the end and handle them later, or expanding each categorical feature to multiple 1/-1 binary features(which seems very expensive). 

How should I go about dealing with categorical features, given that I'm trying to use the Adaboost formula on decision stumps that have primarily dealt with binary features only?

Thank you!
comment_no.=000--> Numeric features should be no problem of the original adaboost formulation (see wikipedia / google), your decision stump should just be able to handle them, and then you can integrate it into your adaboost code. 
Categorical features should be coded with dummy variables or one  hot encoding: https://en.wikipedia.org/wiki/One-hot
-------------
Question=000042  What is the current state of the art in speech recognition?
Sorry for copying a question straight from the sidebar, but it is one that interests me quite a bit, and I did not find it having been asked here recently. 

So what is the current state of the art in speech recognition? Open source and/or otherwise?
comment_no.=000--> [Deep Speech II](http://arxiv.org/abs/1512.02595) should be in that ballpark.
-------------
Question=000043  Need course recommendation
Hey /r/MLQuestions!

I want to get into machine learning, and I've been following Andrew Ng's Machine Learning course on Coursera. However, I'm finding it kinda hard to follow, I'm finding the math kinda shallow.

I'm rusty on my math skills, and thus I'm looking for a course/book/learning path recommendation which will also cover the math side by side, or at least point me in the right direction. 

Any recommendations would be helpful! Thanks!
comment_no.=000--> Hi! I work at Data Science Dojo. Besides our bootcamp, I recommend the books that we use in the bootcamp:

1- A Cartoon Guide to Statistics (easy intro to hard math)

2- Predictive Analytics by Eric Siegel

3 - R for Everyone

4 - Doing Data Science: Straight Talk from the Frontline

Good luck!
-------------
Question=000044  How to identify the input parameter that has the most significant correlation with the output value?
-------------
Question=000045  What are advantages of ensemble learning compared to a single model?
Ensembles (e.g. multiple decision trees) can reduce overfitting and give a better classifier than only a single system. Is there another advantage?
comment_no.=000--> The very statement of your question suggests a pretty good advantage right away.  But it's also the case that various model classes (or hypothesis classes) can be combined in ensembles, producing a classifier (or regression) profile not possible with any single model.

Another advantage is that certain methods of training, such as what you find in random forests, provide a form of global feature selection, as any weaknesses introduced by greedy, local feature selection (e.g. information gain) in any individual model are often offset by choosing from a random subset of features from model to model. 

Ensembles can also be parallelized more easily, leading to more efficient performance.  There is also the issue of *boosting*, which allows one to create a classifier that is tremendously accurate from a set of models that are individually mediocre.

There are more advantages, but these are some of the big ones that spring to mind.
-------------
Question=000046  Theano vs Tensorflow?
In your opinion which of the above two is easier to learn and prototype new Neural networks. i have little experience in theano and it was very hard to get a grasp on how things are working. 

How do these two compare?
comment_no.=000--> I'm no expert but tensor flow is still brand new where's Theano has been out for a while so there may be a bigger community around Theano for when you need help?!?

What about Torch?? I see allot of papers that use it lately...
comment_no.=001--> In my course we had used torch and it was really easy to get started it had lot of the built in stuff and learning lua is easy.But i prefer python so switched to theano recently and i must say it's too tough but it gives you a much better understanding of what you are doing .Symbolic variable,scan,function these are things you have to get a hold of before you start theano.
comment_no.=002--> I recently decided to go with TensorFlow. The reason was simply because it is open source, developed by a big company of which I'm sure it will continue development for a while. Due to this fact I expect more and more people to switch and other nice software to be written around TensorFlow (e.g. [TensorBoard](https://www.tensorflow.org/versions/r0.8/how_tos/summaries_and_tensorboard/index.html)).

See [this comparison](https://github.com/zer0n/deepframeworks/blob/master/README.md) for a more detailed answer:

* TensorFlow supports multiple GPUs, whereas Theano only supports one ([source](https://github.com/Microsoft/CNTK#performance))
comment_no.=003--> This is actually a very complex question with no correct answer... It's also worth noting that with tensorflow and theano (and probably the other ML frameworks as well) there are some great libraries that sit on top of them that make development far simpler. Such as tflearn for TF and Lasagne for Theano.
comment_no.=004--> Tensorflow is the preferred because it supports more GPUs and is faster.
-------------
Question=000047  Cross validation vs test set for determining model complexity?
-------------
Question=000048  Useful machine learning books, videos, courses and more
-------------
Question=000049  kNN Without Predictions?
Excuse my ignorance, I am very new to all this :).


For reference: 

* using Python 3 (numpy, pandas, (hypothetically) scikit)
* a small-ish data set, (4k rows, 15 columns).


I know you can use kNN to find the nearest neighbor(s) and it predicts something (the example I've seen in a bunch of tutorials is iris type). My question is: what if I don't want to predict anything? I've got all my data, I just want to input one row (with all 15 categories known) and find the most similar data point. I am using a simple euclidean distance measurement (after normalization of the data). but is there a good way to do this with ML? Are there other options I am missing? Perhaps you have a suggestion for something else?

Thanks!
comment_no.=000--> Unsupervised learning
comment_no.=001--> Are you interested in clustering your data into groups? if so SKLearn provides a bunch of methods for doing so: http://scikit-learn.org/stable/modules/clustering.html
comment_no.=002--> If you have one row and want to find the most similair point in your data, simply calculate all distances of all rows to your new row, and find the one with the minimum distance, that will be the most similair row.
-------------
Question=000050  Finding similarities in items that got most conversions
-------------
Question=000051  Need help understanding the maths behind gradient descent
Using neuralnetworksanddeeplearning.com book I have hit a wall at an exercise(part http://neuralnetworksanddeeplearning.com/chap1.html#learning_with_gradient_descent). I think I have understood most of the ideas of the gradient descent for training a neural network, however I can't understand the first exercise of the topic:

Prove that the choice of Δv which minimizes ∇C⋅Δv is Δv=−η∇C, where η=ϵ/∥∇C∥ is determined by the size constraint ∥Δv∥=ϵ.

Where C is the cost function(objective function), v is a variable of C.

The question also suggests:

Hint: If you're not already familiar with the Cauchy-Schwarz inequality, you may find it helpful to familiarize yourself with it.

I have tried fumbling around with the formulas like so:

η=∥Δv∥/∥∇C∥

then

η=−∥η∇C∥/∥∇C∥

But I get stuck at this point. Am I going in the right direction?
comment_no.=000--> What you really have to use is the equality case of Cauchy-Schwarz.

CS states that for two vectors x and y and the scalar product < , >, you have:

|<x,y>| <= ||x|| ||y|| with equality if and only if x and y are collinear e.g there exists some constant (let's call it - η) such that x = - η y.

As you want to minimize <x,y>, the best you can hope to achieve is <x, y> = - ||x|| ||y||.
For x = Δv and y = ∇C you get the result because the equality case is a upper bond.

Then as Δv = - η ∇C , you have η = ||Δv|| / ||∇C|| if the gradient is non-zero.
-------------
Question=000052  Plotting hypothesis function in linear regression
Hi guys!
Sorry in advance for what I assume is a very stupid question.
I am taking the Machine Learning course on Coursera, and I'm hitting a roadblock on the Univariate Linear Regression hypothesis function plot. 
[image](http://puu.sh/opDZm/bd7f4ced44.png)

The image linked above is the plotting I'm referring to. I'm unable to understand how theta 1 being 0.5 adds a slope to the line, and what the relevance of 2 is. Also [this](https://youtu.be/EANr4YttXIQ?t=88) is the video in question where the plot is being talked about.
Once again sorry for being dense about this.
comment_no.=000--> Hi,

H(x) = theta0 + theta1x

Just realize that H(x) is your y value. In the first example: 
theta0 = 1.5
theta1 = 0 
So y will always be 1.5 no matter what.

In the second example, theta1 is now 0.5, so whatever x is, y = x/2.

I'm sure you get it by now, too lazy to do example points.

Cheers.
-------------
Question=000053  Classification of biosignals
I have an Electromyograph device that will give me a 200 Hz signal from 8 different sensors placed around the arm. They come to me as a byte array. I'd like to classify these, however I'm not sure where to start. 

Since these are signals, I'm assuming just passing in the raw array for each time sample isn't a viable option. I've tried using a KNN on them and it's results were around 45% accurate. 

Is my use case right for ML or am I missing another algorithm that can do this for me?
comment_no.=000--> The scenario is correct but feeding the KNN with the raw signals is not the right way to proceed. You'll need some pre-processing step on the signals like filtering, than you have to extract features from the signals probably windowing them, than you have to identify the more relevant features for your problem and use those features (predictors) to train your ML algorithm. This is a very very general overview, I'm not an expert as well but I suggest you to search for "Times Series, Machine Learning", I think that would be a good start, but as said, I'm not an expert.
-------------
Question=000054  How to choose an unsupervised classification algo? Food macronutrients dataset
I have a dataset with macronutrient information for ~8000 foods. 
For each food I have the number of grams of protein, carbs, fat that are contained in 100g of that food. 

When I plot the data, I can identify a few clusters visually: http://imgur.com/Cl1koTd

How do I go about choosing a unsupervised classification algo for this problem? 

Using the DBScan algorithm I was able to get this result: http://imgur.com/1e7ALyy

Is there a diff algo that would allow me to do better?
comment_no.=000--> You could try HDBSCAN. It effectively runs DBSCAN for all possible epsilon values and tries to identify the best possible flat clustering from that. This makes parameter selection alot easier and gets you to a good clustering quickly.

HTTP://github.com/lmcinnes/hdbscan
comment_no.=001--> You could also try to use hierarchical clustering for your data and see if it fits better.

http://scikit-learn.org/stable/modules/clustering.html#hierarchical-clustering
-------------
Question=000055  Logistic model trees in Python?
-------------
Question=000056  How can I get a remote, beginner level job in ML?
Hi,

I am a blind programmer from Iran. If you want more background on my question, you can check out my blog post called [The Tools of a Blind Programmer](https://www.parhamdoustdar.com/2016/04/03/tools-of-blind-programmer/).

I've been developing the back-end for web applications for more than 5 years. However, with web development being drawn more and more toward presentation, I'm thinking of changing my field.

I have fallen in love with machine learning. However, here are the issues I have with it:

1. There are no machine learning jobs in Iran. The market hasn't gotten big enough to have big data. This will probably change in the next 2-3 years, but right now, it's zilch.
2. All jobs require you to already be well-versed in the field, specially remote jobs, because they are not looking for people they will have to micromanage. I understand that point of view, but I have no way of connecting with these people in person and asking them to give me a chance.
3. I am not strong in mathematics. In fact, I'm not strong in any kind of theory that I can't immediately apply. While I will definitely get around to learning the theories that drive machine learning once I get more into it, I don't want to start with theory and then learn how I can bring it into the real world.

So, is there a way of getting a remote job in my predicament? If not, how can I get myself to a place where I would be hired, while keeping my day job of writing back-end systems with PHP?

Thank you guys in advance for your help. You're awesome!
comment_no.=000--> I don't know why you would think it's easier somewhere else. Most of these jobs require a postgraduate (Masters or PhD) degree. Not being strong in mathematics means your resume is in the trash before I even finish reading it because I don't want to spend 5 years training someone if I need someone NOW.

Your comment #3 has all kinds of red flags, even if you wrote the opposite of the first sentence. Your best bet is to take some statistics courses on EdX. You'll probably need prerequisite math up to calculus II to get through it. Then, start working with some open source libraries and public data to prove you have the chops. I'd make it a 5-10 year plan so not having anything in your home country for the next couple years won't matter if you really want to get into the field.
-------------
Question=000057  Does there exist any ML algorithm to detect pivotal input parameter?
I want to know if there exist any neural network algorithm that allows you to determine which out of all the input parameters is the pivotal factor. Or in other words can I detect the input parameter, changing the values of which yields maximum variation in the output value.

For instance, I am doing a project to determine the Air Quality Index of any given city using Neural Networks. As input parameters I have considered a variety of meteorological factors (like wind speed, temperature, air pressure, humidity etc) + Air pollution from cars + Air Pollution from industries and my output parameter is PM 2.5 value.

Now I want to deduce, out of all these input factors which is the major contributor to the PM 2.5 value or changes in the values of which of the input parameters changes the output value significantly.
comment_no.=000--> You may want to look at traditional statistical techniques like regression if you need an interpretable model.

[People have done work to help us understand black-box algorithms like neural networks, ](http://arxiv.org/abs/1602.04938) but at the end of the day neural networks aren't easily interpretable and you are better off with good-ol regression unless your problem can't be solved without the added expressiveness that deep learning offers.
comment_no.=001--> I agree with /u/Eurchus.  There will be statistical methods that aren't "machine learning" that can answer this - and whenever that's the case, I always recommend going with the standard statistical method.

It also depends on what "pivotal" means and how you measure it.  I'm going to assume you mean - "If I only had access to just one of these variables, which one will best predict my dependent variable".  In that case one option is to use a Decision Tree.  The decision trees I've used depend on entropy and look for which variable at each level reduces the uncertainty the most.  So the first split will be based on that variable that removes the most uncertainty from the outcome when you know its value.  One challenge with decision trees (at least the ones I've used) is that they tend to work more naturally with categorical variables.

It might be worth asking this question over in /r/AskStatistics.
comment_no.=002--> As a rough rule of thumb, you can look at the size of an input's outgoing weights to estimate it's importance. (And you could multiply it by the importance of the node a weight is connected to, which you determine in the same way.)

You can also do "ablation tests" where you remove each input (or combination of inputs) and see how much this changes the output, either in absolute terms or in terms of error. You can do this after training to find out the important of a (set of) parameter(s) in the final network, or before training to find out how important it is to begin with. 
-------------
Question=000058  Does having a degree from a ranked university ultimately matter?
I started taking CS courses at a local university (University of Missouri St. Louis) and wound up really enjoying it. A lot of the faculty here do research in genetic/evolutionary algorithms/programming  and I ended up taking some graduate level courses too.

Do you all think it would be worth it to get the MS at an unranked university (though its sister school in Columbia is [ranked #101](http://grad-schools.usnews.rankingsandreviews.com/best-graduate-schools/top-science-schools/university-of-missouri-columbia-178396) ), or would it be wiser for me to just bite the bullet and apply elsewhere, such as Urbana Champaign or somewhere else of the like? 

Thanks! 

(edit: spelling*)
comment_no.=000--> You're in one of the fastest growing fields in the world. Don't worry about ranks and do what you like. Be interested, have fun and the rest will happen automatically.
-------------
Question=000059  Good language for introduction to self-modifying algorithms?
Hello, So I am trying to find a language with which i can write code to build/search through deductive reasoning 'nets', as well as self-modify it's search algorithms based on information learned from these nets.

I also want a language that i can use to write scripts for a 2d game engine, as i would like to build visual models of my projects for a web page(to help my job/school prospects).

So far i am only really familiar with MySQL(been working full time as a backend developer for about 7 months), but I have spent quite a bit of time developing relatively formal models for problem solving that i would like to attempt to put into code.  

Any advice/suggestions would be greatly appreciated, thank you!
comment_no.=000--> Well. What kind models? 
That being said, the language you want to mess around is probably python. 
comment_no.=001--> Python is very popular for AI/ML stuff in general and it's pretty widely used to make websites. That means there's potentially a lot of support (frameworks etc.). I wouldn't say it's uniquely suited for self modifying programs though. For that I'd go with (some variant of) LISP.
-------------
Question=000060  Resources for Text Mining/Auto Writing?
There seems to be a lot of data on generalized numeric data, or vision systems. But where are there resources on Text Mining/Writing applications?

I've had a lot of difficulty finding resources to learn about these fields. Do you have any to share? Most frameworks seem to focus purely on numeric data for analyzing vision/sound as first class citizens.
comment_no.=000--> I've continued searching so far I've found [this](https://www.cs.umd.edu/~miyyer/pubs/2014_nips_generation.pdf)
-------------
Question=000061  Wanting to categorize images based on metadata
-------------
Question=000062  back propagation DNN, question about layers
When you are using a DNN is a good strategy to make the number of neurons in a layer always decreasing, from the input to the output?
comment_no.=000--> no.  easy example: auto encoder
-------------
Question=000063  Need help on testing a small image in a CNN
-------------
Question=000064  Extending LDA
-------------
Question=000065  Help with SVM math



	I'm trying to do Support Vector Machine classification. I understand in general how it works, although not all of the nitty gritty math behind it. There's one thing I'm confused about in particular. With reference to this: https://www.csie.ntu.edu.tw/~cjlin/...guide/guide.pdf

it says we want to minimize 1/2 * w^T * w or alternatively 1/2 * ||w||^2

I know that the width of the margin is 2/||w|| so we want to minimize ||w||. Can someone explain to me where the 1/2 and square term come in? 
comment_no.=000--> The 1/2 is added because it leaves the solution unchanged but it gives a better looking solution after derivation. 
-------------
Question=000066  Detecting a specific ring in an audiobook using ANN
-------------
Question=000067  How to re-train a NN with more data?
If I have a trained NN with a training set of 100.000 inputs with diferent features, and I want to re-train the NN with lets say 1.000 new inputs, how can I train the NN, given that it was trained with 100.000 data and now I have only 1.000.

I hope you understand me.
comment_no.=000--> By inputs you mean the number of samples in your data sets, right? Not the number of input nodes of your network.

If the inputs and outputs in both data sets follow the same format, then you can basically do whatever you want. You can now start training on your new data set exclusively, or you can merge them together into a single 101,000 sample data set. Or if you want to keep training on both, but you want to make the second data set more important, you can duplicate it a few times. For instance, you can make 10 copies of each sample in the second data set and combine it with the first to make a 110,000 sample data set. Then you just continue training on that.
-------------
Question=000068  Classification with non-independent, structured, data
-------------
Question=000069  Help with Twitter rumor detection research
-------------
Question=000070  Algorithm for actuator control
-------------
Question=000071  Can you train a network to recognize a pattern, then produce a function that can be used independently of the network?
for example: I want to make an app that detects when I say "hello". From my understanding, I could train a neural network with a bunch of clips of me saying "hello". Would it be possible to extract the network's "knowledge" of what my hello sounds like?

Admittedly I know next to nothing about machine learning, just curious about its limitations and uses.


comment_no.=000--> The short answer is yes, that is totally possible! A great example would be the MNIST Dataset, which contains thousands of handwritten digits as 28×28 images. You can train a neural network, which finds patterns in these images and learns what the numbers 0-9 look like. Once your network has been fed a lot of example images and learned these different patterns, you can the then output what the NN thinks a certain digit looks like. 

I'm on transit atm, will provide links when I get home. 

If this sort of stuff interests you, there some great resources to get started online, just have a look about in this subreddit :)



EDIT: links as promised

1. Visualization of features learned in the MNIST Dataset mentioned above
https://www.tensorflow.org/versions/r0.7/tutorials/mnist/beginners/index.html

2. Get started with some literature
https://www.reddit.com/r/MachineLearning/comments/1jeawf/machine_learning_books/

3. Online Courses
https://www.springboard.com/blog/machine-learning-online-courses/

I would definitely recommend you start easy, do some linear regression with Python/R/Octave and get a feel for the workflow and the Frameworks available out there!

-------------
Question=000072  Understanding Tensorflow LSTM models input?
-------------
Question=000073  presenting large image data to CNN
Hello all,   
I am currently working on a project to recognize features on Mars' surface using a CNN.  An example would be to recognize sand dunes from a large .tif image of an area of interest.  The only dataset I have is a large 30k by 100k .tif image of an area that  has sand dunes and the same image with the sand dunes being marked in white.  I plan to use the annotated image as training and the untouched image as testing.  How would I approach presenting the data to the CNN?  Slicing the image in smaller blocks seems ideal, but that's still a lot of images I think, and would require a lot of memory.  What is the best way to go about this?  
  
Edit: I should also mention that I'm using Theano+Lasagne and Python 2.7  
comment_no.=000--> 1. You don't have to use all of the patches, use the amount your HW allow you to.
2. You could split the patches and read them from the HD when needed

I would go with option 1 as reading from the disk will slow you down quite a bit. And probably lots of the drawn patches are correlated anyway.

-------------
Question=000074  How can I impute single observations in R?
-------------
Question=000075  RBM + ConvNets?
-------------
Question=000076  MLQuestions tutoring on Lasagne, or lasagne for dummies.
Hi,

I have being studying NN's for hobby, and now I decided to move to libs instead of coding simple MLP with sigmoid activations.

 I found lasagne, the idea of stacking layers was to me very interesting, but while reading the doc's a couple of things seems confusing to me (probably the part of theano), and that is the motive of this post.

Since I didn't find a better way to present my doubts, I'll post the code and make questions along the way.

Let's say that we have a train.csv, which has 10000 entries, each row with 15 attributes and one label (1 or 0). The the program would probably begin with:

    import pandas as pd
    import theano.tensor as T
    from lasagne.layers import InputLayer, DenseLayer, get_output
    from lasagne.nonlinearities import rectify, softmax
    from lasagne.objectives import categorical_crossentropy
    from lasagne.updates import nesterov_momentum
    
    train = pd.read_csv('train.csv')
    
    l_in = InputLayer((None,15)) 
    l_hi = DenseLayer(l_in, num_units = 500, nonlinearity = rectify)
    l_out = DenseLayer(l_hi, num_units = 2, nonlinearity = softmax)
    pred = get_output(l_out)

So far, so good. The problem start below

    target_var = theano.tensor.scalar('target')
    loss = categorical_crossentropy(pred, target_var)

If I understood theano correctly, all this is some sort of memory allocation without "real" computation been made. But, trying to run this piece of code generates the fallowing error:

    Traceback (most recent call last):
      File "nn.py", line 20, in <module>
      loss = categorical_crossentropy(pred, target_var)
      File "/usr/local/lib/python2.7/site-packages/lasagne/objectives.py", line 129, in categorical_crossentropy
      return theano.tensor.nnet.categorical_crossentropy(predictions, targets)
      File "/usr/local/lib/python2.7/site-packages/theano/tensor/nnet/nnet.py", line 1842, in categorical_crossentropy
      raise TypeError('rank mismatch between coding and true distributions')
    TypeError: rank mismatch between coding and true distributions

Since the label is '1' or '0', shouldn't 'target_var' be an scalar?

    loss = loss.mean()
Bonus question, is it possible to retrieve the values of the loss function in each epoch, to plot a graph?

    params = get_all_params(l_out,trainable=True)
Bonus question 2, been the network trained, params can be saved in on file and be read later?
    updates = nesterov_momentum(loss,params,learning_rate=0.01,momentum=0.9)
    
    test_pred = get_output(l_out,deterministic=True)
    test_loss = categorical_crossentropy(test_pred,target_var)
    test_loss = test_loss.mean()
    
    test_acc = T.mean(T.eq(T.argmax(test_pred, axis=1), target_var),dtype=theano.config.floatX)
    
    input_var = theano.tensor.vector('input') # correct type?
    train_fn = theano.function([input_var, target_var], loss, updates=updates)
    
And, for the training part, is it the correct procedure?

    for i in range(len(train)):
        input = train.iloc[range(15)]
        target = train['target']
        train(input, target)

To finish, if I just want to evaluate the output of the network, can I just use
    output = get_output(l_out)
or it has to be a theano function?



Note, that the code above (with the exception of the code till the error message) is not a "real"code I just typed (based on the tutorial) to try to understand better the mechanics of lasagne.

PS: It might be a little confusing because it is 4am and I have the wake at 6am. Still, any help would be appreciated. 
comment_no.=000--> 
>    loss = categorical_crossentropy(pred, target_var)

This is wrong. If you have 5 predictions, you should have 5 target variables. They should match shape. Aka, both be (1,k) or (k,1) or (k,). 

--

>      raise TypeError('rank mismatch between coding and true distributions')

That's why you have this error. The shapes don't match.

--

>Bonus question, is it possible to retrieve the values of the loss function in each epoch, to plot a graph?

Loss is returned from your compiled function.

>    train_fn = theano.function([input_var, target_var], loss, updates=updates)

This is your compiled function. It's basically like specifying: 

def train_fn(input_var, target_var):
     return loss

--

>        train(input, target)

You should have 

loss = train_fn(inputs, targets) 

--

In general, I find exploring theano code via a shell or notebook very informative. Most of your confusions would go away if you did this and tested your questions empirically. 
-------------
Question=000077  Can the methods of Sutskever et. al. (2013) be applied to LSTM-RNNs as well?
-------------
Question=000078  Translation invariance in image recognition, without conv nets
-------------
Question=000079  Probably not a great question.
I don't know that this is the proper venue for this question but it's here that my searching has led me so it's here that I'll stage myself. 

I dropped out of college 3 years ago and have recently begun learning programming again. I have a decent grasp on basic programming fundamentals and am relearning a lot of college math. Currently working through a college algebra mooc then moving on to bigger and better things. So as someone with relatively little background, where should I start learning about machine learning. I'm not looking to get a job or anything like that, it's more of a curiosity. I'd like to do a project of some sort using it but first I need to actually learn HOW to use it. I understand that there is a LOT of math, so any direction toward online resources or books or what have you would be immensely helpful. 

Thank you in advance for your time.
comment_no.=000--> Machine Learning - Coursera, Stanford University is your next stop after you know the basics of linear algebra
comment_no.=001--> The three most important math classes for machine learning are probability theory, multivariable calc, and linear algebra. Those are all roughly sophomore level applied math classes. After you're acquainted with those topic check out the coursera mooc.
-------------
Question=000080  Stationarity of prefrences
-------------
Question=000081  Python2 or Python3 for ML?

comment_no.=000--> **3**

Edit, since you asked for it:

Because there is almost no argument for 2 anymore. It still has some libraries that are not available for 3, but those are legacy and you can probably find good, if not better replacements.

* Python 2 will lose it's support eventually, while 3 continues to be supported; it is the newest version.
* New libraries are primarily developed for 3, although some still support both versions.
* Python 3 natively supports unicode, which avoids some problems, especially for newcomers.
* Python 3 comes bundled with pip, a package manager. While people argue that (ana)conda is a better package manager, pip often suffices.
* Python 3 is (in my opinion) more consistent. Things that were done in Python 2 implicitly now have to be done explicitly, which avoids quite some confusion.

Also have a look at: https://wiki.python.org/moin/Python2orPython3

Note that the lack of library support is not really relevant anymore. I have never found a library that I needed that was available in 2, but not in 3.
-------------
Question=000082  Code for image segmentation? Semantic segmentation?
Anyone know a good github repo or other code source for state-of-the-art **image segmentation** or semantic segmentation implementations? Any language or library is fine, I'm just looking for source to compare with and maybe run as a benchmark.


comment_no.=000--> [Here's one](https://github.com/BVLC/caffe/wiki/Model-Zoo#fully-convolutional-semantic-segmentation-models-fcn-xs)

Credit for finding it goes to u/BeatLeJuce for answering my question on the 'ml simple questions thread' on /r/machinelearning


-------------
Question=000083  Convolutional Network: How to determine filter size and number?
Hi, Say you have an 80x80 image, how does one determine the filter size and the number of filters in the first conv layer?
comment_no.=000--> Rules of thumb + trial and error (cross-validation)

https://www.reddit.com/r/MachineLearning/comments/43va6p/is_there_any_logic_behind_the_design_of/

https://www.reddit.com/r/MachineLearning/comments/36rd91/any_reason_behind_the_fact_that_the_filter_size/

https://www.reddit.com/r/MachineLearning/comments/3l5qu7/rules_of_thumb_for_cnn_architectures/
-------------
Question=000084  Machine Learning for determining best network speed ?
-------------
Question=000085  LSTM sequence-wise back propagation of losses in theano
-------------
Question=000086  CS Final Year Project coming up, suggestions needed!
-------------
Question=000087  Importance of transfer learning on semantic segmentation
-------------
Question=000088  Did anyone get the Long-term Recurrent Convolutional Networks activity recognition to work ?
-------------
Question=000089  What are some good concepts/algorithms for this problem?
Disclaimer: I'm not a ML person, and this is not a homework assignment.

There is a deterministic two-player full-information game. (Tic-Tac-Toe, Chess, Go, Diplomacy, etc. etc.). I do know what the game is, but it's pretty silly, so let's just pretend its Chess instead.

So the task that I have to complete is simply to write a heuristic that takes a state of my game and evaluates to some number-value. For example, for Chess, maybe I can write a program that just counts the number of White Pieces on the board. A naive heuristic, but that's the type of program that I could use.

I was thinking it would be pretty cool if I could use some Machine Learning algorithms to learn a heuristic and train it on some data of game records.

What Machine Learning algorithm is best for this sort of task?
Taking a state -> return a number value describing how good the state is

For example, I was taking to my friend about making a neural network. But he said that would be totally inappropriate for my kind of task. If that's bad. What's good?
comment_no.=000--> have a look at the alphago design https://googleblog.blogspot.com/2016/01/alphago-machine-learning-game-go.html?m=1

in essence it combines heuristic evaluation, policy evaluation, tree search, and reinforcement learning.

it is absolutely possible to use neural nets for heuristic and even policy evaluation, but they are hungry for labeled training examples.
comment_no.=001--> I tried to do something like this with Mancala once.
There are a couple different things you can try. You can learn a function for predicting wins from a board state, and/or learn a function for predicting the likelihood of a move given past human games. Both of these would help inform a search algorithm.

The usual way is to train a neural network. I belive I used pylearn2. For predicting board state probability of victory, Your game state comes in as a high dimensional vector, and your value goes out as a 1 dimensional scalar. The likelihood of each move by a human can also be learned in this way, where the output layer of the network has a node for each possible move, and the value of the node is the probability of the move. (don't try using a more compressed representation of the move, you'll just make it harder on the NN)
comment_no.=002--> Honestly, for this task I have used Monte Carlo Tree Search and suggest it. It requires only minimal tuning for each game you apply it to. 

Training a game-specific state evaluation is going to be very dependent upon the specifics of the game. I mean you CAN just use a SVM or ANN or what have you on your state vector but for it to really work well you want to figure out what is the relevant information to do your regression on... this can be very tricky. 

What is the game? I mean, why are you embarrassed because you are trying to solve a silly game? Look at the cool work these guys did with Chutes & Ladders: http://www.datagenetics.com/blog/november12011/

EDIT: Monte Carlo Tree Search (MCTS) is a simple algorithm. It isn't a machine learning technique per se, but it can be combined with them. In its simplest form: From a single game state assume all subsequent plays are made randomly. Play many random games starting at that state. Record the ratios of wins to losses. There are many, many ways to improve on this, and the most important is to bias the random plays towards ones that are more 'reasonable' or 'likely'. 
-------------
Question=000090  "Are you sure you should be doing that?"
Hey people,

I have a non-critical system I'm working on that logs when people decide to do certain things. I want to flag when users do things that don't match what they've done before. An analogy would be "Joe posts on /r/machinelearning at noon every Tuesday".

Here is how I'm thinking about this:

* Output: Which subreddit Joe posted on
* Inputs: Time of day, day of week

Over time, the system "learns" when Joe is likely to post on what subreddit with the ultimate goal being something like this. One day, Joe posts on /r/cars on Tuesday at noon. The system notices this and says "Hey Joe, are you sure you should be posting on /r/cars? Usually, you post on /r/machinelearning at this time."

Another case would be that Tuesday at noon rolls around and Joe does NOT post on anything. The system notices this and flags it for Joe: "Hey Joe, should you be posting on /r/machinelearning?"

The system gets to watch Joe's posting history as long as is necessary for there to be some confidence in the flags that are being generated.

In reality, Joe is not actually watching the flags, these are flags being aggregated across many different people and the flags are just reports generated for the PHBs to decide whether to investigate or not. Then those decisions are fed back into the system.

Besides reading a book on neural networks many many years ago, I am a noob to machine learning. However, this screams to me to be some sort of machine learning project.

Am I way off? How would I approach it?

Thanks.
comment_no.=000--> I think what you're looking for is "anomaly detection".
comment_no.=001--> If Joe always posts on r/machinelearning on Tuesday, and then one week he posts to r/cars on Wednesday, shouldn't that also be a red flag? If Joe is a r/____ enthusiast, then why would the time be important at all?
comment_no.=002--> Disclaimer: I'm not a computer scientist.

Regarding methodologies, if you have a string of characters representing [day of week] and [time of day], then predicting the characters that follow (representing subreddit) is like an n-gram problem in natural language processing. If you express the input as a fixed-length input sequence then you can indeed use a simple feed-forward neural network to learn the fixed-length output sequence representing subreddit. NN isn't necessary though (it's just my hammer).

If you just take the data points of [time of week,subreddit], and plot them in 2D, then you will see clusters around certain locations. This is assuming you have paired a number with each subreddit (which others might point out as being a bad practice unless the number assignments are not arbitrary). Checking if an action is out of place is then like checking to see if a new point is within some distance of an existing cluster. I don't know anything about cluster analysis... From the perspective of finding an outlier, I would just fit the data to something like a high-degree polynomial curve and check how far away new datapoints are from that line, relative to the standard deviation of the fitting.

If you discretize the [time of week] parameter, then each subreddit can simply have a probability associated with it based on previous statistics. So if I've visited r/a 5 times in time-slot 1 and r/b 10 times in time-slot 1 in the past, then there's a 66% chance that I'll visit r/b during the next time-slot 1, where "time-slot 1" would represent a broad bin like "Monday afternoon". The bins can be made less broad as more data is collected.
-------------
Question=000091  Building an ML team for commodities trading
I hope this is the correct place to post and apologies beforehand if not.

I am a commodities trader and I run my own business. We have a large unique data set and so I am trying to build an ML team to sift through and look for possible trades / trends etc. 

I am however - not a tech person. What is the easiest and minimum path to gaining sufficient understanding of ML/DL to be able to have a rudimentary understanding of what we should be doing as a firm and to be able to converse meaningfully with the members of my team. I have a Masters in Economics but I have not done math for a LONG time - so assume I would be starting from zero......


comment_no.=000--> Start with this.
https://class.coursera.org/ml-005/lecture
-------------
Question=000092  How to explain sudden jump in precision
I was training a single layer LSTM model to do sentiment analysis on imdb corpse with a binary bucket. I use pretrained word embedding and padding the sentence to the max and softmax to determine the loss and binary result. I set the learning rate at a pretty high level, and the precision/correctness keeps around 50% for a while, and then suddenly jumps to 98% in one single epoch and then stables at 98%. However, cost/loss doesn't change much. How would someone explain such a jump? All the training process I have seen involve gradual increase in precision, not like this. Is it possible that the prediction just shift over 0.5 line, and thus change the precision a great deal, but not that much for loss?
comment_no.=000--> Did you mean corpus?
comment_no.=001--> Could you share any graphs or code?
-------------
Question=000093  Good tutorials on spiking neurons?
-------------
Question=000094  What is the difference between biological and artificial neural networks?

comment_no.=000--> The main difference is that a biological nueron is uni directional. Basically, if we think of electricity as water, and a neuron as a bucket, biological information flow happens like this:

Imagine 4 empty buckets arranged as tightly as possible together.  Now, lifted over the center of those buckets, imagine another empty bucket suspended somehow.

Imagine you start to pour cups of water into the top bucket. In the beginning, none of that water goes into the bottom buckets. The bucket is filling, or the neuron is polarizing. 

Imagine that each bucket is set up to tip over and spill all its water at once as soon as it reaches a certain volume of water. So you keep pouring cups of water in, still nothing goes into the buckets below.

You're pouring and pouring. Suddenly, as soon as you reach the tipping point, the bucket tips over and spills all of its water into the downstream buckets. It slowly returns back to normal (I.e. Standing and empty)

Meanwhile all the downstream buckets, are only partially filled and none of their water gets to the buckets below them and so it continues.

The cups of water you poured into the first neuron represent each time the neuron was activated from upstream. The size of the buckets are usually the same. (The amount of water it takes for them to tip). But the amount of water they recieve is not always the same.

The buckets underneath cannot communicate with the buckets above usually.

So that's how biological neurons work in a really simplified fashion. Millions of buckets arranged on top of other buckets on top of other buckets.


When it comes to artificial networks backpropagation maybe sort of works like the initial evolution of the biological network in the developing mind. But it doesn't really work that way once the neurons are laid out.

Also in the animal brain, there is a network for each thing our brain can do. One for sight, within sight one for motion, one for contrast etc, one for hearing, one for words, one for singing, one for smell, etc, and they're all integrated centrally. There is a memory relay that the sensory centers connect to, it gets solidified during  dreaming, etc, there ate networks for balance, hunger, love, muscle movements, tools, actions, names, etc.

The 6 layeted cortex in humans acts like a 6 layer convergence pattern. You just throw an entire tidal wave of data at a sensor. The first layer just reads how the sensor responds without making any sense of it. The next layer detects patterns in that noise, the next detects patterns in that layer and so on and so forth.

The result at the end is often fed into a network for concepts and then vocabulary of any language. The network for vocabulary is tied with the network for mouth movement, sound etc and so forth, and that's how we create skynet. 
-------------
Question=000095  Text Mapping via Universal Taxonomy - Looking for practitioners to test use-cases
-------------
Question=000096  Using Regression to solve for multiple variables
I'm just getting started in my first ML project. I wanted to choose something I am interested in for my first project so I decided to try to predict daily fantasy sports outcomes using ML. I read this paper http://cs229.stanford.edu/proj2015/104_report.pdf and I have a few questions that are probably pretty basic. Can linear regression be used to predict more than 1 variable at once? We care about multiple variables after a game which make up a players "score" but only have a small amount of information  pregame to feed for a prediction. Before the game we have features such as player name, home/away, opponent, team, team win %, opp win %, vegas o/u, etc. but we want to predict multiple variables such as points, assists, rebounds, etc. Do we predict each one individually or am I missing something obvious?
comment_no.=000--> In MLR proper, you have Y = X \beta + \epsilon, with Y being the response vector, X being the n x p matrix of observations x predictors, \beta the regression coefficients, \epsilon the error vector.

If you care about multiple variables that aren't transformations of your original Y, then no, because you'd be fitting the same regression coefficients for different variables.

Additionally, unless you're certain that all predictors are useful for predicting all the different variables (due to domain knowledge for example), then it may not even make sense to fit the same model for all variables.
-------------
Question=000097  Exploding linear value function approximation
-------------
Question=000098  Dimensionality in the Bayes decision rule for normally distributed classes.
-------------
Question=000099  Is there any book/lecture series/MOOC for Unsupervised learning?
Apart from K-Means Clustering and some Gaussian stuff, I could not find much on Unsupervised Learning. Are there any set of Tutorials to get started with Unsupervised Learning? Or a book or a thesis of a PhD candidate?

P.S. I did my fair share of searching before asking.
comment_no.=000--> Try the MOOC on Udacity. It is great for beginners and it is really fun!
-------------
Question=000100  How to calculate equal error rate (EER) in multi-class decision?
-------------
Question=000101  What Activation Function to use for Convolutional Layers?
I've read that relu activation functions are used in convolutional layers, but I'm wondering why.

From playing around with a self-made implementation, I'm seeing that the sigmoid activation function works great but the relu and softplus activation functions are resulting in an exploding gradient.
comment_no.=000--> Relus are the best default choice, they should outperform sigmoid if you are just doing standard CNN stuff. If you use relus in your case and it fails to converge, you should check your implementation. 
-------------
Question=000102  Project Help/Advice
-------------
Question=000103  General recurrent neural networks
-------------
Question=000104  I do not understand a step in neural networks, am i missing something?
So i am a programmer, getting into AI/neural networks. Kind of started reading into it and a few tutorials. Mostly basic stuff (teach a few neurons to simulate/figure out how to be an AND/OR gate etc.)

Now i get how those neural networks work, they have input, then neurons with gain and all(?) inputs on them with a weight. Then they provide an output, which you put through another set of neurons which connects to all of the first lines of neurons, and they generate output for all your outputs.

Given this thing, and say a car (output: forward, steer left, steer right) and input: (distance to wall forward, distance to wall left of nose, distance to wall right of nose, distance to goal)

Then write a feedback loop to reward the neural network for not colliding (or punish collisions) and reward it got getting closer to the goal. (using back propagation)

Now we give it an area, set it in a simple maze, simple, but required backtracking or going away from the goal to get to the goal.

We let this neutal network run, give it random values, tons of iterations and generations, whatever way you like it.
No matter how i imagine this thing working, i can't imagine any other situation then ending up with a car, that will steer straight to the goal, and stops JUST in front of wall, and then just sits there. Doing nothing at all, because it can't for the life of him identify or work with the maze in order to go through.

Am i missing something here? Does the neural network have hidden magic i don't get. Or am i simply lacking inputs to complete the task? I can't really find any good examples online to look at either. They either do basic stuff, mayor hand holding (for the AI), or are insanely difficult to understand.
I want to understand if a neural network can actually figure out a complicated problem like this given enough time/processing power and limited inputs and fixed outputs like described. 
comment_no.=000--> I think you're correct in assuming that the system you've described wouldn't really work. It's not because of any intrinsic weakness of neural networks - I think you've just chosen a poor objective function. 

You probably want to only reward reaching the goal (and optimize for doing so in as few steps as possible).
comment_no.=001--> I think understanding some more basic machine learning algorithms would help.

Have you done linear regression and logistic regression?

Do you know the difference between regression and classification? (All ML algorithms have essentially the same API, which allows you to plug-and-play algorithms like the ones included in sci-kit learn with ease)

Logistic regression is basically the same form as linear regression with a sigmoid on top that will make the output between 0 and 1.

Neural networks are basically layers of logistic regressions stacked up.

I've created a bunch of courses on Udemy that go through this progression:

https://www.udemy.com/data-science-linear-regression-in-python

https://www.udemy.com/data-science-logistic-regression-in-python

https://www.udemy.com/data-science-deep-learning-in-python

https://www.udemy.com/data-science-deep-learning-in-theano-tensorflow

[Apologies if I understood you wrong and you're just trying to do reinforcement learning exclusively. Although I do have plans to do a course on that soon also. What I'm trying to get at here is that if you used a simpler linear model with the exact same inputs and outputs, it might give you some more insight into what's happening.]
comment_no.=002--> Try experimenting with [genetic algorithms](http://blog.otoro.net/2015/03/28/neural-slime-volleyball/) and [reinforcement learning](http://cs.stanford.edu/people/karpathy/convnetjs/demo/rldemo.html).
-------------
Question=000105  ML Options for Selling Ranking?
I am currently experimenting with being a seller on Amazon. One of the features they have is to allow for advertising campaigns. They allow for you to 'bid' on certain keywords so your product appears when you search for that particular phrase. For a particular campaign, you can have many keywords and give them a daily budget. At the end of each day, you see how these performed through various metrics, culminating in the ROI for that keyword.

I wanted to use an algorithm to help figure out after every week which keywords were worth investing more funds into. For these types of problems, what algorithms would be best suited? Are there any services (Azure, AWS, etc) that would aid in this research?
comment_no.=000--> I don't want to belittle ML, but wouldn't this seem to be something that could more easily/efficiently be accomplished by just comparing things such as clickthrough/items shipped/etc. ? Just having some kind of comparison of revenue to cost would seem to do exactly what you want, and is a lot less taxing to implement!
-------------
Question=000106  Cost exploding in squared loss optimization. How to avoid it?
-------------
Question=000107  Teaching machine learning code
-------------
Question=000108  Is Donald Trump an artifact of overfitting?

comment_no.=000--> Over-regularization on noisy data,leading to regression to mean.
comment_no.=001--> I'm more thinking some kind of deep instability in the optimization algorithm, leading to a pathological but unphysical solution. 
comment_no.=002--> His unpredictability and changing opinions would point to it yes. 
comment_no.=003--> Does this thread belong in this subreddit?
-------------
Question=000109  Use of Option Parser in sklearn
-------------
Question=000110  Replicating Neural Style
It seems a lot of people have been replicating the results of this paper http://arxiv.org/pdf/1508.06576v2.pdf, training a CNN to recreate an image in the style of another.

I would like to try doing this, but I'm not perfectly clear on how it's done. From what I understand, the CNN is essentially trained to map randomly generated pixels to the pixels of the desired picture? Or is it much more complicated than that?
comment_no.=000--> Have you checked something like this: https://github.com/jcjohnson/neural-style ?
or this: https://github.com/anishathalye/neural-style ?
-------------
Question=000111  Validation accuracy higher when feed-forward with dropout than without
I am training with dropout. If I feed the validation set forward with dropout still on the validation error is lower. If I turn it off, the validation error is far higher. Ideas? Note that this issue isn't present for a shallow net, but it is for a very deep net.
comment_no.=000--> Are you dividing the weights by half ? At rest time or at validation, you need to divide the weights by half if you're using 0.5 as dropout probability.
-------------
Question=000112  Hoping for some guidance in selecting models for feature extraction
Hi, I've been studying machine learning over the last couple of months with the hope of solving a specific problem. I'm hoping someone can help me with advice on what approach to take.

I have a large collection of labeled, connected graphs on which I would like to do unsupervised feature extraction. Ideally I want to discover higher level features present in the graphs, (similar to these features described by Prof Ng in this [video](https://youtu.be/ZmNOAtZIgIk?t=29m36s)) and be able to generate random graphs composed of only those features.

From my research, autoencoders seem to be used for similar problems. Does that sound right? Any advice or suggestions would be much appreciated. Thanks!
comment_no.=000--> Okay, so estimating "features" of a graph and simulating graphs can be difficult.

The best way I know about to do that is based on the "kronecker graph" idea. You suppose your graph can be generated by kronecker product from a small graph.

This actually yields excellent results, close to natural graphs. Now what you want to do is estimate the generator of your graph, and the simulate other graphs from it.

Here is the paper that introduced kronecker graphs, and how to estimate our generator.

https://cs.stanford.edu/people/jure/pubs/kronecker-jmlr10.pdf
-------------
Question=000113  How can I create a topic model with a mixture of multinomials and EM?
-------------
Question=000114  Parameter learning in LSTM
-------------
Question=000115  Question about best matlab libraries for certain classifiers and dealing with NaN values
-------------
Question=000116  Get dataset used in Learning to Execute
Zaremba put up his Learning to Execute code. https://github.com/wojciechz/learning_to_execute
 I want to use data from this program, but I do not know torch so I don't know how to obtain the data. What lines should I add to get the data, or does anyone have a set saved anywhere?
comment_no.=000--> Did you read his paper? He uses the Penn Tree Bank data set. Its a common benchmark. I believe the paper explicitly states how to get it from Mikolov's website.
-------------
Question=000117  Need some direction for doing shared parameter regression.
I have some y vs x data for a number of sub-populations in a population.  For the sake of this question, the data for each sub-population can be modelled using linear regression, y = m x + b.  There are only a handful of unique m and b values that are shared amongst the different sub-populations, and there are typically many fewer unique m's and b's than there are sub-populations.  We don't always know a priori which sub-population matches which m and b values, nor do we always know how many unique values of each there are.  For this problem it is equally as important to get the correct number of unique values as it is to determine what those values are.

I have tried two ways to do this so far:

 * The first way is to decide a priori which sub-populations will share which parameters (for our data this works some of the time).  This way is very fast, but it isn't always a great model because there is no general rule for deciding this.

 * The second way is to find m and b for all sub-populations, then find the unique values by using some clustering methods.  This way takes much more cpu time, which isn't great for our setup but isn't the end of the world; and sometimes we don't have enough data for all sub-populations to make reasonable estimates of the parameters.

Ideally there is a third way that both decides on which parameters are shared by which sub-populations and what the unique values of those parameters are.  

What I am looking for is some literature, or a specific topic to research on.  I have prior distributions for the model parameters, so bonus made up internet points to you if you can suggest a bayesian method.
comment_no.=000--> How about gradient descent https://spin.atomicobject.com/2014/06/24/gradient-descent-linear-regression/
-------------
Question=000118  I would like to perform access control based on a learned confidence level - controlling a gate automatically with a license plate reader. Possible?
-------------
Question=000119  Are there databases for malicious javascript?
-------------
Question=000120  ML for Java Dev
-------------
Question=000121  Giving probability distribution as label
I want to implement a neural net that will recognize images but get a discreet probability distribution as labels while training. Right now, I have it for fixed labels. Now, I want to change this such that I give it the probability distribution instead of the one fixed label. Can I simply do this by changing the label to a sequence of labels, where each label is one probability? I can't find any information on this online.
comment_no.=000--> Why don't you use your softmax layer to directly learn the distribution instead of the labels ? That's one way to go about it.
-------------
Question=000122  What model should I use for predicting a certain value (task completion time) ? Details inside.
Hi All, 

I am new to this and I recently finished a beginner course on PluralSight [this](https://app.pluralsight.com/library/courses/r-understanding-machine-learning). And I recalled that at my workplace, I have a process which takes quite long time to complete and lot of other deliverables are dependent on this process to complete.

I have the historic data of that process which gives information like it's start date, end date and the start time and end time of the subprocesses that it is comprised of. How can use this data to predict the estimated end time of any fresh instance of this process? Which algorithm should I use? Any pointers are highly appreciated.
comment_no.=000--> Does the process itself have any input? How variable is the duration of the process?
comment_no.=001--> Firstly what is your baseline distribution of what you want to predict? 

What is the business value of the thing you are going to predict, and what representation would be most useful to those people? 

What would they do with the answer?
-------------
Question=000123  RNN - Vanishing or Exploding problem
I'm trying to understand an exercise from [Hinton's course on Neural Networks](https://www.coursera.org/course/neuralnets).

[Complete exercise](http://i.imgur.com/9QpYbsn.jpg)

Basically, I need to know if it's a vanishing or exploding problem.

So what I did was first calculate ALL the partial derivatives corresponding to "chain ruling" ∂E/∂Wxy and search for some light there:

    ∂E/∂Wxy = ∂E/∂y * ∂y/∂h3 * ∂h/∂z3 * ∂z3/∂h2 * ∂h2/∂z2 * ∂z2/∂h1 * ∂h1/∂z1 * ∂z1/∂Wxh

    Calculating each component:

    ∂E/∂y = - (t3 - y) = - (0.5 - y)
    ∂y/∂h3 = Why = 1
    ∂h3/∂z3 = h3(1 - h3)
    ∂z3/∂h2 = Whh = -2
    ∂h2/∂z2 = h2(1 - h2)
    ∂z2/∂h1 = Whh = -2
    ∂h1/∂z1 = h1(1 - h1)
    ∂z1/∂Wxh = x1

    So:

    ∂E/∂Wxy = - (0.5 - y) * 1 * h3(1 - h3) * (-2) * h2(1 - h2) * (-2) * h1(1 - h1) * x1

I saw nothing there.

So I payed attention to ∂y/∂z as the exercise says, and this is what I found:

    ∂y/∂z3 = ∂y/∂h3 * ∂h3/∂z3

    ∂y/∂h3 = Why = 1
    ∂h3/∂z3 = h3(1 - h3)

    So:

    ∂y/∂z3 = 1 * h3(1 - h3)

What I see there:

 - h3 is the logistic function, output always will be between (0; 1)
 - 1 minus something between (0; 1) is something between (0; 1)
 - and something between (0; 1) multiplied by something between (0; 1) is something between (0; 1)

So in conclusion, not only it will be something between (0; 1), but it will be pretty small because the multiplication in ∂y/∂z3, and then it will be much smaller because in ∂E/∂Wxy, that logistic function appears 3 times (one for each logistic hidden unit) multiplying together, and that will shrink the whole gradient a lot, independently of the other terms.

My question is, am I correct? and when can it be an *exploding* problem? because as I see here and with that logic (that maybe is wrong), it'll ALWAYS be a vanishing problem.
comment_no.=000--> To my knowledge, as long as you're using the sigmoid activation function then you'll only have the vanishing gradient problem. Notice that the the derivative of the sigmoid produces outputs on the range (0, 1/4]. Every time you backpropagate you're multiplying by the derivative of the sigmoid, and since it's always a fraction you're always making it smaller.
comment_no.=001--> I'm gonna add this here just in case someone has a similar question.

So yes, that deduction was correctly, and could find it [on wikipedia](https://en.wikipedia.org/wiki/Vanishing_gradient_problem) (yeap...):


> *Traditional activation functions such as the hyperbolic tangent function have gradients in the range (-1, 1) or [0, 1), and backpropagation computes gradients by the chain rule. This has the effect of multiplying n of these small numbers to compute gradients of the "front" layers in an n-layer network, meaning that the gradient (error signal) decreases exponentially with n and the front layers train very slowly.*

I suppose that the exploding problem occurs only when other kind of activation functions are used, that allow values over 1, that multiplying together over backprop can "explode" into some really big numbers.
-------------
Question=000124  Lagrange Formulation of SVM model - Question
I'm going through the Caltech lectures on Machine Learning and am confused by one step in the calculating the solution to the SVM model. In slide 13 (link below), why is there a minus sign in front of the summation? I thought that for constraints of the form g(x)>=0, you add a lamba * g(x) term to the lagrangian, not -lambda * g(x). Slide 9 shows that the constraint is of the form g(x)>=0.  I must be missing something small.

http://work.caltech.edu/slides/slides14.pdf
comment_no.=000--> That's because if you have an optimization problem
min f(x) subject to h(x) <= 0, 

the lagrangian will be
L = f + lambda * h where the lambda(i) are positive.


Here with h = - g, he used - lambda with lambda(i) >= 0 instead of +lambda with lambda(i) <= 0 for readability.
-------------
Question=000125  Comparing two models on different instances
-------------
Question=000126  Using Bayes Classifier in home automation to build an Alexa like assistant
Hi,

I have a lot of connected devices at home, I can control everything from my phone on a interface I made, but I would like to go further. 

My goal is to be able to text or speak with my home system, like a kind of Alexa/siri ( with very basic commands to begin ).

I think machine learning can be a good idea to implement that, using a Bayes Classifier ( I found this awesome library to do that : https://github.com/NaturalNode/natural#classifiers )

My goal is to train the system with sentence in input ( "Turn on the light" for example ), and in output the action ( "light-on" for example ), and then use the trained system to detect what I want to do when I speak to my house.

Do you think that's a good way of doing this ?

Thanks a lot,
comment_no.=000--> Mark?
-------------
Question=000127  Examples of Restricted Boltzmann Machines in TensorFlow?
-------------
Question=000128  1D Convolution in Neural Networks
-------------
Question=000129  Short time series of unequal lengths, weights changing with every time step, how to align?
-------------
Question=000130  Why is this NN so dumb?

comment_no.=000--> It's an LSTM put together with jupyter, keras and bokeh.

the yhat (green lines) should have distinct steps in them, like the y (black) but it doesn't seem to matter what hyperparameters are, it just wont fit nicely. 


[code is here](https://gist.github.com/emailgregn/e26a29557178101a2350)
and 

[sample data generator here](https://gist.github.com/emailgregn/ca7ae21181ae3d98abf6)
-------------
Question=000131  python KeyError
KeyError obtained when reading from a csv file. I've created an empty dict and want the most frequent values of a headed column (attribute) to populate the dict. 

Could someone please explain why this is occurring?

 

    def gain(data, attr, target_attr):
    """
    Calculates the information gain (reduction in entropy) that would
    result by splitting the data on the chosen attribute (attr).
    """
    val_freq = {}
    subset_entropy = 0.0

    # Calculate the frequency of each of the values in the target attribute
    for record in data:
        if record in val_freq[record[attr]]:
            val_freq[record[attr]] += 1.0
        else:
            val_freq[record[attr]] = 1.0
comment_no.=000--> You are testing if `record` is in `val_freq[record[attr]]` without being sure if `record[attr]` is in `val_freq`.

What you might want to do is to check:

    if record[attr] in val_freq:

Also you should make sure if `attr` is actually in `record`. I don't know how exactly your data looks like, but that could be another error waiting to happen.
-------------
Question=000132  Question for Tensorflow different execution path in training / testing phase
-------------
Question=000133  Backprop: Squared Error or Mean Squared Error or Cross Entropy?
-------------
Question=000134  Tensorflow Cifar-10 evaluation error: Enqueue operation was cancelled - How to fix?
-------------
Question=000135  Machine Learning for MRI/CT Scans
I have a set of CT scans that I'd like to use for disease classification and severity classification (Likely a regression problem). Is it possible to use networks like CNNs given that the data is 3 dimensional (a set of horizontal slices which put together form a 3d array)? And if not what is the best algorithm/way to go about this?
comment_no.=000--> There's no inherent reason why CNNs can't work on 3D data - see this link for some examples.
 https://www.researchgate.net/post/Is_there_an_example_deep_learning_ie_convolution_neural_networks_code_for_3D_image_segmentation 

A big problem with medical data though is that you may not have enough per class (or output value in your regression case) to train the CNN effectively. CNNs are notorious for requiring a very large amount of training data. They learn to extract the most important image features per target class, but because images have a large amount of variability in image data, this means requiring several thousand (at the very least) images per class to avoid the network from overfitting to unimportant image details. You could perhaps get away with fewer images if you can do some pre-processing (e.g. if your CT scans are of the brain, perhaps map them onto a predefined brain template) to reduce the variability.

Without knowing more about your problem, I think your best bet is to go down the traditional route of figuring out a set of features that you can compute from the images, and then using them to train a normal (i.e. non-image-based) classifier. But feel free to ping me if you'd like to bounce some ideas around.
-------------
Question=000136  How do you guys download massive datasets?
How are you guys getting massive datasets? Anything over 300Gigs which is the datacap by popular ISP's like comcast? 


comment_no.=000--> If you work at a university or company, ask your IT department. Otherwise I don't really know. From what I hear about Comcast, the chances will probably be slim, but maybe you can ask them to make an exception because you're using the data for (non-profit) research or something. 
comment_no.=001--> I'm not sure what datasets you're looking at, but you could always ask the uploader to split it up into separate files, and then put them back together yourself.
-------------
Question=000137  Trying to match transactions with receipts - where to start
I'm trying to match receipts (ocr scanned) with their transaction entry from my credit card provider.

I have training data for receipts I already matched .

But where to start? And can I use an existing service API?


comment_no.=000--> Are you trying to automatically reconcile your statements with your receipts?
-------------
Question=000138  (X-Post from r/Java_Programming) Java-ml Clustering Distance from Centriod
-------------
Question=000139  Machine learning, Deep and wide data, how to start?
This is a very general overview question that I have not seen addressed elsewhere, so any pointers to literature is appreciated.

I have been using SciKit-Learn with some success on a variety of ML Problems. Generally a main challenge is just to get the data parsed and formatted for SK to take it in, ie categorical variables etc.

Now  have a much larger and richer dataset of Healthcare related data to process for insights and there are multiple data tables per patient which are very wide and related. eg a subject (patient) will exists in many tables. Each table is quite wide with many possible columns.. and once I recode the variables for categories it will really blow out.

But that is not even the worst part, we also have encounter data with ICD codes and dates, so many records for a patient, This table is also very wide and may have dozens or more records per patient. Date order may be important..eg sequential occurrence.

Any suggestions for how to think about parsing/formatting the data for exploration? 

is there a library for processing both deep and wide data?  Or how should I be thinking about this?

I'm currently working with a very small subset 500 persons, but the result will be applied to very large populations, 1,000's if not millions.
comment_no.=000--> a bit more research is pointing me to HMM and RNN approaches similar to those used for natural language processing, At least for the "utilization, sequential" portion of the problem..
-------------
Question=000140  Let's have some fun through webcam? register here and i'll be your all night! AmazonGirl44 RHQBEU
-------------
Question=000141  Necessary Math Background?
Considering taking a course in ML and trying to evaluate what specific math skills would be requisite.
comment_no.=000--> Calculus, linear algebra, probability, and statistics
comment_no.=001--> If this is in a university, try writing the professor and asking for a syllabus or at least what text they use.  Machine Learning is a growing field and any one class will have different requirements than another.  A lot will depend on whether it's more theoretical or applied.

Try looking at the various common algorithms (k-means, support vector machines, naive bayes, decision trees, neural networks, etc.) and read up on the math involved.  For example, naive bayes relies heavily on conditional probability and Bayes' Rule while neural networks rely on using linear algebra to manipulate matrices and tensors (thus Google's "Tensor Flow").

What you're looking for also depends on what you're trying to learn how to do.  Do you want learn about various algorithms and how to apply them using common tools (like scikit learn)?  Or do you want to know enough that you could be developing and implementing your own algorithms?
comment_no.=002--> I took an upper level undergrad course in ML and did well enough even though my grasp of math isn't great (I was pretty much re-learning the linear algebra as I went). The most intense linear alegebra I ended up doing is deriving the psudo-inverse of a matrix, which is super easy. Also you need calculus, but only derivatives (in fact, as I recall, mostly only partial derivatives) and they're always super simple. In this case I think the most complex thing you'll need to understand is the chain rule. Be able to do it (it's easy once you understand it). For probability you need to know bayes theorem (which every human being should know regardless), and understanding the concept of a probability distribution is good.

But the best thing you can do is talk to your professor, tell him your math background, and ask for his opinion.
-------------
Question=000142  Kmeans Clustering Library (X-post from /r/Python)
Does anybody know of a library in python that allows the user to change the distance (similarity) function for kmeans clustering to a user defined function?

I've been using sklearns but they do not allow the user to do that.

Thanks in advance!
comment_no.=000--> Ideally the answer to this is to not use K-Means; there are potentially much better clustering algorithms (depending on what you want to do exactly of course). Any algorithm that accepts a distance matrix as input (with metric='precomputed' in sklearn) allows arbitrary distance functions; as long as you have a small enough dataset (or enough memory) that computing the full distance matrix is feasible then this is the easiest solution.

In practice if you have enough data that this isn't a viable solution then a user defined function for distance computations is going to likely result in abysmal runtimes: the reason that many algorithms accept a limited set of distance functions is that those are the functions that have been appropriately optimized (usually via Cython, or C libraries); the overhead of going out to a python user-defined function for every distance call is going to be huge in the long run.

Could you outline a little more about your problem and what you're trying to do (and why you chose K-Means)? I might be able to recommend some better alternative algorithms.
-------------
Question=000143  How to single-node ML lab? For text log classification.
So I've spent time this week on regex filters and field extractions for Logstash to read my log files and insert the logs and extracted fields into Elasticsearch. My application is very log-noisy and I was weeding out the "normal" errors to better identify actual issues, so I've iteratively been identifying patterns of the most common remaining log entries to end up with the more rare ones.

I showed the progress to coworkers, and one asked if machine learning could do the classification for me and free me up to better interpret the meaning. Hmmm.... So a few dozen Internet pages later...

I'm wanting to install Mahout or Spark/Mlib to kick the tires, feed it some logs and see if I can figure out what to ask next. But much of the help material on installing on a cluster. I just want to set something up on a single machine and feed it up to a gigabyte of log files and see what it I can do with it.

So am I on the right track? Can Mahout or Spark/MLib run on a single machine, or should I be looking at something else?
comment_no.=000--> Crickets over the weekend so far. But I got Apache Spark installed and doing a couple of simple things on a single machine, and the actual steps aren't difficult at all:

- Have Java with JAVA_HOME set appropriately
- [Download Spark](http://spark.apache.org/downloads.html) precompiled with Hadoop client
- Untar the Spark tarball into a directory
- Run things in the bin folder

On Windows there were some errors even though there are cmd/bat versions of the commands in bin. I think I need extra libraries. But on a bare Ubuntu 14.04 container plus Java 8 and Spark it's running with no extra steps so far.

[This page ](http://spark.apache.org/docs/latest/) has some example commands.

[This section showing language classification of tweets](https://databricks.gitbooks.io/databricks-spark-reference-applications/content/twitter_classifier/index.html) (YouTube presentation included) is where I'm going to start my tinkering. It demonstrates tokenizing and classifying tweets into clusters that end up being more or less language collections, but I think this can do what I've been trying to manually do: classify log entry types into clusters, and then I can focus on the small clusters as rare log entry types.

I think my steps are going to be:

- Use my existing Logstash field extractions against a couple of non-problem days' logs
- Store that in some intermediate data store...the tweet exercise uses SQL; with my relatively small data set I'll see if I can use text dumps or just pull it back out of Elasticsearch. If those fail, MongoDb?
- Featurize the log text. I may omit the timestamp and thread pool info; or try both with and without. I'll probably start with the [HashingTF](http://spark.apache.org/docs/latest/mllib-feature-extraction.html#tf-idf) method as the example uses, but the [Word2Vec](http://spark.apache.org/docs/latest/mllib-feature-extraction.html#word2vec) method looks worth trying out for this purpose to my untrained eye.
- Do [K-Means clustering](http://spark.apache.org/docs/latest/mllib-clustering.html#k-means) against the featurized log data
- ...
- Profit
- Well, actually then I'll see what size each cluster is and look for small-cluster or unmatched cluster (if that's a thing) against the rest of the log data.
-------------
Question=000144  choose steps to apply various image transform tools to images
-------------
Question=000145  Videos: introduction to Azure Machine Learning to make predictions (learn the basics in under an hour)
-------------
Question=000146  Use or not to use word dictionary?
-------------
Question=000147  Compressing image data to two channels for grasp detection
I'm utilizing transfer learning on an imagenet trained network to train a CNN for grasp location (via regression). I'm using image and depth data from a kinect. To use an imagenet trained model, the network needs to be 3 channel, and I've done this initially by arbitrarily throwing away the blue channel to go from RGBD to RGD. I have a working network from this, but I think it was a hacky solution. I've been thinking how for grasping locations, colour is irrelevant and in fact could lead to biases as my dataset is relatively small. In light of this, I was thinking I could convert RGB to HSV and throw away the H channel. This throws away colour information but preserves texture, which I think it more important for learning grasp features. Is this thinking sound?
comment_no.=000--> If you have to choose 3 channels from color+depth, then it indeed sounds like hue is probably the least important for you. However, I'm a little concerned about how well transfer learning is going to work if you change the features. So I could imagine RGD/RDB/DGB might work better than DSV, because those only "screw up" one channel. If you're training the ImageNet network yourself, then I suppose you could use HSV from the start, and avoid this a bit. 

It might also be possible to just augment the original network with a fourth channel. You can just add the connections for that if you wanted. If you think these could also benefit from transfer learning, you could initialize the weights based on those of the other channels. For instance, you could use their average. And then maybe multiply everything by 3/4 to keep the size of the inputs to the next layer roughly equal. 

Basically I don't know what will happen, so you may just have to try different options. That's usually the answer in these cases, but I hope it won't be too much effort / training time here.
-------------
Question=000148  How would you use this kind of data?
-------------
Question=000149  Single Feature Learning useful?
Hi,
I'm currently working on a machine learning project in university.   
My supervisor wants me to split out features into subcategories and single features to compare their performance.   
While I understand subcategories, i can't see the point in making all experiment series with 100s of single attributes.    
Is this common? Do you think it's scientifically necessary?
comment_no.=000--> I think it's pretty common to make scatterplots for independent and dependent variables to visualize how they appear to correlate. I'm not sure about full-fledged single feature learning. 

What I've seen more often are ablation studies where you start with all of your features and then see how performance deteriorates when you remove each feature. 
-------------
Question=000150  Best tutorial / nn for regression analysis?
New to ML/NNs but not to programming or engineering.

I'm currently using an older version of [```fitnet```](http://www.mathworks.com/help/nnet/ref/fitnet.html) in MATLAB. I followed the [Neural Network Fitting Tool](http://www.mathworks.com/help/nnet/gs/fit-data-with-a-neural-network.html) to make a short script to predict my data, however I'm not happy with the results and would like to try using a deeper NN using Python.

However, there are so many options out there and a lot of them seem to be based on classifications of some sort, I just need a nudge in the right direction or just the right google search to get started.

My output data is a function of multiple input variables:

     y = f(x1,x2,x3,x4,...)

I'd prefer practical guides over the full background in math. I don't need to completely understand the math, just enough to be dangerous. (And perform better than Matlab's NN).


### [```fitnet```](http://www.mathworks.com/help/nnet/ref/fitnet.html) docs:

*Fitting networks are feedforward neural networks (feedforwardnet) used to fit an input-output relationship.*"

fitnet(hiddenSizes,trainFcn) takes these arguments,
	
| ```hiddenSizes``` | Row vector of one or more hidden layer sizes (default = 10) |
|-------------------|------------------------------------------------------------:|
| ```trainFcn```    |                     Training function (default = 'trainlm') |

and returns a fitting neural network.


Example:

    [x,t] = simplefit_dataset;
    net = fitnet(10);
    net = train(net,x,t);
    view(net)
    y = net(x);
    perf = perform(net,y,t)
comment_no.=000--> I don't know what you're doing exactly or why you're using neural network specifically, but have you not heard of scikit-learn, specifically their [regression page](http://scikit-learn.org/dev/supervised_learning.html#supervised-learning)? 

comment_no.=001--> I teach a bunch of courses on regression and classification using various linear and deep nets, and I find that the actual math you need to understand what's going on is usually too much math for those who want to focus on the practical bits. (At least from what I remember, you should have learned about maximum/log-likelihood, gradients, and the chain rule in undergrad).

That said, why don't you just try a more modern library like Theano or TensorFlow? You wouldn't need to calculate gradients yourself, and it contains APIs for more recently developed techniques. So using them proficiently is just a matter of reading and understanding the documentation.
-------------
Question=000151  Inference stage in Batch Normalised Network
I came across this paper http://arxiv.org/abs/1502.03167 and it state that normalising (per batch ) the layer would resulted in faster learning step. 

Batch normalising make sense in training however it's not in inference step. As the matter of fact, section 3.1 suggest that it's undesirable. 

However, I am confused as on how do we set the mean and variance during inference stage. The author suggest to simply use the mean of variance / mean during training. 

How many variance / mean should we collect during the training stage ? Obviously using all of them won't make sense in this case. 
comment_no.=000--> Your options are:

 * Fix the weights, then calculate the mean/variance by running through all the training data again. Use these values for inference.

 * Track a moving window of mean/variance during training, then use these values during inference. 

The second option is the one most people go with. 
-------------
Question=000152  Classification with numerical labels?
I have a dataset in which each row has information about who, where, when and how much a customer has bought/spent in certain products. I was wondering if I could make a predictive model in which given the "who", "where" and "when" I could predict how much money is this customer spending.
comment_no.=000--> What is your question?

Yes, you might be able to build a model for that. How much data do you have? What is important to discover? Do you want to use a certain algorithm or just solve the problem?
-------------
Question=000153  A confusion regarding kernels
I fairly understand kernels in machine learning, how algorithms are kernalised and i also understand how kernels in image processing work, as they do in smoothing and in filters. But I can't help but wonder if they are related. 



For some time I began to relate the Guassian kernel to be some function to transform the image vector into a new feature space and all, but I'm unable to bring out any connection. Could someone help me. 
comment_no.=000--> I have been asking myself what is a kernel for some time now. I thought I understood it when it came to image processing, but then I started doing ML and I too couldn't see the connection.

Looking up the definition on Google, maybe that will help us:
kernel - "the central or most important part of something".
-------------
Question=000154  Machine Learning vs. Data Mining, and recommendations for someone with no background in this?
Hi Machine Learning. I'm a doctoral student in an IT-related field and have an idea for my dissertation. It will involve utilizing big data and automated analysis to make recommendations.

My weak areas involve big data altogether. I don't have any background with machine learning, statistics, or data mining. I am skilled and knowledgeable in my area of study, and am willing and excited to learn about machine learning or data mining. I've been researching which subset of big data I should be using for my dissertation, but have ended up more confused than before. For example, [this reddit post](https://www.reddit.com/r/MachineLearning/comments/24sc5n/data_mining_vs_machine_learning/) attempts to explain it, but everyone's interpretation is different.

Basically, I need to begin studying one of the big data focuses to get my dissertation started, but I'm not sure which one I need. Essentially, I'm looking to incorporate a big data/machine learning/data mining approach that:

* Takes information about a large number of data I have collected
* Identifies patterns with the data
* Makes configuration recommendations to me based on the rules I set
* Optional: Automates the implementation of the "best" recommended config

Is this data mining, machine learning, or something else? Any other recommendations for someone with no background in mathematics, statistics, or big data (but can program, perform the other technical pieces, and learn/study about any topics I need to)?
comment_no.=000--> The distinction between data mining and machine learning is pretty fuzzy and there is a ton of overlap so I wouldn't worry too much about it.

Your question is pretty light on details (what kind of data do you have? what sorts of patterns are you looking for? what kind of rules will you create? are you planning on using the discovered patterns to make rules?) so I'm not sure how helpful the remainder of my answer will be. If you provide more detail about the specific problem you are trying to solve there's a chance someone here could provide some more helpful advice.

Since you aren't knowledgeable about DM/ML trying to find the correct way of formulating the problem so that it can be handled by DM/ML will be impossible. There are many kinds of problems that have been studied quite a bit that aren't discussed in introductory materials so relying on the standard textbooks isn't necessarily a good idea. You may very well start learning about ML/DM and spend weeks of study only to find out it isn't applicable or to realize that you've been studying the wrong portions of the field. 
Some of what you are doing (identifying patterns in data) sounds like it would benefit from ML/DM techniques, while other things (making recommendations based on user defined rules) does not (if you were trying to get a computer to learn potential rules from data you should look into association rule mining).

**Instead you should probably try to find previous work in the field solving similar problems and look at the sorts of techniques they employ and use that as a jumping off point for further background reading.** If you are trying to automate the configuration of databases then search something like "automated database configuration" in google scholar.

If you want to try to learn about DM/ML your best bet would be to [checkout some of the introductory links on /r/machinelearning](https://www.reddit.com/r/MachineLearning/wiki/index) and read the first couple chapters of one of the textbooks listed there or watch the introductory lessons from a MOOC so that you have a basic idea of what sorts of DM/ML tools are out there. But chances are good that your problem won't fit quite right into the basic paradigms discussed in introductory materials so it is important that you look at previous work on problems similar to the one you are trying to solve.
-------------
Question=000155  How to recognize fields in web pages ?
Hi,
I have got into machine learning recently and I would like to ask advice on a couple of questions:
1. How do you recognize 'fields' in unstructured web page data? For example, I have 2,000 web pages about the same topic and I want to to recognize the top 5 fields contained on each page. Lets say that my 2k pages dataset is about cars (taken from the popular car reviewing websites). Then the output for the 5 fields would be:
>
>* Car manufacturer : Ford
>* Color: blue
>* Car type: pickup
>* Engine: 3.0L
>* Cylinders: 6
>

But , for example the field 'number of passengers' would not enter the 'top 5' fields list because, lets say, only 100 pages are talking about it, so , statistically , it is not included.
What is the sequence of steps/algorithms to achieve this and what open source package would you recommend me to use ?
I have found tools for topic creation and classification, but they seem to be focusing on some specific fields, like Name entities, or Places, but what I want is to statistically detect the 'top 5' fields.

2 Second question, if I may, of course:
How do you take advantage of already existent knowledge implicitly contained in HTML tags and logical webpage structure?
For example, the car model, can be already embedded int the 'title' tag of the HTML page describing the car. Or maybe you don't need to extract anything because there is already an html TABLE with a lot of fields. How do you extract this knowledge from HTML? But, you could not relay on it completely, because every web page will have different HTML template, so , I suppose, you must first scan the website fully, identify its template and then , extract the data from templates. I am correct? Do you know any tools that already parse HTML to prepare it as an input for machine learning algorithms?

Thank you very much in advance

comment_no.=000--> Found this:
https://www.youtube.com/watch?v=VINCQghQRuM
Maybe I could feed the HTML directly to the RNN , having such a complex internal logic it may understand the patterns and extract the data i need without any HTML preprocessing ?
-------------
Question=000156  Small Project on Evolutionary Algorithms / Machine Learning
I have a small university project concerning evolutionary algorithms in which I will work on the [NCAA dataset](https://www.kaggle.com/c/march-machine-learning-mania-2016/data) that's part of the current kaggle competition.

The idea is:
1. Train some (simple?) learning algorithms on the dataset / subsets of the data
2. Create an ensemble predictor by weighing the different trained models, so we get a prediction based on all the base models.
3. Repeat a lot: Use evolutionary methods to find good weights for step 2. 

The focus of the project is to try different approaches in step 3 for selection, mutation, reproduction and evaluate which one works best. Of course this might be easier on another dataset, but taking part in kaggle competitions is super fun :) 

What I am still unsure about is **which base learners I should use** and this is my question that I hope you can help me with. I know that a random forest approach works well in many settings, so simply training a big amount of trees might work.

However, since the dataset contains numerical and ordinal data, as well as binary nominal data, I think it should be beneficial to use different learning methods that are able to handle those respective types of data well and combine them.

What do you think? Which models should I try to combine?

Thanks for your help!

EDIT: The task is predicting the probability of which basketball team wins a given matchup, so I'm looking for regression models.

EDIT2: After looking into this some more, I realised that what I want to do is called bagging. I am probably going to first try a big bag of regression trees (which I guess would resemble random forest). Secondly, I will try to combine different base models, so I will try to find good weights for a combination of a regression tree (or multiple ones?), an SVM, maybe MARS, maybe ANN. If you have any more suggestions on this, I'd be very happy to receive more suggestions :)
comment_no.=000--> If I understand correctly you have to predict some kind of probability, right? That means you need to do regression (as opposed to classification). Regression trees should work fairly well. They should be able to handle both qualitative and quantitative data. Maybe [MARS](https://en.wikipedia.org/wiki/Multivariate_adaptive_regression_splines)... 

If you want to use something like a neural network, it deals fairly naturally with numerical data (although you may want to normalize). You can use one-hot encoding for nominal data (if you have one variable that can take on N classes, create N corresponding input nodes and turn all off except for the one that corresponds to the variable's current value). For (bounded) ordinal data you can use a progressive encoding, so if there are N possible values, create N-1 nodes, and if the current value is the third, turn on the first two nodes. (So if you have Temperature which can be Cold, Warm, or Hot, then make 2 nodes and turn them all off for Cold, turn the first on for Warm and turn both on for Hot.) These encodings might also work for other algorithms. 
-------------
Question=000157  Quick question about my Facebook dataset
Classification trees/random forests/... take classification data and result in a binary dependent variable. For example: 

monday:cloudy, warm and windy. I go and play tennis
tuesday:raining, warm and windy. I don't play tennis 
wednesday: cloudy, cold and not windy: I go and play tennis.

From this I predict whether I will play tennis on a given day.

The data I was provided with is also data in classes. It is a Facebook data set (with all data from different Facebook profiles). The dependent variable is whether a person will like a Page or not. Some of the variables in the dataset have **multiple** classification values per user per variable. 

For example: user 3 speaks English, French and Dutch. So this gives me 3 values for the language variable (3 rows.)


How do I tackle this? This is the case for a lot of variables in my dataset
comment_no.=000--> Instead of a single "languages spoken" variable which can have multiple values, you turn this into multiple variables ("English", "French", "Dutch" and more if other people in your data speak other languages) with binary values. 
-------------
Question=000158  Reframing K-means using neural networks.
By all indications, K-means is a numerical method for unsupervised clustering. 
  
I'm looking at doing K-means using neural networks because I believe there would be a speedup in doing so.  
  
Unfortunately, I'm having trouble dealing with output representation. For this, self organizing maps were recommended.   
  
Can someone run-me through self organizing maps?  
(Explain it like Im 10)
  

comment_no.=000--> First of all, you should know that k-means and self organizing maps (SOMs) are different things and SOMs are not just a faster way to calculate a k-means clustering or something like that. The results for small SOMs will be similar to k-means though. 

In a SOM you have a number of nodes/neurons that are organized into a n-dimensional point lattice (almost always a 2D grid). If you have m-dimensional data (i.e. each data point has m features / values), then each node in the SOM has m weights or parameters. In that sense, these nodes could be compared to the cluster means in k-means. When the network is trained, each new data point is simply associated with the most similar node (just like in k-means). 

To train the network, you take a data point D and calculate the distance to each node N. Usually this just means the Euclidian distance. Remember those m values that the nodes and data points have? Just subtract the D's values from N's values, square each result, sum them together, and take the square root. We will call the closest / most similar node C. 

Now we want to move C, and the nodes that are close to C (on the grid), even closer to N. We want to pull C and nodes that are very close to it very hard in the direction of N, and pull nodes that are further away a bit softer. To do this, we define a similarity function called the "neighborhood function". A similarity function is basically the opposite (inverse) of a distance function. If two nodes are the same, it should be 1, and if they could not be more different, it should be 0. One way to do this is to calculate the Euclidean distance (on the grid, not of their feature vectors), plug it into a [Gaussian function](https://en.wikipedia.org/wiki/Gaussian_function) and take the absolute value (if it's negative, multiply by -1). You can start out with a really wide Gaussian (high value of `c` on that Wikipedia page) and make it narrower when you've been training the SOM for a long time. 

Now we're going to pull every\* node N towards the current data point D. The amount by which we change N's parameters is determined by the difference with D's features, multiplied by the neighborhood function, multiplied by the current learn rate. The learn rate is a number between 0 and 1 that determines how fast we should pull each node towards the current data point. You want this value to be high when you start training and then decrease over time. 

You do this for all of the data points in your data set, and then keep repeating that until you are satisfied with the result. What you end up with is not just a mapping from data points to a cluster as in k-means, but also a mapping to a lower-dimensional space (the m-dimensional point lattice / grid). This means you could also use it for dimensionality reduction (like e.g. principle component analysis). 
-------------
Question=000159  Trying to predict my manger's arrival times.
I've collected data that records when my boss walks through the office e.g. 9am, 9:15am, 9:12am, 9:05am, 9:30am and I'd like to predict what the next values may be to make sure I'm at my desk as often as possible during his next walkthroughs. 

Is this just a case of simple linear regression to detect if there is a trend and just extrapolate forward and make sure I'm at my desk during the average of those times or at least within 1 standard deviation?

Is there something more advanced in the machine learning area that can deduce something interesting about this data to help me?
comment_no.=000--> What are all factors you recorded?
comment_no.=001--> Check the auto correlation and see if there is a potential to use an ARMA model (using the minutes after 9 for each day as a time series).  To get more fancy try random forest regression and add some categorical variables for holidays, long weekends, sports events etc the night before.
comment_no.=002--> Trying to not get too far into creepville you can use this as an added component. http://googlegeodevelopers.blogspot.com/2015/11/predicting-future-with-google-maps-apis.html?m=1

I second adding calendar info (holidays, day of week, etc)

You can grab public transportation data if you think they might be using buses instead. You'll then get discrete chunks when the buses general arrive coupled with their walk into your department. 

However, I think you already narrowed their morning down to a 30min chunk, isn't that good enough? lol
-------------
Question=000160  Using ML to determine whether a webpage is an article or not?
Does anyone know if this sort of work has been done or not and if there are any good labeled data sets to work with?

Any thoughts on where I can look to start approaching this problem would be great. 
comment_no.=000--> if there's nothing existing, what scraping a news site's archive, like Bloomberg, to get a shit ton of example articles? Then maybe a site like reddit for non-article examples. I bet this takes less than a few hours depending on how much web scraping you've done, since theres already existing reddit scraping apis
-------------
Question=000161  SVM kernels are like heuristics?
In the Machine Learning coursera course, Andrew says that different Kernels are represented by different similarity functions. He then gives an example of the Gaussian Kernel which is just a similarity function.
So basically different kernels are different similarity functions/heuristics?
comment_no.=000--> Yes, in a sense. They are dot products that make computations easier and they do look for similarities for all intents and purposes.

Kernel functions are actually(or should be) based on the domain knowledge about how the data should appear. Is it linear? polynomial? radial? That's basically the best option for choosing kernel functions, although there are some automated functions out there now.

Sharing 3 links I enjoy on kernel functions:

[Quora](https://www.quora.com/What-are-Kernels-in-Machine-Learning-and-SVM)

[Great Visual Representation](http://www.eric-kim.net/eric-kim-net/posts/1/kernel_trick.html)

[Basically all kernel functions](http://crsouza.com/2010/03/kernel-functions-for-machine-learning-applications/)
-------------
Question=000162  How to learn user behavior?
Hi,

I'm a absolute beginner in this field and I don't have a plan to solve my problem (it isn't a real problem, I'm only interested in this).

Okay I have a Android app with 3 buttons "a", "b" and "c".

Every weekday between 9am and 10am the user clicks on button a.
But every weekday between 5pm and 8pm the user clicks on button b.
At weekend the user clicks on button c.
But on Sundays between 10am and 11am the user clicks on a.

What I want is that my Android app learns this click behavior and reorders the buttons depending on the time.

I played a litte bit with Apache Mahout example[1] but I'm not sure if it is the best solution the recommend the right button.


[1] ... https://mahout.apache.org/users/recommender/userbased-5-minutes.html
comment_no.=000--> It doesn't sound like you need machine learning for this, why not just keep track of what times which buttons are most frequently pressed, and use that?
-------------
Question=000163  Question Answering System where to start?
Assuming I have a raw dataset with thousands of questions and answers, what would be the best way to tackle a system that is smart enough suggest possible answers for similar questions?

Are there any libraries or systems that can already do this and that I can build on top ?

Cheers!
comment_no.=000--> You might take a look at the papers we covered in an AI/ML course last term at PSU (Melanie Mitchell's class).  We started by covering Recurrent Neural Networks and LSTMs.  The papers starting Nov 12 specifically covered QA systems.

http://web.cecs.pdx.edu/~mm/ils/fall2015/fall2015.html
-------------
Question=000164  Using sub-datasets while experimenting with a learning algorithm
-------------
Question=000165  Help finding gradient of neural network for temporal difference learning.
-------------
Question=000166  Can someone explain to me or provide me with a practical application of the use of a Perceptron Learning Algorithm?

comment_no.=000--> The PLA is useful in that it reflects the basic structure of most learning algorithms ... predict, feedback, update ...
-------------
Question=000167  Multi Dimension inputs for NN
How can i structure inputs to an NN so that trends within each day are not lost.


 My inputs are a 2D array for each weekday. Y is a single real for each day that is ultimately the bottom right value from that days X. I want to get an updated Y as each vector of X ticks by.

Just banging in the Xs one after another, my NN is not learning the trend/function within each day. I've been experimenting with rearranging features, lambda and the number of hidden units without success. Seems I have a bias problem. 

I suspect I need an RNN or LSTM to factor in the memory but I still dont know how to flag each day as a self contained training example? 
comment_no.=000--> You could do a bi-directional RNN, then every hidden state of the RNN represents a feature in the both the forward and backward contexts, then use these as features for upper level stuff.  Think you'd want to do this over your week dimension.  This might be overkill though.  
-------------
Question=000168  Should I get into ML?
I'm not entirely sure how I should proceed and looking for some advice. 

I'm an artist who is looking to create a neural network and train it to my taste (Taste, from what I understand, is the way our neurons are wired which are wired based on experience) and have it search for art for me.  I'd basically train it to art pieces I like and ones I don't like. I realize this is not an accurate method for pinning down my taste, it'd be better if it could scan my memories, or even be a neural network modeled on my neural circuit, but these days it seems that's not possible.

Art discovery is something I try and do every single day, looking for fine art photography, music, movies, interior design, furniture, architecture, fashion, cooking, etc. 

I do this because, in a quote "A child never writes his own alphabet" - Jacque Fresco

And "You can't exceed your environment. If you give a cannibal a watch, he doesn't look at it and say 'the gears are not precise, they are ten thousandth of an inch off' He doesn't say that. It's impossible. That's what I mean by you can't exceed your environment. You can't exceed what you've been exposed to." - Jacque Fresco

So I'm basically trying to expand my environment by exposing myself to as many different systems as possible. I've been doing this for 3 years and my work has reached levels I never thought it could as I consistently experienced the dunning-kruger effect.

Now I'm wondering if it's possible I could create a subjective neural network, feed it my favorite films, music, books, etc. with a rating, and my least favorite with a rating. I'd be basically training it everyday as I discover more art. 

After that I want to program it to search for art for me, with the intent of finding *new information* on the internet (this is critical, because it is the whole point), that aligns with my taste. Art that maybe combines all areas of my taste. 

If I'm being honest, the subject of machine learning does not give me the same ecstasy I experience when making a film. Which is why I've come here to ask for advice and guidance. 

I'm not sure if it would be better to hire a developer (after I've saved up enough money) to write these for me - or would it be worth the time investment to get into ML on my own and build neural networks from scratch on Python? For the kind of program I want to build, how long do you think it would take?

I looked for programs that build neural networks and only found one so far called Simbrain, but it doesn't seem (although I'm probably wrong), it can do what I want. It seems it can only analyze data/numbers. 

Would love some help :/
comment_no.=000--> Hell yeah it's possible, that's what the 'recommended for you' sections of Amazon and Netflix are doing! You should totally teach yourself and build something on your own. Who knows, maybe the ecstasy will come later. Especially after you've made something that works! Look into recommender systems, that's what they're called. Also there's an online course called CS231n on convolutional neural networks that's used in image recognition. Sounds like a fun project.
-------------
Question=000169  R or Python, which is best for an ML beginner?
I would like to know which language I should use for starting Machine Learning. From what I can tell most people in the field use R, but I've also heard that Python is being used more and more. I have some experience with Python, should I stick with what I know or should I learn R? 
comment_no.=000--> If you already know Python, continue using Python. For all intents and purposes, they're analogous. 

Packages: [here](https://github.com/rasbt/pattern_classification/blob/master/resources/python_data_libraries.md)
comment_no.=001--> > From what I can tell most people in the field use R

Not true at all. I fact it seems like R has become a minority language for ML at this point. Many popular ML & Deep learning packages are either written in Python, or have Python bindings (TensorFlow, Theano, Keras).
comment_no.=002--> Python, because it extends better to programming systems beyond where R covers. 

It's a more generally applicable tool, and you'll need to do "regular programming" for many tasks (data cleaning at a minimum). 

The Big Data infrastructures typically have Java and Python interfaces. 

If you small data and you can read everything into memory, and you have programmers you can order to write lots of stuff for you quickly, then R could be enough and you benefit from the wide package library. 

There are fewer jobs like that. 
comment_no.=003--> Focus on the statistical techniques. Use what's easier.
-------------
Question=000170  Tensorflow layer stacking question
-------------
Question=000171  Best Algorithms for learning on sparse data?
I have a dataset that I have collected with ~ 5000 binary features and less than 1% of them are 1's.  I did a quick search to see if there are algorithms that are particularly good at working with sparse data and only found stochastic gradient descent.  Are there any others that are particularly good at working with sparse data?
comment_no.=000--> Not sure of your goals but recommendation engines/recommender systems are built on this premise.
comment_no.=001--> What are you trying to do?  Supervised classification?

If so, then linear models with L1 constraints/penalties (called 'lasso') have been developed with this problem in mind.  Comes up frequently in gene expression studies.
-------------
Question=000172  How do MS-TDNNs work?
-------------
Question=000173  Non-restricted Boltzman machines
How are non-restricted Boltzman machines trained? What are they used for? Are they used at all?
comment_no.=000--> The maximum likelihood gradient of any Boltzmann machine learning breaks down into two terms, the "positive phase" and "negative phase". Part of the reason that RBMs are comparatively tractable is that an unbiased estimate of the positive phase statistics can be had in closed form. The negative phase statistics need to be estimated via sampling (it is an expectation under the distribution p(v)), and because you can't sample p(v) directly, you need to run a Markov chain, the most popular method being to do block Gibbs sampling of p(h|v) and p(v|h) alternating.

[Deep Boltzmann machines](http://www.cs.toronto.edu/~fritz/absps/dbm.pdf) are another form of restricted topology where things break down into layers. They can be trained, by the method outlined in that paper (involving RBM pretraining) and also jointly all at once in a few different ways. Neither the positive phase statistics nor the negative phase statistics are tractable but people have successfully used mean field approximations for the positive phase and Monte Carlo (you can do block Gibbs sampling by sampling the odd layers given the even layers and vice versa).

Training general, fully-connected Boltzmann machines is hard because there's no block structure you can exploit for efficient sampling. [This tech report](http://www.cs.toronto.edu/~rsalakhu/papers/bm.pdf) outlines a procedure that can apparently train at least some reasonably-sized general Boltzmann machines.

Boltzmann machines (including RBMs) aren't really used for much anymore; RBMs were used for pretraining layerwise deep neural networks but it's become crystal clear that they are unnecessary most of the time (there are a small number of tasks where RBM-pretrained networks still reign supreme for the time being, apparently, notably some large-vocabulary speech recognition tasks I think). The exception might be collaborative filtering, I know [this variant](http://www.cs.toronto.edu/~rsalakhu/papers/rbmcf.pdf) is supposedly one of the models in production at Netflix. DBMs were an active area of research but interest in undirected graphical models for deep learning seems to have dried up.
-------------
Question=000174  Two output vectors?
-------------
Question=000175  Training set for job title classification.
-------------
Question=000176  Choosing hyperparameters of Gaussian Process
-------------
Question=000177  I don't have the time to moderate this sub. Any volunteers?
-------------
Question=000178  Help understanding Hiddeen Markov Models
I'm a bit confused about inputs and outputs when it comes to Hidden Markov Models.

Say there is a general problem of some sequence X with some labels Y. For example, X can be a sequence of words, and Y can be their parts of speech tagging. Let's assume that the alphabet (state space) for X is size D and the alphabet of Y is size L. 

Say I want to create a Hidden Markov Model (HMM) to infer Y from X (i.e., guess parts of speech from words) by training a transition matrix A and an observation matrix O. Is the observation matrix the probability of seeing an X given a Y or the other way around? Is the transition matrix a the normalized transition between the X states or is it between the Y states? Do we normalize it for all previous states, or for all current states? In other words, is the transition matrix a LxL matrix or a DxD matrix?
comment_no.=000--> You are talking about learning an HMM with fully observed data. Can I refer you to a few pages of Kevin Murphy's book? It's a simple read through.

And the transition matrix will be LxL, the other matrix would be an emission matrix which would be either DxL or LxD.
-------------
Question=000179  PredictionIO vs other? Just starting out.
-------------
Question=000180  ML for regression using predefined set of parameters
-------------
Question=000181  Detect/count persons in picture of a vehicle, what method?
Hello!
I am looking for a method to detect/maybe count people in a grayscale-photo of a vehicle. 

example picture, but with backseat included. We could change the viewangle as we wish: http://puu.sh/mUaH3/a5d0e86bcc.jpg

There are a lot of available methods but we are unsure of which have the potential to work decently well and are quite “easy” to implement since we are quite new to the area. We are mainly working in c++ or python.
We have been looking on some open source libraries like OpenCV etc.

We got a tip that Machine learning,  random forest, neural networks might work in some extent. 
There are also a bunch of object detection algorithms like cascade classifiers  “traincascade”(haar, HOG or LBP).
And at last there are a few face detection algorithms (fisher,eigen or LBH).

We don’t have the time to test all of the methods and would like to know what you would choose or not choose for the task. 

comment_no.=000--> Nice try copper. 

Deep learning is best for image recognition. Look at software  (they have turorials) such as torch, theano, and cafe.
-------------
Question=000182  Automatically changing regularization level during training?
So I've been playing with neural nets in lasagne, which outputs the train error/validation error ratio during training. It seems like a very useful way to tell if your nn is suffering from high bias or suffering from overfitting.

Anyway,  I noticed I would often see that the error ratio was too low, then regularize the net with a higher dropout p or weight decay term and start training over. 

This seems like it could be very easy/beneficial to automate. Why not just average the train/val error ratio of the past 10 epochs and change the dropout probability/weight decay term size accordingly? 
(eg if the val error is too much higher than the training error, increase dropout p and l2 regularization size. if the val error is too much lower then training error, decrease dropout p.)

It seems like doing this would help by optimizing the regularization level so the net will never suffer too high bias or too high variance. 
The main thing I'm worried about is it will somehow lead to the net sort of over fitting the validation set.
I also don't have nearly enough programming skills to actually implement and test this myself, so can you folks offer any insight?


-Is this a good idea, or will it lead to poor results on the test set?

-If it will end up generalizing badly, how/why? 

-Has something like this already been done?
comment_no.=000--> I don't it's a bad idea, it might work out.

This is somewhat similar to a learning rate schedule, e.g. halve the learning rate when validation loss stops improving. 
-------------
Question=000183  [Backpropagation]Is there a general update rule for both hidden and output layers?
-------------
Question=000184  Sparkit-learn random forests?
Is it possible to use sparkit-learn to build random forests on a spark cluster? Does anybody have an example of this?
comment_no.=000--> Sounds like this group tweaked mllib (unfortunately no syntax).
https://spark-summit.org/2014/wp-content/uploads/2014/07/Sequoia-Forest-Random-Forest-of-Humongous-Trees-Sung-Chung.pdf
-------------
Question=000185  Algorithm to implement?
-------------
Question=000186  Q-learning and neural networks - the weight update step
-------------
Question=000187  sklearn tree pruning question
-------------
Question=000188  Can someone explain thresholding an image for me?
Im doing an image recognizion task and I think thresholding would work. I need to make a program that detects bright spots on an image and circle them(active neurons in a calcium probe image). Im pretty new to these things. Thanks.
comment_no.=000--> At its most basic, if the luminance value of a pixel is above the defined threshold, then set it to white, otherwise set it to black.  

E.g.

    threshold = 200;
    pixels = [100, 150, 201, 150, 100];
    threshed_pixels = [0, 0, 255, 0, 0];
comment_no.=001--> Typically in neuro, people want to keep activated pixels/voxels above a value or zero out some values. 

You might also use thresholding to make a mask from an image. If you had a brain or other region that is clearly defined, you can turn it into all 1's with 0 everywhere else. Then any other image that is in register (aligned anatomically) can be masked by multiplication.

If this is MRI or PET, you can use fsl tool's fslmath to do the thresholding. fslmaths -h has all the options.

-------------
Question=000189  Where does Numenta's HTM fit into the current machine learning landscape?
-------------
Question=000190  What is the difference between a (dynamic) Bayes network and a HMM?

comment_no.=000--> HMM has the Markov property of the first order.
-------------
Question=000191  Format question using scikit
attempting to put [this](https://archive.ics.uci.edu/ml/machine-learning-databases/magic/magic04.data) data into scikit & work with it.  I am having a bit of an issue seeing how the data correlates to the features specified.  Can anyone help me ?

[here](https://archive.ics.uci.edu/ml/datasets/MAGIC+Gamma+Telescope) is a link about the data 
comment_no.=000--> I'm not sure if I understand the question. Do you have problems to see what column corresponds to which feature?

In this case the [pandas](http://pandas.pydata.org/index.html) library might be helpful:

    import pandas as pd
    features = ["fLength", "fWidth", "fSize", "fConc", "fConc1", "fAsym", "fM3Long", "fM3Trans", "fAlpha", "fDist", "class"]
    df = pd.read_csv("magic04.data", header=None, names=features)
    df.head()

This will output the first few rows/datapoints of your data set with the header names as defined in the features list.

I also recommend using [jupyter notebook](http://jupyter.org/) for just trying around with datasets. The pandas library outputs datapoints in a neat little table, which makes it a bit easier to see what you are dealing with.

If I misunderstood the question, please clarify :)
-------------
Question=000192  Extra constant inputs to multilayer perceptron cause diverging
-------------
Question=000193  X-post from /r/machinelearning: Question about tensorflow/cifar10
Hi guys,
I'm currently learning tensorflow. I'm working on the Cifar-10 tutorial. I'm a bit confused and wanted to check whether I'm understanding correctly how I would prepare data myself to train the model. Here's what I think I should do, I'd appreciate any feedback:

First, convert the images to a numpy array of shape height x length x channels (3 for RGB image). I'm unsure what I'd do with the labels, actually...


Let's say I call the final file foobar Then, I could use read_cifar10(foobar) to read in the image in my code. In the code, the individual images are then arranged in batches the way tensorflow wants all to receive them.


Is this correct? If I want to read in more than 1 image, what will I write instead of foobar, let's say the files are called foobar1 and 2?
comment_no.=000--> For each batch, you'll want to run tf.train.YourOptimizerOfChoice(your-learning-rate).minimize(your-cost)

The cost function Google uses in their examples is cross entropy, in the form of tf.reduce_sum(your_correct_labels * tf.log(your_algorithm's_output))
-------------
Question=000194  Machine learning uses the same few equations over and over
The more papers and ML books that I read, the more I see the same equations repeated over and over, only with slightly different names. 

I'm wondering if the machine learning literature is overly complicated by what is essentially duplication with obsfucation ~~to keep out those who are afraid to wade through the math.~~  

Edit: Maybe I was little steamed when I wrote that. :)
comment_no.=000--> Cite a few examples?
comment_no.=001--> I would also like to see some examples.

I'm imagining that you're describing something like the fact that the linear regression equations appear over and over again - they do! The basic concept of a linear model is the intuitive jumping off-point for just about every supervised learning model. The fact that this is a pattern we can exploit is wonderful and for some people (myself included) very counterintuitive. We use these over and over again because they are well-studied models. 

I find it difficult to believe any claims that it's simply a matter of spiteful obfuscation on the part of authors, though. There are many alternative hypotheses that fit the data here, such as:

1) The same equations appear over and over again, but the variations between each usage are hard enough to systematize that we have yet to find an overarching system which totally describes all of them. 
 
2) Most papers are written by experts in the field, who have seen these equations and patterns before. 
When they write their papers, they purposefully reuse ideas and equations, both because it gives them a theoretical base to build on and a formalism that their audience will already be familiar with. Note that in this case, it's the opposite of your claim - the various authors actually go out of their way to present the material in a familiar way, knowing that it would aid understanding to do so.

Moreover, why on earth would someone spend months or years of their life researching something, only to publish a paper or a book which is intentionally obscure?
-------------
Question=000195  Using an RBM to learn the distribution of an other RBM
-------------
Question=000196  If GLM performs better than GBM or RF, what does that mean ?
Hi,

I was asked this question in interview and I admit I'm not sure about my answer. What would be yours ?
> If GLM performs better than GBM or RF, what does that mean ?

I think the answer expected was something about the data, and maybe a "solution".

Cheers

Note :  
- GLM : Logistic regression  
- GBM : Gradient boosted trees  
- RF : Random forest (of trees)
comment_no.=000--> Google is your friend my man... https://www.quora.com/What-are-the-advantages-of-logistic-regression-over-decision-trees

But the short version is it means there is a clear linear separation in the logistic regression via slope, whereas the data is not conducive to a clear split for decision trees which (for ease of explanation) are parallel to each axis.
-------------
Question=000197  [UFLDL][TensorFlow] Can I consider my Sparse Autoencoder implementation a success?
Trying to follow the UFLDL tutorial with TensorFlow as my tool. I'm on lesson 1. 

[Here is my nicest looking output.](https://i.imgur.com/zW9Ept0.jpg) I am concerned that I found splotches rather than edges. Should I be? Or is the sample image of the tutorial an unrealistic expectation for how these filters go?

Unfortunately, I had to do two things differently than Andrew Ng's sample code to achieve these results.
**I used 50k sample patches instead of 10k.** Trained with gradient descent instead of L-BFGS, so that could explain why I need more. By contrast, [here is a 10k sample.](https://imgur.com/RfTeTGz) It has several filters that look like random noise.

**My learned filters are incredibly high contrast!** The above samples were first put through my normalizeData function (which is exactly the same function used in SampleImages in the exercise) before being sent to the visualizer. [Here is an unnormalized image!](https://imgur.com/WrK8qnt) Gross!

My first instinct tells me this is an issue with my regularization of the cost function.  The exercise has a lambda of .0001. Increasing this one-thousand fold to .1 [produces something a tad better](https://i.imgur.com/N2R7SJs.png). Perhaps this means I am using the L2_loss function of tensorflow incorrectly? Here is that line of code:

    cost_reg = n_lambda * (tf.nn.l2_loss(weights['hidden']) + tf.nn.l2_loss(weights['out']))


 Finally, for Tensorflow, AdamOptimizer did absolutely nothing for me, leaving me with completely random filters in the end. Is there a reason replacing GradientDescentOptimizer(learningrate=learningrate) with AdamOptimizer() shouldn't work for me?

[Here is the code on github](https://github.com/gosp/TestingCenter/blob/b9b2cab17dcec8c90375b0bd29bc239719585111/testTensorFlowSparceAutoencoder.py)

**EDIT** One issue I've found so far: I was not activating my output with sigmoid. Now I don't need to normalize my data! But I'm also no longer learning anything that looks like structure... back to square one...


comment_no.=000--> I've got a working solution based on your code.  Hope it helps.
https://github.com/trackbully/UFLDL_TensorFlow/blob/master/SAE.py
-------------
Question=000198  Tensorflow seq2seq model unsatisfying results
-------------
Question=000199  Which is your favorite tool for machine learning and predictive modelling when working with R/Python?
Some examples would be:
R - caret, h2o, etc.
Python - scikit-learn, pybrain, etc.

comment_no.=000--> Numpy
-------------
Question=000200  Interpreting the results of LSTM-based Recurrent Networks
Does any of you know, if any research has been done interpreting the results of LSTM-based Recurrent Networks and linking them back to the features that were the input to the model?
comment_no.=000--> Visualizing and Understanding Recurrent Networks, http://arxiv.org/abs/1506.02078
-------------
Question=000201  Best starting place for beginners?
I'm 15 and starting to learn about machine learning, I am currently read Artificial intelligence a Modern Approach but after that what should I do?
comment_no.=000--> I'm by no means an expert, but learning Python, or another similar language, so that you can actually implement the stuff they talk about in the books is probably a safe bet.
comment_no.=001--> You're 15 and already understand stochastic calculus, linear algebra, and optimization theory? Impressive.
-------------
Question=000202  Aren't features more important than any particular algorithm or ML method?
Hi all,

I just finished my first neural network (Ng's Coursera assignment, so as basic as possible, but still quite cool to me), and I was curious about how these can be scaled when it comes to object recognition, etc. This assignment is digit recognition on a 20x20 image, a mere 400-element input array. But real-world images are exponentially bigger than this.

So my question is, if individual pixels aren't used as inputs, how do features get extracted from images for use in neural networks? 

It seems to me that learning how to develop quality features is a better investment of my time while trying to learn ML than diving deep into the particular algorithms. Is there any validity to that? 

For example, I learned of Josh Tenenbaum's name from the ML: A Probabilistic Perspective, and on his front page there's an intro of how children learn to distinguish a horse from just a handful of examples (which seems so common sense to me, but in the perspective of machine learning is quite remarkable). But is the problem that there's no algorithm that can adequately generalize after such few training points, or is it that the features being fed to these algorithms don't encapsulate enough information to allow such few training points?

I really appreciate your time and any insight you can give me.

Cheers
comment_no.=000--> For image work and neural networks, they often use convolutional neural networks (CNNs) to find features that are combinations of individual pixels.   As the CNNs are trained to find "useful" features, you get things like "vertical edge" or "diagonal edge".  These then become the features used by subsequent neural network layers.

I'm not an expert - I've just read about this in one of my classes.
-------------
Question=000203  random forests (to 7000 oaks)
-------------
Question=000204  Localizing an object with neural networks (Conv-Nets)?
-------------
Question=000205  What are some good resources for someone looking to delve deeper into Neural Networks after having taken an introductory undergraduate course in ML.
I am familiar with feed forward networks and how they work and am even taking a graduate course on Neural Nets this coming spring semester but would like to do some reading in the mean time.

The thing is, a vast majority of the posts/papers I read over on /r/MachineLearning about the topic are way over my head.


Does anyone have any suggestions? Thanks!
comment_no.=000--> If you're explicitly interested in Neural Networks then I'd recommend Geoffrey Hinton's (One of the pioneers in Deep learning) course on Coursera which goes through a lot of the important topics relating to NNs.
You'll find the course with the name "Neural networks" offered by University of Toronto -Sorry can't get the link on Mobile-
comment_no.=001--> https://see.stanford.edu/Course/CS229
-------------
Question=000206  Looking for free offline resource to learn Machine Learning and prerequisite knowledge
Can anyone point me to any? I'm new to ML, I have a basic understanding of Stats or Probability, and I wanted to be able to download it for offline reading. I'd prefer something in the EPub or Mobi format, but PDF works as well. 
comment_no.=000--> Scikit learn and ipython notebooks
comment_no.=001--> I already bookmarked [this](https://redd.it/1jeawf) reddit post to read at my leisure.  Great place to start (I'm new to ML as well), but I would also recommend Coursera.org ML courses to any beginner. 
comment_no.=002--> Elements of Statistical Learning is a good book.
-------------
Question=000207  Coding own CNL in TF
-------------
Question=000208  RNNs as generative models
I was going through [Ilya Sutskever's thesis](http://www.cs.utoronto.ca/~ilya/pubs/ilya_sutskever_phd_thesis.pdf) and in section 2.5.2 he states

"An RNN defines a generative model over sequences **if** the loss function satisfies L(z^^t ; y^^t ) = -log(p(y^^t ; z^^t )) for some parameterized family of distributions p(· ; z) and y^^t = v^^(t+1) " 
(Emphasis mine)

Does this mean that the RNN will not act as a generative model if we use, let's say, the squared loss function?

comment_no.=000--> -log(p(y^t; z^t)) is the "negative log likelihood".

it does not specify a specific loss function (maybe you missread and thought that it is cross-entropy?)

It is also a generative model if the loss function satisfies L(z^t; y^t) = - p(y^t; z^t) for ... (adding the log doesn't change anything, as log is monotonous and increasing function).

maybe related to your question: we write the mean squared error without the log because it doesn't help. Cross entropy has exponential terms so there are computational issues because of limited precisions of floats, right? But squared loss is a sum so there isn't such a problem.

Please, correct me if i'm wrong.




-------------
Question=000209  How do you keep track of the progress of a machine learning project ?
I am talking about what features and algorithms you use, their performance etc.

comment_no.=000--> Best way is often to use R Markdown language (Or whatever software you use). You have the syntax and output but you can also write a paragraph opener and closer in each section that will describe what you did and why and the final results.
-------------
Question=000210  Do I need a PhD?
Is a PhD required to do work in machine learning?
comment_no.=000--> No.

Is that a long enough answer? There are data scientists with bachelors, masters, and PhDs. Hilary Maso, one of the most noted Data Scientists only has a Bachelors. It's all contingent on your ability to learn and apply information.
comment_no.=001--> Absolutely not,
If you don't have an upper level degree indicating you learned the foundation and have a project to show for it (thesis, dissertation), etc. Your next best option is to create projects with definitive results to show your competence and understanding. 

I am part of a machine learning meetup type group. About every month or two there are industry hirees looking for people to bring into their businesses to solve a machine learning problem. They essentially ask who in the group had done a similar problem then dig into that project. If they feel the individual or individuals can tackle it they get hired for the project or the job. 
A good chunk of the financial guys are either still in undergrad, have graduates in a completely different field, or have nothing at all in paper degrees, but have a huge amount of high results in online ml financial challenges. 

Unless your looking for grants or something that essentially has a clause indicating the pi needs to have a PhD your good to go on your merits of previous results. 

I've seen this same thing happen in python groups. Someone will ask what you're working on or present an open job/project available and as long as people know what you're working you'll be steered towards each other.

I personally feel this is true in all of computer science. Simply because a degree isn't always indicative of being a problem solver of novel questions. (Sure a degree helps, but this industry knows that it's not the only way, unlike say practicing medicine on a human.) Hirees for hard questions simply want results not a person in a cubicle. 
-------------
Question=000211  VC dimension of decision trees.
-------------
Question=000212  What is the most efficient way to do visual salience detection?
-------------
Question=000213  Deep learning neural network to perform SED fitting and having some problems with bad outputs.
-------------
Question=000214  Trying to predict user behavior. Achieved some results, stuck on choosing a better model / input.
Total newbie here. Using SKLearn.

So I have an web app with a RESTful server. This means I have data about the user approximately in this form, I'm using Reddit urls just to give you an idea:

    user 123 requested /r/machinelearning at 15:43:32
    user 123 requested /r/machinelearning/top at 15:44:15
    user 123 requested /r/MLQuestions at 15:45:56
    user 123 requested /r/MLQuestions/submit?selftext=true at 15:47:01
    ...
    etc.

Basically, I have the user's request log of the web app. What I'm trying to achieve is classify whether the user adopts the app or not. There's a 30 day trial period where you can use the app for free and at the end of this period you'll have to start paying for the app in order to continue using it.

On my first try I disregarded the URLs completely and just fed in the number of requests per a day. So I might feed a vector like this to the ML algorithm:

	[24, 10, 0, 15]

So 24 actions taken on the first day, 10 actions on the second day, etc. With 14 days, I achieved very close to 90% accuracy. This was with a balanced set, so the ROC AUC score was also very close to 0.9. I just tried a bunch of classifiers from sklearn and chose the best one. Random Forest and a simple linear Logistic Regression performed the best.

I'm now trying to do the same thing again, but only looking at the first hour of usage. I'm also trying not to disregard the different categories fo actions. However, this has made the input matrix really huge and very sparse, etc for user 123 you'll have something like this:

    [
        [1, 2, 2, 1, 0, 0, ..., 0],    # actions performed within the 1st minute
        [0, 0, 0, 0, 0, 2, ..., 0],    # actions performed within the 2nd minute
         ...
        [0, 0, 0, 0, 0, 0, ..., 0]     # actions performes within the 60th minute
    ]

I thought I might feed each of the minutes to an unsupervised clustering algorithm for dimensionality rediction, to end up with a vector like this:

    [1, 2, 0, 0, 0, 5, ..., 0]

Where the numbers, hopefully, represent types of user behavior the clustering algorithm found. I just tried this using K-Means and MeanShift. MeanShift did find lots of clusters, probably around 1500 different clusters. It's a huge number, so maybe I'm doing something wrong. I don't thing new users can do 1500 different general things within the first hour of usage...

I could also try to reduce the number of different requests by just taking the first category, etc /r/me_irl and /r/me_irl/top would both be just /r/me_irl just so that I could reduce the dimensions of 1 minute.

Okay so my current plan is to feed these clusters to a classifier. The thing is, the whole thing is a time series, so would another algorithm be more suitable for this? I've heard LSTM is good for time series. Am I heading in the right direction or am I doing something stupid?
comment_no.=000--> Just saw this paper "Session-based Recommendations with Recurrent Neural Networks": http://arxiv.org/pdf/1511.06939v2.pdf.
I haven't read it yet. Maybe it helps to solve your problem.
-------------
Question=000215  Hardware?
I'm new to the ML game.  Starting on Kaggle competitions. Thinking about getting a new computer.  What hardware will make the biggest difference?  Not looking to break the bank, just some guidelines i.e. minimum sys requirements, amp up the speed,  bang for my buck kinda stuff, etc.  
comment_no.=000--> The biggest speedup will come from a video card that supports CUDA (i.e. nVidia). I recently swapped my R9 280X for a 970 to utilize Theano's GPU support, and the speedup is considerable. 
comment_no.=001--> All major open source ML libraries support CUDA hardware acceleration. So something that supports an NVIDIA card.   
Correct implementation took some of my compute times down from 2 hours to 5 minutes.  
-------------
Question=000216  Are decision trees basically just a data structure to build an acyclic Bayes net?
-------------
Question=000217  Reshaping in TensorFlow MNIST tutorial?
Greetings, I'm trying to follow [TensorFlow's expert_mnist tutorial](https://www.tensorflow.org/versions/master/tutorials/mnist/pros/index.html), and I cannot understand the role of reshaping. Can someone briefly explain why and how is it done? (I'm also a little confused by the negative value in the first place, what does that mean?). The code in question is...

    x_image = tf.reshape(x, [-1,28,28,1])
comment_no.=000--> A matrix is a vector of vector of values. Reshaping turns it into a single large vector of values. Then turns it into a Matrix of vectors again of whatever dimensions you want it in.    
  
simplest application for this is vec-ing a matrix to calculate a Jacobian or a Hessian  
     
-------------
Question=000218  Choosing a Learning Algorithm for Real-valued Instance Clustering/Classification
-------------
Question=000219  Will be possible to develop a Learning maching to try to predict tennis matches with RapidMiner?
Hi, I've got tons of data with a lot of info about tennis matches of the last 5 years.
I would like to create a Machine Learning and train it with the data from 2011-2014 and then test it with 2015 data.
Is this possible? The info in the datasets is a little confusing. Is this possible with RapidMiner?
Thanks.
comment_no.=000--> Yes and Yes. Split the data into training and test sets (splitting by years may affect your tests predictability). Run neural nets, random forests, etc. Profit.

You have to make sure your data is clean and structured well.
comment_no.=001--> You might look at "Analyzing Baseball Data with R" by Max Marchi.  Different sport, but the principles are the same.
comment_no.=002--> At the most naive level, you might just look at each player and determine a mean and standard deviation for the number of points they score.  Then to predict the outcome of a game, for each player, randomly sample from a normal distribution using the mean and sd you found for each player.  The one with the most points wins.  Maybe you do that an odd number of times, and choose the winner from who won the most simulated games.  This is essentially a monte-carlo simulation.

You could get more sophisticated by using more of your variables in your data to predict a number of points.  You could even try to get more clever if your data can help you model when they'll score in the game and their probabilities of scoring points given things like... "late in the game, and behind", etc.  The possibilities are endless.  

You might also try to find ways to classify players based on your data... good on clay vs grass, aggressive, consistent, good defender, etc.  These would then be factors to help refine your prediction of their performance.

But I'd go with starting simple and adding complexity as you go.  I'd probably look at some simple regressions to see how well your variables predict points or outcomes.
-------------
Question=000220  Advice request: Using ML for neuroscience research, do I need to abandon MatLab...
Hi all!

I'm a behavioral neuroscience researcher in the field of electrophysiology, meaning most of my work involves recording and anything signals from one or more neurons in order to determine their function. 

One of the common problems in my field is decoding the activity of neurons that are suspected to be encoding information about something, without any firm idea of HOW they're converting that information. The information can be encoded in so many different ways... By the timing of a spikes relative to an external triggering event, by the timecourse of the rate of spiking, by a precise temporal pattern of spikes, by the timing of spikes relative to the phase of a particular neural oscillation... 

It is common in my field to use machine learning as a way to show that a particular type of neuron encodes information in a particular way, by using the neural activity as inputs to a classifier (and the different types of events / stimuli preceding or following that activity as the target categories). The underlying implicit assumption is that if a ML classifier can identify the stimulus using that information alone, then there's a decent chance that the brain areas that receive that information are doing so as well.

I started off learning MatLab for data analysis reasons. I had no real programming experience (just writing scripts for data analysis software, or to control stimulus delivery systems), had no idea what convolution was, and had no experience working with matrices and vectors and whatnot. 

I've become reasonably competent in MatLab (I think), and found the various ML tools in MatLab to be helpful for data analysis. But I have run into a number of difficulties. My issues are:

Difficulty with circular data (phase angles) using feedforward nets and/or SVMs.. I have been just providing inputs as the sine and cosine of the angles, but I wish I could just use complex inputs.

Difficulty training classifiers when the differentiability of the classes is not known (e.g. there are ten classes, but it is quite possible that the inputs only contain enough information to discriminate the samples into 1/2/3/4/5, 6/7/8, and 9/10); classifiers often get hung up trying to minimize errors instead of maximizing information. I have tried to get around this by using evolutionary algorithms to train ANNs, using mutual information (or normalized variation of information) between network outputs and targets as the fitness function. 

Difficulty figuring out how to get MatLab's training algorithms to use MY division of data into training and validation sets. 

Uncertainty about how to determine the best type of classifier for my data.. This is more of a 'me problem' than a MatLab problem. 

Uncertainty about how to expand my analyses to a broader range of input types: I have so far usually just narrowed my data down to some number of phase angles and then used those as inputs, but I might wish to do analyses in which the input is a continuous signal alongside a discrete event signal, with the goal being to identify periods in the signal during which certain events are occurring.

Things I DON'T need to do:

Image recognition

Gigantic super-complicated models

So, that's my situation. I could use advice on whether it's worth it for me to abandon MatLab in favor of something more flexible. I could also use general advice about how to solve any of the issues I've described. Any help is appreciated! And if you want to know more, just ask.
comment_no.=000--> What do your fellow researchers use? Do you need to share code?
-------------
Question=000221  Ranking (with SVMs?)
-------------
Question=000222  What is the difference between convolution and correlation. Why do CNN's use convolution?

comment_no.=000--> Correlation indicates a relationship between two variables.

Convolution is a product operation that combines two distributions (or series) to create a new distribution.

In the context of a time series, the convolution operation combines the past information of two time series to create a new series.  

In the context of a CNN, at each time step the network contains the history of all past inputs and activations.  In this way it is a generalized version of the convolution operation.

*edit: I just realized that I confused CNN and RNN.  :( must have been on crazy pills.
-------------
Question=000223  Guide to implementing RNN
Hi! I want to build a simple RNN with matlab to learn reading text. I have tried to understand how to implement it in code, but I would love to see if there are good guides out there that gives exercises in implementing the code! Any tips would be great :)
comment_no.=000--> Sorry if I'm misinterpreting what you are saying, but, if you aren't implementing it to learn about NNs but rather to do a specific task, I'd really suggest not implementing your own NNs.  

Not matlab, but the deep learning for NLP class will help if you want to implement stuff.
comment_no.=001--> This is a very good post on RNN's helped me understand them alot better and has some good code samples, though not in matlab.
http://karpathy.github.io/2015/05/21/rnn-effectiveness/
-------------
Question=000224  Want to focus on the financial application of machine learning, looking for good study material
I've already completed the coursera Machine Learning course offered by Stanford so I believe I have a good understanding of the basics of machine learning.

I want to now focus on the financial applications of machine learning, but I want to make sure I pick the right material to study. Does anybody have any recommendations? 

I am primarily interested in the prediction of stock prices.

Thank you.
comment_no.=000--> It's worth doing some finance related course to get more of an understanding of things from that angle. This coursera one is quite good https://www.coursera.org/learn/financial-engineering-1/home/welcome
-------------
Question=000225  I want to make a program that will help me hunt for apartments. Is ML the right tool and where should I start?
I have this project to write some kind of app (I'm more proficient in Ruby and JS) that would eventually be able to sift through apartments for rent ads and send me the relevant ones.

The way I see it is that I'd feed the program ads and then tell it whether this particular listing is of interest to me or not and hope the machine will learn to eventually be able to do the sorting on its own.

Is this something that can be achieved with ML? Are there any frameworks that are better suited to this? I am a developer but I don't know much about statistics or maths and I've never worked on anything related to ML. Any pointers?

Thanks!
comment_no.=000--> it just sounds like a job for regular expressions. Anything more is overkill. 
comment_no.=001--> Machine learning can help you figure out how good of a deal a particular listing is, and if you want to get even more advanced then go ahead and try to train an algorithm that could even figure out if a given posting is too good to be true and is probably a scam. 
comment_no.=002--> I am planning to do the same thing for job hunts.. Let's do it together?

I have ML and NLP knowledge, wanna work together?
-------------
Question=000226  Deep Learning SIMPLIFIED: Episode 5 - An Old Problem
While Deep Neural Nets are the state of the art in machine learning, the flipside is that they are really hard to train. Up until 2006, there was no way to train them satisfactorily. Here is a clip that explains further. 
https://www.youtube.com/watch?v=SKMpmAOUa2Q
comment_no.=000--> This one is on the vanishing gradient. Enjoy :-)
-------------
Question=000227  Trouble of using VGG for Super-Resolution
I'm trying to implement the net in [Accurate Image Super-Resolution Using Very Deep Convolutional Networks](http://arxiv.org/abs/1511.04587). It's inspired by VGG: the only difference is that there are no pooling layers, so all intermediate weight layers have 64 channels.

I followed the details of the paper

1. Gradient clipping
2. Learning rate = 0.1, decreases by 10 every `K` epochs
3. Glorot-like weight initialization: `stddev=(2/(3*3*64))^0.5`, where filter width = 3, number of input channels = 64
4. Momentum = 0.9, L2 regularization = 1e-4

In Tensorboard, I'm keeping track of the weights/biases histograms, and it seems after a certain time, the weights stop changing. This may mean the gradient vanished, or we've settled on a bad local minimum. However, I am getting performance worse off than bicubic interpolation. In the paper, they get better performance even in the 1st epoch. I was wondering why is this happening?
comment_no.=000--> Initialize the weight matrix of the last convolution, the one that results in the residuals, with zeros. It also helps to use YCbCr instead of RGB.
-------------
Question=000228  Deep Learning SIMPLIFIED: Episode 4 - How to Choose
Deep Learning as a field has developed quite a bit in the last decade, and we now have a variety of models to pick from with new models and improvements arriving frequently. The flip side to this is, the burden of choice now falls on you to figure out which model to pick for what application. Here is a clip that gives you some guidelines to help you decide.
https://www.youtube.com/watch?v=JjZDoojyzXQ
comment_no.=000--> Some simple rules of thumb on how to pick a deep net. Enjoy :-)
-------------
Question=000229  Deep Learning SIMPLIFIED: Episode 3 - Why Deep?
-------------
Question=000230  Deep Learning SIMPLIFIED: Episode 2 - What is a neural network?
-------------
Question=000231  Calculating the recall and precision values of multilayer neural network with TensorFlow
-------------
Question=000232  Looking for EM derivations exercises with solutions
Hi,

I am looking for a good source of exercises of derivations of the EM algorithm (possibly also other inference methods, such as Variational Bayes) with solutions. Ideally this would contain an assortment of different graphical models on which to do the derivations of the loglikelihood, the E and M steps. Any pointers?
comment_no.=000--> * [EM](http://www.david-andrzejewski.com/publications/em.pdf)
* [Variational Inference in 5 minutes](http://davmre.github.io/inference/2015/11/13/elbo-in-5min/)
* [General purpose variantional inference](http://davmre.github.io/inference/2015/11/13/general_purpose_variational_inference/)
* [Auto-Encoding Variational Bayes](http://arxiv.org/abs/1312.6114v10)
-------------
Question=000233  Deep Learning SIMPLIFIED: YouTube Series
Hi everyone! I am new to this sub-reddit and wanted to introduce myself. I have been working on a YouTube series for Deep Learning that you may like. If you are ever need to explain Deep Learning to a newbie (or are new to Deep learning yourselves), you may like this series. Content you'll typically find online on the topic is highly mathematical/technical, which is great! But if you're like me, you probably want to just understand the models and the intuition. Thats what this series is about! Here is the link to the series intro. Please take a look and let me know what you think!
https://www.youtube.com/watch?v=b99UVkWzYTQ
comment_no.=000--> This is the series intro - 6 total videos so far and many more to come. Enjoy :-)!
comment_no.=001--> Very nice! I watched the intro video. The production quality is great and I like the narrator's voice. I look forward to watching the rest. Thanks for doing this!
comment_no.=002--> I found that really useful! Thanks very much. 
-------------
Question=000234  Correct cost function to use with a softmax output layer with a continuous target distribution?
Lets say I am training a neural network to play Rock, Paper, Scissors. The network outputs a probability distribution over the three actions. I am using a softmax as the final layer of the network to ensure that P_rock + P_paper + P_scissors = 1.0.

I understand (I think..) that if my networks target output was discrete, ie (1,0,0), (0,1,0) or (0,0,1),  I should use Cross-Entropy as my cost function. My question is what cost function should I use to train the network if my required target output is continuous, for example (1./3, 1./3, 1./3)?

I have tried mean squared error, but it does not converge. I don't know if I have a bug, or I am using the wrong cost function. Is mean square error or cross-entropy appropriate in this case? Would anyone be kind enough to point me in the right direction? Thanks in advance...

comment_no.=000--> Output of your network is still discrete, it is classification problem, but you will never get full one and zeros, because softmax is giving you probabilities that it should be that output. If you want result you need to sample from that distribution. So you should use cross-entropy.
-------------
Question=000235  Weird error message when tuning svm with polynomial kernel: "WARNING: reaching max number of iterations"
It is my first time working with support vector machines. I am trying to solve this homework, but am receiving the above mentioned error... here is my code:

    library(e1071)
    test_data = #upload test data here.
    training_data= read.table('Digits_training.csv', sep =',', header = TRUE)
    y = training_data$y

    chosen_svm = function(y,training_data,kernel_name){
      obj <- tune.svm(y~., data = training_data, gamma = 10^(-3:1), cost = 10^(-3:1), kernel = kernel_name)
      gamma = obj$best.parameters$gamma
      cost = obj$best.parameters$cost
      model = svm(y~., data = training_data, gamma = gamma, cost = cost, kernel = kernel_name)
      return(model)
    }

    radial_svm = chosen_svm(y,training_data,'radial')
    lin_svm = chosen_svm(y,training_data,'linear')
    pol_svm = chosen_svm(y,training_data,'polynomial')

Any idea why this is happening?
comment_no.=000--> I am a bit busy to check but I would guess that SVM requires convergence and it was failing to converge.
-------------
Question=000236  What effect does loss function have on training?
I understand it is only used to observe progress, ie. it does not really have any fingers in how weights are updated?


If so, why is the choice of loss function important? The network will converge to what it will converge anyway...


A slightly related question about training criterion, ie. error for the output layer, from what I've seen sometimes its just

target - output

sometimes its

(target - output)*deriv(output)


Is it chosen independently of loss function?
comment_no.=000--> False alarm, figured it out
-------------
Question=000237  CNN for 3D data sets
-------------
Question=000238  CNN-RNN architecture - Techniques to deal with large 5-D input data?
-------------
Question=000239  Given a list of centroids, how to find optimal set of length k?
Think this is an easy one I'm having a brain fart about.
Given some dataset where each observation X has a pre-calculated distance to N centroids. If I know I want to separate the entire set into K centroids, how can I pick which K? 

Or more concretely, lets say I have 1000 rocks and each rock has a set of 1000 distances to some centroid representing a feature (maybe color, likelihood to be found in a yard, etc) And I know I want to cut the whole set of rocks into 5 centroids (either a rock is closest to the yard centroid or to the color centroid). How do I find the 5 centroids which will minimize my distance to the whole set of rocks?

I feel like there's some kind of reverse-knn I'm just not thinking of.
comment_no.=000--> [k-means clustering](https://en.wikipedia.org/wiki/K-means_clustering).
-------------
Question=000240  Need help with understanding how to compute the weight gradient in a convolutional layer.
Hi,

So i have a have a hard time understanding how to compute the gradient for the convolutional layer. To test my understanding i have constructed a simple convolutional network featuring one convolutional layer, one maxpool layer and a fully connected output layer. The whole network looks like this:

http://i.imgur.com/gryUKtU.jpg

So when applying backpropegation computing the output delta is simple, but I have some uncertainties when computing the other deltas. For the maxpool layer most instructions will say to simply "repeat" the errors from the previous layer since this layer does not do any learning.
 So for delta^3 I compute a matrix with the error values from delta^4 for the pooled indices and 0 everywhere else.  So for the delta for the convolutional layer (delta^2) I guess one only computes the hadamard product between d/dz and delta^3?. When computing the gradient for the weights I furthermore compute the convolution between the Input (I) and delta^2. Using this gradient does not yield usefull results (I.E reduce the error) when updating the weights, so I am obviously doing something wrong. How I compute the gradient is shown in this picture:

http://i.imgur.com/6lcR7V4.jpg

Basically my question is what is the correct way of obtaining the weight gradient in this example.
comment_no.=000--> Figured it out, to compute delta^2 you ofcourse compute it
thusly = upsamle(w^T delta^3) hadamard ( sigma' (z)). Where delta^3 = delta^4.
-------------
Question=000241  [Help] Supervised Classification Implementation
This is my frist ML project, so I am trying to build a really simple web analytics tool. I want to use apache web logs to determine whether or not to issue a coupon to a customer. The web logs contain ip address and browsing history, so I can determine what pages a customer went to and whether or not they bought the product. Using this data as a training set, I hope to build a model that can report to me who I should issue new coupons to, in order to improve sales. 

My question is: how can I implement this? Any details I can get will be really helpful. I already have a java program that can take the web logs and split them into individual pieces of data. How do I get this data into a form that is acceptable, and how do I make the program? Its hard for me to find tutorials that are low-level and implementation focused. 

Thanks!
comment_no.=000--> Let's get the data extraction sorted out. What you want is the pattern of a customer's browsing before he/she makes a buy/doesn't ever make a buy (no buy for next 4 weeks). So your data is only limited to ppl making a buy. Can you see how to extract this data?

Now you have a time series data (don't be scared, fancy way of saying simple stuff) of buying patterns. Split into training and test.

I guess a Hidden Markov Network would do good here. The observed data is the page being browsed, and hidden state is probability to buy. Learning this model is straightforward (tractable). Read Kevin Murphy's section on learning a partially observed hmm.

You can limit the length of the hmm to a value based on the amount of data and performance on test set. Smaller lenght may give less test performance, longer hmm might not have enough data to train on.

Feel free to ask follow-ups :)
-------------
Question=000242  The typical 25 horse problem with a twist
The typical interview question:

You have 25 horses and want to identify the 3 fastest horses. You have a track that can hold 5 horses at a time. What is the minimum number of races it would take for you to identify the 3 fastest horses.

Twist:
Same problem, except now you want to identify the 6 fastest horses.


Edit: I forgot to mention, the times are not recorded so you cannot just find the speed of each horse individually and compare.

EX: For the top 3 horses, the answer is 7 races

To display this, I will show label the horses with a letter and a number. The letter refers to their first race set and the number refers to their order for the first race.

Race 1|Race 2|Race 3|Race 4|Race 5
:--|:--|:--|:--|:--
a1|b1|c1|d1|e1
a2|b2|c2|d2|e2
a3|b3|c3|de|e3
a4|b4|c4|d4|e4
a5|b5|c5|d5|e5

We can then find the fastest horse by comparing the best in each of the first 5:

Race 6|NA
:--|:--
a1|na
b1|na
c1|na
d1|na
e1|na

Lets say that horse a1, b1, and c1 are the fastest, showing up in their respective orders (i.e. a1 is the fastest). Then to identify the 2nd and 3rd fastest, we race the set of candidates:

Race 7|NA
:--|:--
a2|na
a3|na
b1|na
b2|na
c1|na

This will give us the 3 fastest horses.
comment_no.=000--> 5 for both assuming the horses are in random order and you do not have any information on them. Since 5 horses x 5 horses per track is 25 and you want to cover each horse exactly once because you do not have any prior information on them. Then you sort the results by speed. Any additional races can bias the results because horses used multiple times can be fatigued.
-------------
Question=000243  Machine learning on paths ?
Hello everyone, 

I'm currently a student working on a projet with a [database](https://snap.stanford.edu/data/wikispeedia.html) from Wikispeedia
>(From their website)
>>
You are given two Wikipedia articles
Starting from the first article, your goal is to reach the second one, exclusively by following links in the articles you encounter.

I'd like to train an algorithm to guess the shortest path to go from one article to another, knowing only the categories (e.g Historical figure, Geography, japanese videogames...)

The idea is that computing every possible path from one point to another is simply impossible on such a large graph in a reasonable amount of time, so I want to use machine learning to have a satisfying solution in a relatively short amount of time.

I'd like to say that i'm a novice un machine learning, although I already completed a few projects, but I never worked on graphs before : do you have an idea where I could find some ideas on what kind of algorithms to use for my project ?

(If it can help, I code in python and used scikit-learn for my previous projects)

Thank for your help
comment_no.=000--> This seems similar to (not exactly) how Google maps would give you a route from one end of US to another.. (edit) The difference being that in maps you have a measure of closeness to the destination (euclidean distance), in this problem it's not that obvious.

My take at it (multiple ideas)
Extract a graph from Wikipedia, the articles are the nodes and the links are directed edges. Then, this article is a good start: https://en.wikipedia.org/wiki/Pathfinding

1) A* Search from source to destination. You'll have to figure out the heuristics to use. We can discuss further on this.

2)By breaking down the problem into subparts (sub-graphs) based on major topic in wikipedia(or other clustering metric). You can run single source/all pairs shortest path on all subparts and then join the outputs (details need to be figured out). This would be useful if you want shortest paths from all articles to all other.

The graph you construct would be sparse, that property could also be used. At the least storage should be done by an array of LinkedLists. Computational gains might also be needed to look into.
-------------
Question=000244  Help with a Deep Convolution Network
I'm building a deep convolution neural network on my own with C++ and am getting stuck. How does the convolution layers update its weights? I understand how the hidden layer and the output layer's weights are updated, but I can't think of how to update the convolution filters. 

Can anyone explain how to update the convolution layer, provide an algorithm, or resources where I can learn more about this? 

Oh, for some context, the goal of my network is to learn to play Breakout. With some hand coded filters I was able to get it to bounce the ball 2-3 times, but not that reliably.
comment_no.=000--> It's same as in the DNN. [cs231n](http://cs231n.github.io) would be a good start to check it out, or see how people have implemented it in frameworks e.g. keras.
-------------
Question=000245  Designing a Random Forest from scratch, with no ML libraries allowed.
Hi all, not a programmer but I'm taking a machine learning module I've been asked to design a RF from scratch. I am lost.

I have basic skills in python - very basic. 

Can anyone help?

So far I've scraped a decision tree from the internet but I'm a bit lost on it as its telling me my features in the function argument aren't defined. 

My attempt at the code below.

    import numpy
    import csv
    with open('banks.csv', 'rt', encoding='ascii') as csvfile:        #      type name of datafile with extension in first set of apostrophes.
    data = csv.reader(csvfile, delimiter=' ', quotechar='|')
    # for row in data:
       # print (', '.join(row)) # prints out all data in file

    def newDT(data, features, targetClass, fitness_func): # define a new decision tree

    data=data[:]
    values=[record[targetClass] for record in data]
    empty=majorValue(data, targetClass)
    features=(data[:0])

    if not data or (len(features)-1)<=0:
        return empty
    elif values.count(values[0])==len(values):
        return values(0)
    else:
        best=choose_features(data, features, targetClass, fitness_func)
        tree={best:{}}

        for value in get_values(data, best):
            subtree=newDT(get_examples(data, best, value), [attr for attr in features if attr != best], targetClass, fitness_func)
            tree=[best][value]=subtree

    return tree

    newDT(data, features, targetClass, fitness_func)
comment_no.=000--> I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit:

- [/r/machinelearning] [CrossPost from MLQuestions: Random Forest from scratch. Any help is appreciated.](https://np.reddit.com/r/MachineLearning/comments/3v2hts/crosspost_from_mlquestions_random_forest_from/)

[](#footer)*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))*

[](#bot)
comment_no.=001--> Hi! Once you implement a decision tree you are almost there, just build n of them, predict your validation set through all of them and for each input vector take the mean (regression) or the mode (classification). If you can share your banks.csv file I will take a look at it tomorrow. Plus a link to where you found the script.
-------------
Question=000246  How do I describe a Perceptron, comparing two inputs x/y triggering when x>y.
I tried to setup a perceptron witch outputs an 1 when output x is bigger than y. How can I realise this? Like wtf?
comment_no.=000--> If I recall perceptron well, this Python code should describe such perceptron:

    def gtp(x, y): # inputs are x and y
        s = 0.0 + 1.0 * x + (-1.0) * y # bias is 0, weights are 1 and -1
        if s > 0.0: # threshold is 0
            return 1.0
        else:
            return 0.0

Training it is another matter, here the weights and bias are fixed.
comment_no.=001--> Y is an input? Are there only 2 inputs? Are you going to use back propagation for training?
-------------
Question=000247  Question regarding inversion of softmax function
-------------
Question=000248  Does changing contrast, brightness,etc in data set increase the accuracy of the trained net?
I have a data set of around 15000 images, if I modify these images by changing their properties such as contrast, brightness, rotation ect to create a larger data set will the accuracy of my net be greater? 
Very new to this!
comment_no.=000--> Depending on what your images are, it can. I remember someone who won a Kaggle competition on identifying galaxies(?) did a write-up on his processes, and part of it was stretching the images in different ways to create a more generalized dataset

edit: http://benanne.github.io/2014/04/05/galaxy-zoo.html
-------------
Question=000249  Simple projects to begin with?
Hello! I'm in a research program at my school and choose to learn about ML. Currently I'm learning Algebra II/Trig, so my math education is not nearly enough to easily understand ML. 

I've been working at a decent pace and I currently understand gradient descent thanks to Coursera and Andrew Ng, however suddenly my teacher requested 10 pages of original research done in a very short amount of time (Monday, 11/27/15). Are there any basic data sets and techniques I can use to at least get credit for my research thus far? ML seems to be a very deep topic you learn about over time, and I currently don't have time. 

I was thinking about trying to use some modified gradient descent algorithms on data sets, but my issues currently are: I don't know what kind of data I need/where to get it, I don't know how to make a gradient descent algorithm work for more than 2 parameters (linear regression).

Any advice would be appreciated. I'm sorry for asking this question, I know it's very "how do I learn ML quick" style, but really I just need a simple project to pass and continue learning at a normal rate.

comment_no.=000--> Andrew Ng's intro to ML is really good and I'm not sure how you would go about learning ML much faster than that. I think I would recommend binging a bit on his lectures so that you get a bit beyond linear regression with 2 parameters. If I recall correctly, there are assignments in this course, right? Maybe you can use that in your report (maybe extend the exercises a bit). 

I don't really know anything interesting to do with 2 parameter linear regression. You can fit a line through a bunch of data points. If you learn to use more parameters, you can make things slightly more interesting by comparing the lines you get with various numbers of parameters. 

If you learn about logistic regression, you can start doing classification/detection. You could make "networks" that simulate AND and OR logic gates. For XOR you need to use a nonlinear technique since it's not linearly separable. For instance, a neural network with a hidden layer (i.e. a multilayer perceptron). 

One fairly small but interesting problem is classifying the [MNIST](http://yann.lecun.com/exdb/mnist/) dataset of handwritten digits. However, this is quite a bit beyond linear regression with 2 parameters. 

You could also take a look at some of the challenges on [HackerRack](https://www.hackerrank.com/domains/ai/machine-learning).

Good luck!

BTW, I hope you meant Monday the 30th and not Friday the 27th (today/yesterday depending on your time zone). (Monday the 27th doesn't exist.)
-------------
Question=000250  Can two Machine-Learning computers be identital?
Hi,

I have a question that's been eating me for some time and don't know where else to post it. So here it is:

Assuming that two computers (M1 & M2) are fed the exact-same training data, would they behave identically, to the point where we can accurately assess/predict the behaviour of M2 based on M1.

I guess I'm trying to find out whether it is impossible, due to inherent differences in the hardware (not the exact same chip, even if from same manufacturer) akin to a genetical difference. It's a bit like the rhetorical "If two humans grew up in the *exact* same environmnent, would they behave in the same way", though I would assume it's technically impossible to do such a test.

If anyone could shed some answer I would appreciate it, thank you :)

Peace
comment_no.=000--> I'm not sure if I understood the question correctly.

First of all this depends on the algorithm you use and if it has a random component. For example Random Forests are very unlikely to produce the same results, even if trained with the same data.

A nearest neighbor algorithm on the other hand would give you the same results if it is trained with the same training data, since it is deterministic, there simply is no random part. Edit: This means a simple k-nn algorithm, not some fancy approximation :)

In the latter case there **might** be a difference based on floating point operations and rounding errors as /u/Nixonite was asking, but I think this is rather unlikely if you run exactly the same setup/architecture and software. The amount of errors that may occur are probably not that significant to change the classification outcome. But this is just a rather unfounded guess of mine.
comment_no.=001--> You mean would their roundoff error be the same for the same algorithms?
comment_no.=002--> The hardware differences will not affect your algorithm in any way (other than how quickly it trains of course).
comment_no.=003--> I think it depends if the algorithm used is deterministic or not. If it uses randomness in some way, then there's no way to guarantee that the results will be exactly same.
-------------
Question=000251  How to create a custom objective/cost function?
-------------
Question=000252  [Optimization] Is multi-days Travelling Salesman Problem the same as mTSP?
-------------
Question=000253  TF as individual chatbot
What happens if you feed in your whole Chat-History of Facebook to the tensorflow-framework as sequence to sequence model whereas the target-output it the text I wrote and the input the text my Facebookfriends wrote.

Will the result of the training be a chatbot which is using language like me ?
comment_no.=000--> I doubt it. The biggest challenge in NLP is actually understanding the context, not just following the grammatical rules of humans. Also, I imagine the training data is not going to be large enough since I assume most of the conversations are unique and largely depends on the situation. Of course it will output in the language of yours, but it won't effectively answer the question.
-------------
Question=000254  Question about types of Neural nets
Hey all! 

So I have one main question about the different types of Neural Nets that exist. It seems like a lot of different types of NNs fall under the umbrella term of Neural Net and I was wondering what kinds there are.

I'm familiar with a CNN, an RNN and so forth. Do those employ backpropogation? The only one I've had experience with has been the one as done in the Andrew Ng coursera lectures (feedforward with backprop- not sure if this is its own type of neural net or if this is just a general term)

I'm also aware that many different types of cost functions exist? Does this actually change the type of net or is this really just a different way to characterize "cost"?
comment_no.=000--> Those are the 3 main types of nets that you will usually see. Also, there are variations of these (LSTM, GRU, etc are types of RNNs). 

Back propagation is the optimization method pretty much always used. You could really use any method you wanted (genetic algorithms, particle swarm optimization, etc) but back prop is much faster pretty much always. 

The cost function you choose is more of a hyper-parameter, like learning rate and batch size. It will depend on what your data looks like, and mostly depends on whether you're doing regression or classification on which cost function you choose. 
comment_no.=001--> There are certainly many types of neural networks. Check out the Wikipedia page on [recurrent neural networks](https://en.wikipedia.org/wiki/Recurrent_neural_network). The complement of RNNs are [feedforward NNs](https://en.wikipedia.org/wiki/Feedforward_neural_network). This is one of the more clear-cut distinctions since FFNNs are directed acyclic graphs and RNNs contain at least one cycle. However, I would say that a recurrent MLP is more similar to a feedforward multilayer perceptron than it is to a Hopfield network (which is also recurrent). Basically you can come up with all kinds of variations by adding, omitting, sharing and fixing connections or changing some activation functions. It's not always clear when a change constitutes a new "type" of network. 

What most of these networks have in common is the the node's activation is given by an activation function applied to the weighted sum of other nodes' activations, although some nodes in LSTMs compute the product. This is different in [spiking neural networks](https://en.wikipedia.org/wiki/Spiking_neural_network), but they are rarely used since they are hard to train. I would say [Boltzmann machines](https://en.wikipedia.org/wiki/Boltzmann_machine) are also quite different. 

Most of the time some form of backpropagation is used. I think backpropagation technically refers to a fairly specific algorithm, and people often use variants like RPROP or Quickprop instead, but it's still all about propagating an error back throughout the network. Things like momentum and dropout and other training/regularization tricks can be added. It's also possible to use other optimization techniques (like genetic algorithms). Hebbian learning (increase weight between nodes that are simultaneously active) is not used much anymore I think. (Restricted) Boltzmann machines have their own training procedure that uses Gibbs sampling and gradient descent. I would say that the used cost function is probably more part of the training procedure than it is of the network.

Finally, I should mention that there are also computational neuroscience projects that basically aim to actually simulate parts of the brain (like IBM's Blue Brain project). These are technically also neural networks, but they are also pretty different.

-------------
Question=000255  TF for DSP-Programmers
-------------
Question=000256  Sklearn PCA not making sense to me.
I'm under the impression that if you have a 2D data set and you perform PCA on it, without any dimensionality reduction it will essentially rotate your data to a new coordinate system. Why then when I attempt this in sklearn does the data seem like its being transformed in some way?

Here is my code:

    import numpy as np
    from sklearn.decomposition import PCA
    import matplotlib.pyplot as plt

    x = np.linspace(0,10,101)
    y = x + 2*np.random.randn(1, 101)
    y = y[0]
    data = np.asarray(zip(x,y))
    pca = PCA(n_components=2)
    new_data = pca.fit_transform(data)
    plt.figure(0)
    plt.scatter(data[:,0], data[:, 1])
    plt.figure(1)
    plt.scatter(new_data[:,0], new_data[:, 1])
    plt.show()
comment_no.=000--> I think it may just be appearing that way because of the different scales. When I look at it the corresponding points seem to be in the correct spot.

Add colors = range(101), and pass in c=colors to scatter() and you'll see it more easily.
-------------
Question=000257  Whats the difference between manifold learning and decomposition in the context of dimensionality reduction?
-------------
Question=000258  Teaching a computerprogramm to "run" another
-------------
Question=000259  Reinforcement Learning help!
Im learning reinforcement learning. Can someone provide me with some example problems which I can try out and improve my understanding. Thank You! 
comment_no.=000--> [RLPy](http://mloss.org/software/view/514/) is an open source framework for performing sequential decision making experiments in Python.  It has a bunch of [example domains](https://github.com/rlpy/rlpy/tree/master/examples). 
comment_no.=001--> In [the book by Sutton & Barto](https://webdocs.cs.ualberta.ca/~sutton/book/ebook/the-book.html) there are sometimes scenarios used in the chapters, as well as some case studies in the end, which you can use to create some own problems.

Did you mean something like this?
-------------
Question=000260  ML newbie: How to approach this problem, given a screenshot, find areas of interest or interaction. Suggestions?
**The problem (in detail):**

Given a screenshot, I want to be able to detect areas where a user would typically interact - whether that is through clicking or typing.  I would want my output to be somewhat similar to an OCR pipeline.


**Approach 1 - OCR like document analysis:**

Use some kind of modified version of [document layout analysis](https://en.wikipedia.org/wiki/Document_layout_analysis) more suited to this problem.


**The problem:**

I really have no idea how to even begin here.


**Approach 2 - Attempt to learn areas of interest:**

My idea would be to download a bunch of web pages, screenshot them, and use that as training data because areas of interaction can be identified fairly easily in a webpage (as either an input, anchor, or button tag... it doesn't get everything, but may be good enough).

**The problem:**

My hypothesis is that something like a neural net would not be able to distinguish interactive and non-interactive elements with a high enough accuracy.  Another problem, even if I think it could be accurate, how could I leverage this as a classification problem to be useful to me?  i.e. my output would just tell me whether or not that screenshot contains an interactive element?  I'm looking at this the wrong way but I can't see another approach.


I'm definitely lost, so any pointers would be greatly appreciated!

comment_no.=000--> You are like 90% of the way to a complete solution. It looks like what you really need is a classifier that, given an image, produces bounding boxes around the interactive elements. Happily algorithms to both detect (decide if an entity exists in the image) and locate (draw bounding boxes around the entity) are quite common in computer vision. Its called "object detection" or "object recognition", and its a common enough thing that you could find tutorials about online. See the PASCAL VOC challenge for more cutting edge algorithms. I am not an expert in this field but if you dig around you should be able find implementations of such algorithms, including ones that use deep learning and ones that don't. 

As you say, you could then imagine downloading webpages, identifying the interactive elements by parsing the HTML (in practice this might be the hard part), and then using that as training data for your classifier. Since you have potentially unlimited training data, and I would guess that this is a difficult but not extremely difficult classification problem, my guess is that that approach could achieve very good accuracy. Note, of course, it is liable to only work for screen shots of webpages, not screen shots in general, since that is where what your training data coming from.
-------------
Question=000261  Unsupervised Learning with Theano/CGT/TensorFlow
I have a huge amount of respect for computational graph frameworks, and would love to start working with them more, but most of what I want to do is unsupervised or generative.  I've not been able to figure out what kind of loss function to use or how to utilize them to accomplish what I want, so I've always been relegated to writing my own libraries.  A Google search yields a few pages from deeplearning.net wherein they define an RBM loss function, but that's really about it.  The rest are just links to frameworks.  I realize the universal trend is to do unsupervised pre-training on the bottom layers of the network and slap a FC network on top, but I really want to do unsupervised all the way to the top.  Are there good resources or tutorials on using any of these frameworks?  Any advice for writing loss functions there?

EDIT: Or am I relegated to only using RBMs/auto-encoders with graph frameworks?
comment_no.=000--> [deleted]
-------------
Question=000262  Regularized Linear Regression
-------------
Question=000263  Help me with my thesis: Need Facebook friends and Reddit IDs!
-------------
Question=000264  Are nonlinearities needed if you add bias at each layer?
From what I understand the stacked weight matrices are no longer reducible to one matrix if you have bias at each layer... Yes or no, what am I missing here?


Btw, does the same apply if you have normalization at each layer during training and forward pass?
comment_no.=000--> Let's say you have a MLP with two inputs (x1 and x2) and two neurons:

    Neuron 1: w11*x1 + w12*x2 + b1
    Neuron 2: w21*x1 + w22*x2 + b2
    Output: W1*neuron1 + W2*neuron2 + B = W1*(w11*x1 + w12*x2 + b1) + W2*(w21*x1 + w22*x2 + b2) + B = A*x1 + B*x2 + C 

(The formulas for A, B and C are left for the reader, but they do not depend on the inputs x1 and x2).

Clearly the output is linear with regard to the inputs, the only thing we've accomplished is a convoluted way to determine the linear coefficients A, B and C, which would make the *training* nonlinear.

So the answer is no, biases do not add any nonlinearity to the network, activation functions are definitely required. 

What do you mean by normalization at each layer? Batch normalization?
-------------
Question=000265  [Career Question?] Help with preparing for an interview?
"Desired Qualifications". 

*Experience with machine learning or pattern recognition algorithms

*Strong experience with conducting machine learning experiments. This includes writing and evaluating machine learning algorithms

*Excellent programming skills in either Python (preferred), Java or C++

*Enrolled in undergraduate or graduate studies in CS, EE or related disciplines

*Experience with data analytics

1) Any recommendations on books/ sections I should know in and out? 

2) Should I focus on those things or focus on typical "software engineering" questions? Like Big-O of search/ sort algorithms, writing data structures and all that.
comment_no.=000--> > Any recommendations on books/ sections I should know in and out?

You should probably read the [docs](http://numenta.org/#docs) that Numenta has made available about NuPIC and HTM. I would probably focus here, because apparently they use this.

All of the other desired qualifications are "Experience with XYZ", which you can't really get in a short amount of time. I mean, you could take some intro to ML courses on Coursera, EdX or Udacity, but it sounds like that will not be advanced enough for this job. I like Christopher Bishop's [Pattern Recognition and Machine Learning](http://research.microsoft.com/en-us/um/people/cmbishop/prml/) book, but it's also not something you breeze through. It's probably a lot more doable to read the [Wikipedia page](https://en.wikipedia.org/wiki/Machine_learning) and make sure that you at least know what they're talking about with all of the approaches. 

I would probably try to figure out some specific things about the company. Are they doing computer vision? Then read a little bit about that. Are they focusing on facial recognition (I'm just making stuff up here)? Then so should you. What kind of algorithms do they use aside from NuPIC?

> Should I focus on those things or focus on typical "software engineering" questions? Like Big-O of search/ sort algorithms, writing data structures and all that.

Judging from the desired qualifications I probably wouldn't *focus* on this. They seem to care much more about ML in particular than they do about more general software engineering and computer science stuff. I'd worry more about knowing various ML algorithms than the traditional CS fare. It's good to know the basics though. Learning the complexity (Big-O) of these algorithms and data structure operations should not take too much time, so it might be worthwhile. 
-------------
Question=000266  Text input BRNN example
-------------
Question=000267  Need help in audio signal processing
So i am implementing a project on determining a birds species from its song (wav file)
I figured [this method](http://hearinghealthmatters.org/waynesworld/2012/animal-vocalization-analysis-the-gplp/) can be tried out...I need help on implementing it on python...
I'd give anything to anyone who gets it xD
comment_no.=000--> You might want to check out [librosa](https://github.com/bmcfee/librosa). It's a python library that takes care of the audio processing (by way of another library [audioread](https://github.com/sampsyo/audioread)) and also implements many of the common audio features. Check out the tutorial. Librosa makes it trivial to get a [spectrogram](http://bmcfee.github.io/librosa/generated/librosa.core.stft.html) and transform it into the [cepstral domain](http://bmcfee.github.io/librosa/generated/librosa.feature.melspectrogram.html). The filter bank mentioned sounds similar to a mel filter bank which is included in librosa.  
I didn't know we had equal-loudness curves for birds, that's awesome. If you have that it shouldn't be too hard to apply it to the spectrogram, once you're familiar with the matrix representation.  

Hopeufully this helps. Good luck!
-------------
Question=000268  After 30 hours, I still can't figure out how to properly implement a Backpropagation Algorithm. Help please.
I spent nearly 30 hours thinking, reading, and trying to implement this. Everything is correct. I checked them a hundred times, so there is no problem in the logic as far as I understand it. Below are the issues I'm facing.

1- So I want to approximate a function (x^2, sinx, ...) using a neural network, but how do I make this work? I have one input neuron, variable hidden neurons, and one output. How would my network ever produce the correct outputs if the sigmoid function has a range from 0 to 1?

2- My network can't even figure out the xor problem with 2 hidden neurons, two inputs, and one output. But the forward pass works if I manually set the weights and use a threshold activation function for both hidden and output layers.

3- It always produces the same output for any input.
comment_no.=000--> Code for weight correction in the hidden layer (modified for clarity), where I suspect it's wrong:

    float oGradient=(target[i] - y[i])*y[i] * (1 - y[i]);
    float h1Gradient= (phi(hidden1_sum)) * (1 - phi(hidden1_sum)) * hidden1_out_w1;
    float h2Gradient=(phi(hidden2_sum)) * (1 - phi(hidden2_sum)) * hidden2_out_w2;

    input1_hidden1_w+= eta*h1Gradient * oGradient * input_signal1;
    input1_hidden2_w+= eta*h2Gradient * oGradient * input_signal1;
    input2_hidden1_w+= eta*h1Gradient * oGradient * input_signal2;
    input2_hidden2_w+= eta*h2Gradient * oGradient *  input_signal2;

    hidden1_bias+= eta*h1Gradient * oGradient;
    hidden2_bias+= eta*h2Gradient * oGradient;

Where phi is the sigmoid function, and y[i] is the current output of the output neuron.
comment_no.=001--> PLEASE, can someone at least  give me the correct weights and biases for a 2-2-1 network that uses sigmoid(1/(1+e^(-x)) for all neurons? It's driving me insane. I just want to know where the error is.
comment_no.=002--> 1 - you just give up sigmoid for the output layer. Keep sigmoid for hidden units only.

2 - Does it learn OR and AND functions without problems?
comment_no.=003--> On Mobile so please forgive smelling errors.  I'll try to give a more thorough answer when I get to my laptop.  In the meantime:

1:  You can either try linear output nodes instead of sigmoid, or just scale your answers by some constant that allows signal outputs to generate answers in the range you want.  Stick to approximately 0.3 to 0.8, not 0 ... 1. 

2: A single hidden layer ffnn with two hidden nodes can solve XOR, but backpropagation isn't guaranteed to find that answer.  TRY more nodes and different initialization.  Do not initialize weights to 0.


-------------
Question=000269  [Work question] What does the workday of someone working in Machine Learning look like?
What do you love the most about your job? What is your least favorite thing? What steps have you taken to get to your position? What are personal qualities that are necessary to succeed in Machine Learning? If you could give a piece of advice to a young person seeking a career in Machine Learning, what advice would you give?
comment_no.=000--> I wouldn't say I every really worked in ML, but I have had work where I made heavy use of ML. Currently I'm a PhD student in AI, which means that I'm mostly just reading a lot of articles and then occasionally implementing some algorithms to try them out, but ML isn't my focus.

I used to work for a small computer vision company where I worked on custom software for clients, which included stuff like facial expression recognition, video surveillance, body pose estimation, object recognition/detection/tracking and behavior analysis. When a project started we would have some meetings about what the client wanted, what we expected to be able to deliver, and how we might be able to tackle the challenges. We would usually have some initial ideas of what techniques could be used, after which I would dive into the scientific literature about those techniques (if it was my project). Then at several points in time I would present my ideas and progress to the rest of the team, who would then give feedback. Also at different points in time the project would get redefined in several ways because the client wanted something different, or some algorithm turned out to really not work, or we adjusted our expectations of what was possible, etc.

I probably spent most of my time programming. After deciding what techniques to use, they needed to be implemented. You can read about these things in scientific papers, but it's (almost) never quite what you need, so you have to adapt it to your specific situation and problem, and usually this would involve cobbling together multiple ML algorithms. That also means that you have multiple points of failure, and that you need to train a bunch of separate algorithms. 

A lot of time was also spent gathering or producing data, training the algorithms and then testing them. This often involved creating separate annotation and training and training tools, and then actually doing the annotations. Sometimes we could tell the client what kind of setup they should use (i.e. number and kind of cameras and computers), which would mean researching what was best/cheap and building the setup. Otherwise,  I occasionally visited some clients in order to get data from their actual setups, or I'd try to recreate locally it to get data that was somewhat similar. 

Once I had all of that, I could bring it all together in some piece of software for the client. Even if you have all of the ideas, this still takes quite a bit of time. If you have a whole pipeline of algorithms, it can be difficult to get them to work together, and there is always something that is inaccurate or too slow, which means you might have to replace it or gather more data/retrain it or change the setup (if that's an option). Finally, you need some nice way of visualizing the results for the client, and the program needs to actually be usable by other people who are not necessarily experts. This is usually pretty easy compared to the other things, but it can still take a lot of time.

Aside from that it's just regular company and miscellaneous stuff. Meetings, some administration and reporting, creating tools for internal use, trying to upgrade the dev stack, updating/upgrading the website, learning new stuff, organizing and attending events, and generally just having a nice time with coworkers...
-------------
Question=000270  In Machine Learning is it necessary to take software engineering?
There's a Statistical and Machine Learning Major which I'm going into but I would like to pair it with either a Language Technology Minor or a Software Engineering minor. Right now I'm not sure which would come in more useful in the field.
comment_no.=000--> When you say "the field", what field do you mean? Software Engineering (or at least programming) is going to be relevant for all machine learning, no matter what direction you go. In fact, it is so relevant that I expect your major will already include quite a bit of it. I don't know too much about Language Technology minors, but it sounds like it might be useful if you want to do Natural Language Processing. If you want to do NLP, or want to learn something more "unique", then you should probably that Language Technology minor (you'll have to learn to program anyhow). If you just want to do ML in general, then software engineering is the most obvious choice.
-------------
Question=000271  Can I use machine learning to recognize professional photography (vs amateur photography)?
I'm a complete beginner and have no experience with machine learning, I was wondering if it would be possible for me to train some kind of neural network to recognize professional photography reliably.

An example of photos that should rate very high as professional:
https://www.flickr.com/photos/steamster/sets/72157656521743602

what kind of a dataset would I need to achieve this? and how much machine learning do I need to learn to implement it? (I currently know a decent amount of basic programming in python and java)

comment_no.=000--> So actually, you're going to need to use a neural network because you're dealing with complex non-linear hypothesis. The theory is similar, but the the implementation is different. I can't help there yet. 
comment_no.=001--> I'd say this is a pretty big problem. Should be possible but might need some month or even years of research.
comment_no.=002--> What you have to first understand is modelability vs model choice.

I work in finance using machine learning techniques. If the problem can be modelled, there is a few techniques that will work. I recommend the book computer vision with python to get you up to speed with lots of image problems in python.
http://www.amazon.com/Programming-Computer-Vision-Python-algorithms/dp/1449316549

Where I suspect the issue may come is modelability. Of course we can contrast some amazing shot with something I took on my 10 year old mobile and find some different features.

But, in lots and lots of cases the line between 'professional' and amateur will be very blurred. I have guys on my facebook who take pictures better than many family professional portrait photographers do, students who take pictures that could be on national geographic.

I don't want to dissuade you, but recognise that the important step is not really training a neural network - it's finding a reasonable hypothesis of what features select between amateur and professional.

This is why we don't take TBs of data, feed into some black box model and out pops predictions for stock prices, horse race winners etc.

I generally believe the best approach is 'I have data that separates based on XYZ features, how can I model this?' rather than 'How can I use this model in my problem'. I suggest you go through some resources and example problems on image classification like I referenced, this will enable you to get some experience investigating problems.

 
comment_no.=003--> Yes you could. There's something called a Classification Algorithm, or Logistic regression. Fancy words that just mean "is it probably this or is it probably that"? The more training examples you provide, the more accurate your results will get usually, and for this example you would benefit from having a bunch of amateur photos as well. Since there will be a large number of features, you won't really be aware of what the algorithm knows, but you would be able to tell if one matches your training set, or doesn't. Maybe someone else can provide more details, because I don't know exactly how to work with photos yet. I just know how to work with data, but with these photos the algorithms will simply look for patterns in the pixels. 

For increased comprehension, read slowly:

basically, what you do is provide a set of training examples with several features that the algorithm can focus on, then you provide it a definite answer. So you say, this is professional. This is not. The algorithm then identifies something called a hypothesis function that tries to match as many of the features observed in the training examples as possible, based on a set of numbers called parameters or weights, that when plugged into the hypothesis, can predict if the data you're feeding it falls in the matching side or the not matching side. 

It involves something called a cost function which figures out how different your hypothesis is from the actual data. 
This cost function is set to be minimized, or to find the partial derivative of the curve with respect to the parameters (basically, when the hypothesis differs from the actual data the LEAST), and this is done through another function called gradient descent. 

Gradient descent is a stepwise calculation that takes arbitrary initial values for your weights, calculates the tangent of the cost function, and repeats until it reaches a minimum, meaning the difference between the hypothesis and the actual training examples is minimized. 

Once you have the right parameters or weights, you can then feed the hypothesis new information, and using those parameters it figured out through gradient descent, it runs a comparison saying, yes this image likely matches the professional set, or no this image doesn't match the professional set. 

Like I said though someone else may be able to help a little bit with how to exactly do this with images. But all in all, it's just programming a few very simple functions. Nothing too algorithmically complex. 

EDIT: Grammar fixes and clarifications
-------------
Question=000272  Is there something driving the nodes of an RBM to learn different features?
-------------
Question=000273  Basketball (or any sport) predictions
Are there generally accepted algorithms used for predicting the outcomes of sports games? I'm getting ready to scrape historic data for NCAA BB and want to actually use it for something. I've come across the Pythagorean Formula, but there has to be something better than a predictor for baseball.
comment_no.=000--> Check out this blog: http://netprophetblog.blogspot.com/

What a wonderful breakdown of many different prediction algorithms! Definitely one of the best blogs out there on basketball prediction.
-------------
Question=000274  ML n00b here. Starting to learn ML on my own. Classification technique guidance required.
Im starting to learn ML by myself. I have a course starting ML from next year, but Im too eager to learn it before the course begins. I always learn by starting to solve a problem and then read up the theory behind it. So in a way, when I solve the problem Id have read the theory behind it and applied it at the same time. So now, Im trying to classify images into 5 categories. To make my life easier I have even got the feature vectors. I just need to classify them into multiple categories. How do I go about doing this? What are the various techniques? How accurate are they? How do I tweak the performance? Which programming language should I use? Please help me out with this. Thanks in advance! 
comment_no.=000--> For a complete beginner I'd recommend playing around with [scikit-learn](http://scikit-learn.org) in python. The site has a lot of tutorials and explains the difference between classification (binary and multi-class) and regression. It also talks about feature selection and normalization a bit.

More than anything, it has the shallowest learning curve of any ML toolkit I've worked with, and it has great pointers should you want to learn more.

Also, read up on evaluation. It's really key to getting good results and comparing models: accuracy is not everything, and sometimes it's not informative. For example, if you were trying to predict whether a car has a boot on it from an image, you could probably get 99% accuracy by always predicting "no boot", since most cars are not booted.
-------------
Question=000275  Class of Concepts
I have a class of concepts C of the form (a ≤ x ≤ b) ^ (c ≤ y ≤ d) where a, b, c, d ∈ {0, 1, 2}. I have two questions.
1. How many distinct concepts C can be formed?
2. What happens if we allow a, b to take also negative integer values, that is, a, b ∈ {−2,−1, 0, 1, 2} while c and d remain with values in {0, 1, 2}? How many distinct concepts as described above are there in this case?
comment_no.=000--> Your notation is a little bit unclear to me. What does ^ mean, and why can't you just say that x and y are elements of {0,1,2} or {−2,−1, 0, 1, 2}? What are you trying to do here?


-------------
Question=000276  General question about data sets with large number of boolean columns.
So let's say I have a data set where each row is a very long list of booleans and I want to predict the value of certain columns given other columns, but not always necessarily the entire row (minus the column I'm predicting). So basically I will have a subset of a complete row and would like to predict the target column's value. What methods should I be focusing on to accurately tackle this problem?
comment_no.=000--> A [Bayesian network](https://en.wikipedia.org/wiki/Bayesian_network) would probably be ideal, since it lets you input the values you know and then gives you probability distributions over the rest. This can be a fairly good option if you know the structure of your domain so that you can construct the network manually so that only probability distributions need to be learned. Otherwise you need to do structure learning as well, which is more difficult.

Neural networks may also be an option. I've never seen them used in the wild, but it sounds like maybe you could use interactive activation networks (see [chapter 2 here](http://web.stanford.edu/group/pdplab/pdphandbook/)), which actually end up functioning a lot like Bayes nets. Otherwise you could try using some kind of [autoencoder](https://en.wikipedia.org/wiki/Autoencoder). These are basically networks that are trained to output their input. If you then specify incomplete input, it should autocomplete. Make your true/false inputs 1 and -1 so that you can use 0 for missing data. 
-------------
Question=000277  Checking to see if an annotation tool exists before building one -- labeling images?
-------------
Question=000278  Just started with Introduction to Statistical Learning
and i have some basic questions.

equation 2.1 states:

    Y = f(X) + e

quoting:

"Here f is some fixed but unknown function of X1, ..., Xp, and e is a random error term, which is independent of X and has mean zero.  In this formulation, f represents the systematic information that X provides about Y."

questions:

- what do they mean here by systematic information?  
- if e has mean zero, then why even include it in the equation?
- are there any scenarios where e is non-zero?  if there are, what are some examples?
- is e a vector?  a set?  a scalar?

thank you.
comment_no.=000--> Let me see if I can help:

"systematic information" probably just means "information". The idea is that knowing X gives you some information of the value of "y". The  function "f" describes what different values of "X" tell us about "y" so it could be said to represent that information. "systematic" might be just be a reference to the fact "f" is deterministic.

e has the same form as Y and the output of f(X), so if we are trying to predict a scalar with f(X) it would be scalar, but if we are trying to predict a vector with f(X) it will be a vector of the same size. However note e is not a fixed, deterministic value. Instead it is a random variable (or vector of random variables).

It's actually quite important to include 'e' even if it is zero mean. We expect it to be non-zero is almost all cases where we are trying to model something occurring in the real word. 

To explain further, typically we would be using this sort of equation to model a real world process. For example, maybe we say Y = 0.18*X where "Y" is the amount a customer tipped and "X" is the amount they spent. However inevitably this equation will be wrong much of the time (many customer tip more, many tip less). Even if we tried to make the equation more complex (we could add terms to account for the time of day, where the customer was a repeat customer, ect.) the equation would still be wrong most of the time. In fact in general most real world processes are too complex to exactly model with a single deterministic equation "f", so we commonly model processes by finding an equation that is almost correct, and account for the fact there are other random or unaccounted for factors that affect what values of "y" we get for a given "X" in random, hard to predict ways by including the "e" term.

comment_no.=001--> if e has mean zero it means that the error can go on both sides of a regression line more or less with equal probability. i.e. positive and negative error (positive error as in your f + positive value e, negative error as in your f + negative value e) will occur normally.

if e is non-zero? Sure, if you tend to have a model which for example always over-estimates the regression output, then the error will be leaning towards negative values. e.g. if you predict on average a value of 110 for every true value of 100 then your model is estimating higher values on average and so the error would be 100-110 = -10. 

e can be called a vector sure if it's the list of errors for a list of predictions, but if you're talking about one prediction, then there is only 1 error value for it, this is of course assuming only a single variable. 
If you have a multivariate model then if you predict (10,100), the error will be (eX, eY) i.e. a vector of the same size with individual errors inside corresponding to the errors in the prediction for each axis. 
comment_no.=002--> also, is there a subreddit dedicated to the discussion of this book?
-------------
Question=000279  Good Data Preprocessing Python Module?
I have to do a lot of data preprocessing for my machine learning task.

Specifically scaling, inserting missing values, splitting large multidimensional numpy arrays into training, validation and testing sets, handling timestamps and timeseries data.

I have been building small methods to handle the data with mostly sklearn and various numpy functions.

Is there a good library I can use that will help me with preprocessing data? Is sklearn the best option?
comment_no.=000--> For scaling, inserting missing values and handling timestamps and timeseries data the answer is [pandas](http://pandas.pydata.org/). It has labelled dataframes as basic objects, a wide variety of tools and methods for handling NULL or missing data (filling, dropping, back filling, forward filling, interpolating, etc.), and unparalleled timestamp and timeseries facilities. It also has great tools for sampling, munging, merging, filtering, summarising and plotting data, but those are just gravy at this point. If you don't know pandas you should learn it. There's a [great tutorial](https://www.youtube.com/watch?v=5JnMutdy6Fw) by Brandon Rhodes that will get you started, and after that the docs are excellent.
-------------
Question=000280  Resources to learn wavelet based dimension reduction.
-------------
Question=000281  How to whiten ReLU input units?
-------------
Question=000282  How to construct logistic regression model with scikit-learn's coefs and intercepts?
-------------
Question=000283  sklearn PCA with Pandas DataFrames
When passing along a pandas DataFrame to a PCA() object,  how can I tell which of the input vectors are being chosen when using n_components='mle'?

    pca = sklearn.decomposition.PCA(n_components='mle')
    pca.fit(df)
comment_no.=000--> PCA doesn't choose input vectors. It (lineary) combines them all.
-------------
Question=000284  [Beginner]How to use K means clustiering on data with missing values?
-------------
Question=000285  Anyone care to take a stab at this non-descript classification task? Trying to validate my results.

comment_no.=000--> I get 80,4% on a 4 x 20 x 1 ReLU network
comment_no.=001--> I'm trying to validate the results I'm getting. I replicated (or so I think) an algorithm from a paper that achieved ~78% accuracy using a deep neural network ([4, 3, 2] hidden layer), but I cannot seem to get more than 63%. SVM gives me 55%, a generic MLP model from deeplearning.net gave me 48%.

The dataset has 4 inputs with domain [0,1]. The single output is a binary 0 or 1. There are 1000 samples in the dataset. I recommend you come up with your own train/test split to validate your model. I used 5x2 cross-validation, with 33% of the training data set aside for validation.

Feel free to use any model you wish, but please explain how you got it :o

Added difficulty: don't transform the inputs (i.e. use them as is).
-------------
Question=000286  [Beginner] What is a good ML textbook that contains pseudo code of algorithms?
I have been looking at books. I like Tom Mitchell's book on machine learning as it has some pseudo code for C4.5. Does anyone else know of other similar books that have pseudo code? If not, does anybody know where decent documented source code is for a ML library? I took a look at WEKA and it wasn't the easiest code to follow. 
comment_no.=000--> [Introduction to Statistical Learning with Applications in R](http://www-bcf.usc.edu/~gareth/ISL/)

When I help companies interview data scientists, I advise that candidates should be able to open this book up to any page and explain the concepts.
-------------
Question=000287  [Beginner] Can I use Machine Learning to Predict the Likelihood of an Order Being Shipped?
-------------
Question=000288  Fraud detection with unsupervised learning
I'll preface by saying I am brand new to ML.

I have a data set of ~10,000,000,000 (fake) transactions where each row is the username, time of transaction, credit card number, device used (iOS, android, laptop, etc) and amount. I need to determine which transactions are likely malicious/fraudulent and which aren't. I would prefer to approach this with unsupervised learning of some kind, as going through these and manually labeling ones I *think* are bad would be super tedious and not necessarily correct.

Is there a good approach to this problem with unsupervised learning? I've read about random forests and decision trees (high level overviews) and maybe they would do the trick?

Thanks
comment_no.=000--> Fraudulent transactions are likely different from 'normal' transactions, so you are basically looking for outliers. You could start looking at some histograms / scatterplot of your data to begin with. You could use clustering (unsupervised learning) to determine what are 'normal' transactions, and then you could find which transactions are not near clusters, and are thus likely fraud. You could start with some simple clustering algorithms such as kmeans or hierarchical clustering approaches. If you really want to do something fancy you could try using TSNE, but I would try easy approaches first. You will have to find some algorithms that will scale to your dataset though (look for large scale clustering toolbox or something like this)...
If you have no information about which transactions are fraud (it sounds like you don't have access to this information) you cannot train any supervised models such as random forests or decision trees (those are usually supervised models, unless you are talking about some special models). 
-------------
Question=000289  What type of machine learning or decision making should I start looking into for this task?
Thank you in advance for reading this - I know enough to know that I don't really know anything.  But here is what I am trying to accomplish:

I have two populations of individuals, let's call them A and B.

A is a mostly static group of people with the normal traits a person would have, age, gender, locale, specific degrees in some form of education, a number of years involved in our project, etc.  They also have a history we could review of encounters and outcomes we could teach a system with.

B is an ever changing population of people with the same traits but with the addition that we want them to take an action.  Let's say we want them to read more books, or walk more.

The "game" is that a person from A is "up" to talk to someone and try to convince them to take an action.  I am wondering what sort of machine learning or approach to AI would watch and learn the successes and failures in population A and make the best decision on who from population B they should be matched up with.  What would most likely result in an action taken.

I simply have no idea on what direction to head with this?  A few word answer would get me going for some time on my own.

Maybe I am dreaming but I have always been fascinated by AI since I read Marvin Minskey's society of the mind decades ago.  I think that was the name.  I would love to pursue leveraging some sort of machine learning in this task.  Sadly aside from a fascination and some more light reading I trended more into mainstream programming :(

Thanks for any help, even if it is "Your nutz, this is not something for machine learning"

Thank you,

Bill


comment_no.=000--> If you have data on 'successful' interactions. That is when a member of particular A succeeds in getting a particular member of B to perform an action then I would model this as classification problem for edge prediction on a graph.

You have a bipartite graph of A's and B's and you have your known edges, when an A has successfully persuaded a B for a task. You then use this data to train a model to look for additional connections. You can either model each edge as 'successful' (binary classification ) or each edge as 'successful for action K' then a multiclass problem.

Either way this differs from normal classification in that you are using attributes of both A and B to make the prediction. 
This may make an advantage over just modelling A to predict if they can persuade any B.
Have a look at this paper: http://arxiv.org/pdf/0806.0215v2.pdf its a biological paper so you need to read through the biology but could be useful. 




-------------
Question=000290  Find the wonky data [x-post r/askstatistics]
-------------
Question=000291  [Beginner] error rate stays same from beginning to end
I started learning NN. After some reading of theory I stumbled upon this [blog](http://iamtrask.github.io/) and decided to follow it and implement by myself. I am trying to use iris dataset (iris-setosa and iris-vericolor) but it seems that my NN stays on 0.5 error rate. I tried to change alpha, hidden layer size but it seems that in the end error rate is equal 0.5. Code is [here](http://pastebin.com/1AaMvQg9). Output:

    Error (iterations: 0): 0.436383143672
    Error (iterations: 10000): 0.500000789314
    Error (iterations: 20000): 0.500000195026
    Error (iterations: 30000): 0.50000011538
    Error (iterations: 40000): 0.500000692513
    Error (iterations: 50000): 0.500000173484
    Error (iterations: 60000): 0.500000148476
    Error (iterations: 70000): 0.500000059061
    Error (iterations: 80000): 0.500000134286
    Error (iterations: 90000): 0.500000466787

Any help or suggestions?
comment_no.=000--> If its a balanced binary classification problem then the expected error rate of a random guess should be 0.5.

I would suggest implementing grad check, though it may not help you here.
comment_no.=001--> Couldn't spot an obvious error in the code. I don't speak alot of python though.

Some questions that might help you:

- How do the learning curves look like? (alpha too big?)
- What are the values of the target vector? (y in [0,1]?)
- Is your data normalized? 
comment_no.=002--> Try

l2_error = l2 - y

?
-------------
Question=000292  Can a NN be taught to multiply two numbers?
Hi,
I've been playing with regular neural networks with ReLU input, hidden and output units. 


I have successfully trained it to add two real numbers, just approximating binary functions for fun, of the form f(x, y) = z. 


The input is 4 numbers

[negpartofx pospartofx negpartofy pospartofy]

Ex. f(-5.1, 8) becomes f([5.1 0 0 8]). Output is similar [negz posz]


Addition is then simply to learn direct transfomration matrix

[1	-1	1	-1]

[-1	1	-1	1]


I've tried similar approach to learn general multiplication, but with no luck.
Ie. I expect it to work for any real numbers, it sort of works for numbers within a limited range, but the results become too low for large numbers it has not seen before. Something tells me it cannot be done to learn general multiplication... yes or no?
comment_no.=000--> I'm afraid your model does not fit the problem. You might want to think about what kind of functions you can model with a three layer ReLU feedforward net. 
As a first step: What is the smallest neural network that can be trained to execute general addition of real numbers? (You might need to use another activation function)
-------------
Question=000293  What is the best way to address the vanishing gradient problem in neural networks?
I'm relatively new to neural nets and machine learning in general, and recently stumbled onto this problem when attempting to add more layers to a network used to classify digits. As I added more layers, my accuracy fell.

I have heard of a few ways this is addressed, but was wondering if someone could highlight what the best techniques were, and what the pros and cons of each of them were.
comment_no.=000--> Relu units are the most common solution.  Basically the idea of these is to have a non-linear function whose gradient is either on or off, that way it pushes the error signal all the way down without diluting it at each layer.  

I don't know enough myself to talk about pros and cons of different methods.
-------------
Question=000294  What's the hardest part about training ML algorithms?
Is it getting the massive datasets and cleaning them? Or is it waiting for the algorithms to finish? Or is it guarding against overfitting?
comment_no.=000--> Cleaning > overfitting > waiting
-------------
Question=000295  CNN with engineered features
If I am working with a data set of faces, 32x32 pixels each, and I want add an engineered feature like 'is the user wearing glasses'. Would it be reasonable to add a new row, 32x33 pixels, with a binary value to signify glasses or not. What's the pro / con of this approach ?    
    
Thanks 
comment_no.=000--> I suppose that an advantage of this approach is that it is easy to implement. It's usually a good idea to try the simplest approach first and see if the result is good enough for you before you invest more time and effort developing something more complicated.

I do think this approach isn't quite *right*... Convolution works because all the inputs are the same kind (i.e. pixels) and there is a meaningful structural relationship between them. Your high-level feature is not a pixel, and it's not really meaningful that pixel [32,32] is adjacent to a "glasses-pixel" and pixel [16,16] isn't. 

I think the more correct approach would be to just add a single input node to your network to represent this feature, that you then connect to *all* nodes in the first hidden layer. Or actually, you could play around a bit with this, because they say that each layer extracts higher level features, so perhaps it is better to connect your glasses-input to a hidden layer that has similarly high level features. Or you could connect your glasses-input to *all* hidden nodes regardless of layer. 

The "important" thing is that you treat this as the feature that it is and not a pixel (or row of pixels), but like I said: you might try that first and see if the result is good enough for you, because this probably involves fewer changes to your code.
comment_no.=001--> (Mostly just echoing what /u/CyberByte said) Adding just row is a bit odd because only some convolutional filter applications will 'see' the feature. So, if you add the indicator as the 33rd row, convolutional filters applied to the 1st row will take only pixels as input while convolutional filters applied to the last rows will take both pixels and the indicators as input, which is probably a bad thing considering those filters will have their weights tied. Instead I would add the an extra channel to the input, so instead of 1x32x32 input switch to a 2x32x32 input, where the second channel is binary value indicators. This paper did exactly that to encode information about player skill level when it comes to playing Go:

http://arxiv.org/pdf/1412.6564.pdf

A more efficient but maybe harder to implement method could be to add the indicator with a weight to the output of the first convolutional layer before the non-linear functions is applied. So you could perform convolution on the input, add w*b where "w" is a new parameter and "b" is an indicator to each output unit, then apply your activation function and proceed as normal.
-------------
Question=000296  Is over-training a risk when a person marks lots of messages in their preferred email client/site as spam/not spam?
Let's say that I receive on the order of 50 messages a day.  If I diligently make sure that every one every day is correctly flagged as spam vs. not spam, is over-training a risk?

If so, how do I know at what point to stop training?

My knowledge level: I'm can describe a bit how Naive Bayes works, I can define the term over-training, and I know that I should be saying "ham" instead of "not spam" - but the latter seems more appropriate for a "questions" subreddit.  :)

(I use Thunderbird specifically, but I think this is a more general issue that would be applicable to any email client/app/site?)
comment_no.=000--> I haven't really used Naive Bayes much, but I very much doubt that you're going to "overtrain" your spam filter. The problem with overtraining a classifier is that it can lead to overfitting, which is when random error/details/noise (from the training set) is modelled instead of the real underlying relationship. This requires that the classifier is flexible enough to model all of those tiny details; this is usually called high variance/low bias. Variance typically increases when the classifier has more learnable parameters, so e.g. a big neural network has higher variance than a smaller one. Naive Bayes classifiers tend to have fairly low bias and are therefor less susceptible to overfitting.

Overtraining tends to only be a problem when a classifier's training procedure goes over the training set multiple times to incrementally refine its model. One example is (again) a neural network (multi-layer perceptron trained with backpropagation). If you have 500 training items and you can afford to run 1,000,000 training iterations, then you can train on each item 2,000 times and you might overfit. If your training set is larger (e.g. 5,000) then you can train less on each item (e.g. 200 times) and you are less likely to overfit. In fact, even if you do train each item 2,000 times as well, overfitting is still less likely because it's more difficult to capture all the details of 5,000 items than for 500. Expanding your training set virtually always leads to less overfitting. 

With Naive Bayes you really only consider each training item once. The only way to increase the number of training iterations (and potentially run the risk of overtraining) is to expand the data set.

It's probably not literally impossible to screw up a classifier by expanding the training set. For instance, if you add (almost) the same item a million times (and few other items), that will probably do the trick. But you should be fine if the e-mails that you flag form a sample that fairly represents the whole body of e-mail that you're getting. In ML terms you want your sample to be independent and identically distributed (i.i.d.).

tl;dr: Flagging more e-mails probably makes your spam filter more accurate, not less.
-------------
Question=000297  What do you use for gradient descent? Do you implement your own or use pre-built software?
As I understand it, there is software that implements cross-validation and gradient descent for you, and all you have to do is supply the cost function and its derivative. I was wondering, do most people implement their own, or use off-the-shelf software/library for it?
comment_no.=000--> scikit-learn allday errday.
comment_no.=001--> I use fmincg inside of Matlab.
-------------
Question=000298  What is the best classifier from the ones tested here?
For starters, I'm a complete beginner in this field and I'm just dipping my toes into this sea of knowledge. 

And now, what I'm trying to do is to test and understand which is the "best" classifier and the one who performed best from the below list of classifiers ([**click the link**](https://drive.google.com/open?id=0ByAaxTJ8CHF7flREZ05BMzRTdmhuVjZNYlRrRFRMeW1pVFZ2TnlyTDlFNHJXMGpuR3FXeWM)) - 
I've explained what the graphs mean, in detail, down below.

To put this into context, I'm trying to write a supervised-learning application with Python and Sklearn and what I am trying to do is find the right classifier for correctly classifying a resume from a list of resumes.

So far, my learning algorithm has these 2 phases:

* pre-processing
* model training
and then prediction based on the trained model.

The pre-processing phase is where I made all of the adjustments and tried different methods before generating both a CountVectorizer matrix and a TFIDFVectorizer matrix and compared the "performance" of the classifiers trained with them.

The distinct parts of my pre-processing phase are simply using a TFIdfVectorizer and CountVectorizer, or in combination with the following:

* stemming (with Lancaster / Snowball stemmer from NLTK)
* word correction using [Peter Norvig's approach](http://norvig.com/spell-correct.html)

So, for training my model I've tried combinations of all of these (which can be found in the link):

* a simple CountVectorizer over my training text
* a simple TFIdfVectorizer ... over my training text
* Lancaster stemming + CountVectorizer (... over my training text) etc.
* Snowball stemming + CountVectorizer
* Lancaster stemming + TFIdfVectorizer
* Snowball stemming + TFIdfVectorizer
* Lancaster stemming + Peter Norvig's word correction algorithm + CountVectorizer
* Snowball stemming + Peter Norvig's word correction algorithm + CountVectorizer
* Lancaster stemming + Peter Norvig's word correction algorithm + TFIdfVectorizer
* Snowball stemming + Peter Norvig's word correction algorithm + TFIdfVectorizer

and in order to have a better overview of how my classifiers would perform in a dynamic context (resumes can have more or less the same number of words), I've used a variable value for min_df (minimum document frequency) to range from 18 - the number of documents in the test scenario TO MAX(word document frequency) - WHICH IS WHAT IS DISPLAYED ON THE GRAPHS.

So, actually I'm testing the "performance" of my models and how they work with different numbers of training features.

As mentioned at the beginning, I'm a complete beginner and I've picked the classifiers to test these based on suggestions from people both here on reddit and on other forums related to ML.

One of my main questions, which I hope to find an answer to here is if some classifiers would be considered overfitting, since this is not currently clear to me. 

For example, I can understand that the Multinomial Naive Bayes is overfitting for most of the CountVectorizer approaches. 

* NuSVC is too "unstable", which I'm guessing makes it a poor choice of a classifier for this scenario.
* But how about Bernoulli Naive Bayes? Would it be considered a "good" classifier? Does it overfit at the beginning but then become a more "real" and better performing classifier towards the end, even if it's prediction rate is not 100%? 

* Same thing for Gaussian Naive Bayes..
Also, some people suggested that LDA ( Latent Dirichlet Allocation ) is a good classifer for this kind of scenario.

I'm hoping that someone could help me make some sense out of my results, since I'm a bit confused on how I should interpret them.

And if there's anyone interested in how I actually did this, with code, you can find it [**on GitHub**](https://github.com/radu-gheorghiu/RecommenderSystem/blob/master/Implementations/bag_of_words_sklearn/bag_of_words_sk.py). 

Disclaimer: I'm a terrible programmer with a very non-Pythonic way of writing code.

And in the end, thank you in advance for reading this "story" and your help is greatly and immensely appreciated!
comment_no.=000--> http://scikit-learn.org/stable/tutorial/machine_learning_map/index.html
comment_no.=001--> I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit:

- [/r/machinelearning] [What is the best classifier from the ones tested here? : MLQuestions](https://np.reddit.com/r/MachineLearning/comments/3inerg/what_is_the_best_classifier_from_the_ones_tested/)

[](#footer)*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))*

[](#bot)
-------------
Question=000299  A question about softmax layer neurons.
How are the partial derivatives ∂a^k' / ∂a^k = 0 for all k' != k, where a^k is the output from the kth neuron of the softmax layer. Shouldn't a change in the output of one neuron of the softmax layer cause a change in the output of other neurons as well?

I'm having a lot of trouble wrapping my head around this. Any help is appreciated, thanks!
comment_no.=000--> You should do the derivatives with respect to the node's *input*, which I'll call `z`. ∂a^k' / ∂z^k = a^k' \* ( 1 - a^k ) if k=k', and -a^k' \* a^k if k' != k. 
-------------
Question=000300  I have a square matrix of data (from simulations of 2D PDE's) What kind of fun things can I do?
basically as title says, I have a lot of data from solving some basically stochastic/noisy diffusion fields in 2D and I am interested in just playing around with the data with some ML tools (probably with pythons sklearn toolkit)


I just don't want to head in blindly at the moment, so I wanted to ask for suggestions first on what could/can/should be done! (quite vague I guess, apologies for that but this entire field is very very new to me but fascinating)


Thank you :)

quick edit: forgot to add these solutions give some sorta nice pattern formations/fractals-like structures hence my interest in ML application
comment_no.=000--> plot that shit
-------------
Question=000301  Can you please help with a perceptron (neural networks) problem?
I have the following neural networks problem and couldnt find any answer on the web. Any hints would help. I am not looking for a complete solution, just some pointing in the right direction.

PROBLEM:
Write the upper bound Ko of the number of iterations needed to a perceptron to learn a linear separable set with the Rosenblatt rule:
w(1) = w* / 10^6
What category of points makes the learning hard? Justify the answer.

Thank you,
Dan.
comment_no.=000--> It's basically asking you to prove the perceptron convergence theorem (easy to google for writeups).

Gvien that perceptrons are linear classifiers, points with non-linear boundaries are impossible to learn. The canonical example is the xor function. Minsky's perceptron book proved this and started the first AI winter.
-------------
Question=000302  Standard Deviation Between Different Runs of SVM Cross-Validation Folding
-------------
Question=000303  How should I use the textual data given below in order to carryout a classification task?(Please help by giving some example code)
-------------
Question=000304  Hoping to make a project involving the classification of scanned stamps (postal)
-------------
Question=000305  Dataset recommendation for binary classification?
-------------
Question=000306  Support Vector Regreesion Model/Equation with SMOReg in WEKA?
-------------
Question=000307  Training methods other than by example
Hey team, pretty general question for you. Every time we talk about training a machine learning algorithm, we are referring to training by example. Are there any other methods to training?

Take image classification via neural network for example. You can show a convoluted deep net 10,000 images of airplanes, and eventually it learns to find patterns in the edges and so forth. Is it not possible to help the algorithm along by letting it know that most bi-planes have 2 sets of stacked wings, or something similar?
comment_no.=000--> That is an idea older then ML. However, "explaining" to algorithm facts about bi-plane is far from trivial. So ML is exactly trying to avoid most of explaining and lets algorithm learn facts from examples. 
-------------
Question=000308  What is "Deep Learning"
As opposed to just "Machine Learning"?
comment_no.=000--> "Deep Learning" is a technique developed by the field of Machine Learning. Deep learning is a rebranding of neural networks, which historically didn't perform very well on many problems. More recently, neural networks have been performing well because machine learning researchers figured out a set of techniques that make it easier to learn deep (complex) neural networks. Machine learning does not necessarily involve neural networks.
comment_no.=001--> My understanding of this topic is a bit limited, so I'd be interested if this response is off target, but here's what I think it means:

When you have a neural network, you might train it using some form of the backpropagation algorithm. This algorithm basically looks at the rate of change of the value at a node as the final "answer" node varies, so you run your inputs through the network, look at the answer you get, and then adjust the weights based on how far off you were from the right answer. Follow so far?

Well, when a neural network is very "deep" (has a lot of layers of neurons), the gradient for adjusting those weights can get vanishingly small... or it can blow up, so the values swing wildly. That's because as you have more layers, instead of layer A depending on layer B, you have layer A depending on layer B, which depends on C, which depends on D, and so on. So when layer M changes a little bit, layer A either changes a huge amount or not at all. Point is: the numbers get a little fucky.

A solution to this was to have an algorithm "learn" what kinds of features are going to be most useful to distinguish various types of output in advance of trying to train the network. That way, when it comes to tweaking your weight vectors to get the output you desire, you don't have as far to go. If you can figure out how to do that, then you can have much "deeper" networks. Why are deeper networks good? Well, I think that it's because the network can model nonlinear interactions much more easily. In theory, a very simple neural network can approximate any function ("universal approximation theorem") but in practice that may require many thousands of nodes in a layer. A deep network gives you a lot more power with a lot fewer nodes, which means faster computation.

So the trick is to train in the "features" before you ever start the neural network learning. The way I learned to do this is with a restricted boltzmann machine, but I would imagine that many unsupervised learning tasks would be useful for this purpose. What you do is basically set up a network that becomes very good at taking the input and... reproducing inputs, honestly. Because to reproduce inputs "convincingly," (so it's hard to tell made-up inputs from real inputs) the network has to have a sense of what the inputs "look like" generally. So you run your RBM until it is pretty well converged. Then once it has learned the data, you can add additional layers on top of it. There are some tricks to doing this, but that's sort of the idea. That way, when you start your neural network learning, it already has some pretty well-trained layers in the first couple layers (in the sense that the first few layers are picking up important features from the data). That's important, too, because as you add layers, the lower layers become harder and harder to train (for the reasons explained earlier).

In summary, you basically use a bunch of tricks to train networks with a lot of layers because they are better at modeling more complicated functions with fewer nodes.
comment_no.=002--> "Deep" learning refers to having a hierarchical stack of models on top of each other, each of which uses the output of the previous "layer" to produce another intermediate result that is a little closer to what we want to get. This is in contrast to "shallow" models that only do a single (albeit very complicated) transformation of input to output. 

Hierarchical/layered approaches work very well in computer vision, because images are made up of objects, which are made up of parts, which are made up of edges... So having a succession of highly specialized models detect ever more complicated features is a much more practical approach than trying to have one model detect everything from raw pixel values. 

Currently this is almost always done with many-layered neural networks, but there is also growing interest in "deep" graphical models. 
-------------
Question=000309  Neural Network to learn to play Video Games
Is it possible to use a Deep Belief Network to learn and play a video game (e.g Tetric, Pacman, Snake)? 

I've read about Deep Reinforcement Learning being used for it, but I'm unsure how far this is from a DBN.

Apologies for how nonsensical this may be, I'm still new to Neural Networks as a whole as I'm in the planning stage for an undergrad project involving it.
comment_no.=000--> Hey OP ! 

So I'm not very familiar with a reinforcement method but you should look up MarIO. Not sure if it's related but hopefully it can help give you some inspiration. I'd link it now but I'm on mobile. 
-------------
Question=000310  I'm an R user but know zip about ML. Help me out...
Alright guys, I know at some point I'll need ML in R for future projects at work. I want to be ahead of the game so the transition is as smooth as possible. Dear experts, consider the following:
1) I have no idea about ML;
2) I know R;
3) I know Statistics/Econometrics well enough to understand how to model some real world issues;
Given that, would you answer me the following questions:
1) Where can I learn the absolute basic about ML?
2) Given that I've learned the foundations, is there any books focusing on R and ML? I've downloaded a few ones but wouldn't mind a few extra tips.

Thank you!
comment_no.=000--> ["Introduction to Statistical Learning"](http://www-bcf.usc.edu/~gareth/ISL/)
-------------
Question=000311  Tutorials for SVM?
Where is a tutorial to use/implement SVM using a programming language such as R, Python, Java, or C#? I would like to both gain a deeper theoretical understanding and how to implement it in a programming language please. 

I tried google, but have come up empty. Please help. 
comment_no.=000--> Machine Learning with R by Brett Lantz and Python Data Science Essentials by Alberto Boschetti; Luca Massaron might be the answer to my own question
-------------
Question=000312  ReLU derivative in 0
Hi,

Recently I found a mistake in my code where I defined the derivative of ReLU as

1 if node unit is >= 0

This doesn't make much sense because it would be 1 for all nodes in a ReLu layer.


I corrected it to
1 if node unit is > 0


However, I've been running some tests switching back and forth, and it seems I have slightly faster convergence with the old derivative (>= 0).


What do you guys use?



It's undefined in 0

http://www.wolframalpha.com/input/?i=derivative+of+max%280%2C+x%29

http://www.wolframalpha.com/input/?i=derivative+of+%28x+%2B+abs%28x%29%29%2F2

comment_no.=000--> False alarm, I think I might be wrong on this one
-------------
Question=000313  Have a data set of more than 1GB, how to read it in R?
the 'read.csv' command leads to a hang. And R doens't allow opening files more than 5MB. What can I do in this case? Any help will be highly appreciated. 
comment_no.=000--> [Found this bit of reading](https://theodi.org/blog/fig-data-11-tips-how-handle-big-data-r-and-1-bad-pun)

Or maybe you could push the data into an RDMS (i.e. MySQL) and then pull the data into R?
comment_no.=001--> What do you ULTIMATELY need to do with the data??
-------------
Question=000314  Any recommended papers or books on time-series/sequence classification?
Hello everyone,

So I tried solving a previous research problem (first one I had the pleasure of working on alone) like a noob, i.e. I didn't really understand things like HMM and dynamic time warping so I didn't use it. In retrospect, it would have seriously helped out with my problems, and it would have definitely given me a magnitude of difference in results. 

Anyway, that was my first attempt at research as an undergraduate. Am I alone in this 'ooooh that would have solved all of my problems but I didn't use it' feeling? I hope not.

Anyway, I'm curious now, can you all recommend papers that would help with sequence classification for time-series data? My goal is to classify sensor data into one of several classes. 
comment_no.=000--> I am working on anomaly detection from dynamic strain or accelerometer measurements. In my case I try to extract some damage sensitive features (modal parameter) before the anomaly detection step. 

The following book may be helpful for you. The authors describe multiple methods for damage sensitive feature extraction from time-series.
 
Farrar, Charles R., and Keith Worden. Structural health monitoring: a machine learning perspective. John Wiley & Sons, 2012.

-------------
Question=000315  FREE COURSE: Amazon Machine Learning

comment_no.=000--> I saw the roadmap for the course, seems pretty good. The instructor, unfortunately sounded a bit monotonous to me at the preview video. In case anyone here has taken the course, would you recommend it? 
-------------
Question=000316  Can artificial neural networks be programmed to 'mutate'?
I'm a regular scientist curious about machine learning. So, please be patient with me.

Can artificial neural networks be programmed to 'mutate' (quasi)randomly characterised nodes that may predict or act as hereustics to reach desired output nodes better than those features which may be hypothesised in advance? 
comment_no.=000--> Yes, they can. The [NeuroEvolution of Augmenting Topologies (NEAT)](http://www.cs.ucf.edu/~kstanley/neat.html) algorithm is one method to do it. Such an approach works well when the optimal network topology isn't known (or surmised) in advanced, and when there is a good way to formulate incremental improvements. 

But generally speaking just trying different hyperparameters (what kind of net, how many layers, how many nodes in each layer...) and going with what works best tends to be preferable from a certain network size upwards, because it is quicker.
comment_no.=001--> I don't know enough to fully answer this question, but there are genetic algorithms in machine learning which perform similar to what you described. It is at least a starting point for you to research.
comment_no.=002--> Yes, most all machine learning algorithms do use a type of "artificial evolution". Even in something as simple as linear regression, or finding a line of best fit, a program will probably iterate through "generations", continuously adapting a hypothesis until cost between known values is minimal. 
-------------
Question=000317  Interpreting improvements to Logistic Regression model in R
I have an existing logistic regression model that works fairly well (100% precision and 95% recall in initial training), but I'm looking to make improvements to it after about 6 months time.  I've seen some false positives in that time and I've added some features (from about 7 to 15) to improve the accuracy.

The initial model was fit on about 1200 observations (800 train, 400 test), and was fit using stepwise regression to determine the optimal feature set based on AIC.  I used R to fit this model using glm.

I'm looking into a couple options for improving, primarily PCA (prcomp) and regularized regression  (glmnet).  In addition to my goal to improve accuracy, I'm also interested in keeping the model interpret-able to someone without R access (i.e. if someone were to query new data, can they easily apply my model to predict the result without needing R?)

My problem is that I don't know whether to use PCA and then apply logistic regression on those PCs (i.e. Principal Component Regression) or work toward regularized regression instead.  I'm leaning toward regularized logistic regression using glmnet, but having a hard time interpreting the plots from glmnet.  I've been following the [glmnet vignette](http://web.stanford.edu/~hastie/glmnet/glmnet_alpha.html#log), but I'm pretty lost, to be honest.  

I understand some of the use of the alpha and lambda parameters, but how do I use the output from glmnet to determine:
a)  the appropriate feature set, and
b) the appropriate values for lambda and alpha?  When would I use lambda.1se over lambda.min?

It's been a while since I was in university learning about regression, so I only remember the basics...I get lost quickly in the mathematical notation.  I'm also not able to share anything related to my data.  Can anyone link to a resource showing a detailed walkthrough of using glmnet, or explain to me more simply than the vignette?

Finally, I fear I'll end up with a model result that I'm not able to fully grasp.  Let's say I were to use this model result and later find prediction errors in production...how do I go back and determine where my model is failing (and therefore determine how to improve it)?

I know the answers to these questions are heavily reliant on my specific data set, but I'm looking more for general answers or links to good resources that could help me figure these answers out for myself.
comment_no.=000--> [deleted]
-------------
Question=000318  Minimizer won't converge for logistic regression program
I'm trying to do the logistic regression assignment from Andrew Ng's course in Python. I got it to work in Octave, but for some reason when I try migrating it over to Python the mininimzation procedure fails. The cost function just diverges to NaN. I can't figure out what's causing it. To me everything looks exactly the same. Here's my Python [program](http://pastebin.com/zGXKb7KF). Here's the [dataset](http://pastebin.com/yjrxw4DW). Here's a description of the [assignment](http://s3.amazonaws.com/spark-public/ml/exercises/on-demand/machine-learning-ex2.zip). Can anyone see an obvious bug?
comment_no.=000--> You should try using a debugger or print statements to inspect values
-------------
Question=000319  How many learning curves should I plot for a multi-class logistic regression classifier?
If we have K classes, do I have to plot K learning curves?
Because it seems impossible to me to calculate the train/validation error against all K theta vectors at once.

To clarify, the learning curve is a plot of the training & cross validation/test set error/cost vs training set size. This plot should allow you to see if increasing the training set size improves performance. More generally, the learning curve allows you to identify whether your algorithm suffers from a bias (under fitting) or variance (over fitting) problem.
comment_no.=000--> ||Ypredict-Yactual||2
-------------
Question=000320  Cost function confusion
In many of the guides I have read online, (in particular [this one](http://www.holehouse.org/mlclass/09_Neural_Networks_Learning.html)), there is a step at the beginning of back propogation where you calculate the discrepancy between your observed output and your target output.

The part that I am very confused about is that sometimes this discrepancy is calculated as 𝛿 = a - y (where "a" is the observed output and "y" is the target output). Later on, the guides will discuss a "cost function," which seems to me to be the same thing as the difference formula above? However, a totally different equation (http://imgur.com/guz1oxp) is then given.

Is the difference formula just an abstraction of the more complicated formula?


Thanks!
comment_no.=000--> In regression, the cost function is usually the mean or sum of squared errors -- each individual term in the sum is (a-y)^2. What you called the discrepancy is actually the derivative of this error wrt to a as used in backpropagation: http://www.wolframalpha.com/input/?i=derivative+.5*%28a-y%29%5E2+wrt+a

When you do binary classification the cost function is [the cross-entropy](http://deeplearning.stanford.edu/wiki/images/math/f/a/6/fa6565f1e7b91831e306ec404ccc1156.png) between the wanted distribution and what the network predicts. This is a special case of the [negative log-likelihood under a multinomial distribution](http://deeplearning.stanford.edu/wiki/images/math/7/6/3/7634eb3b08dc003aa4591a95824d4fbd.png)\* that is used in multiclass problems.

\* for neural networks this formula is slightly different
-------------
Question=000321  My classifier has 100% accuracy, is something wrong or is this good data?
Hello everyone,

I ran several models on a binary classification problem, each performing at around 70-80% classification accuracy. 

I then used [stacking](https://en.wikipedia.org/wiki/Ensemble_learning#Stacking) to build a new model on top of those and I got near 100% accuracy.

My process:

* run several models to see how well they performed for accuracy
* pick the top 4 models (lda, random forest - 15 trees, logistic regression, and random forest - 150 trees)
* create new features from those models - taking the predictions of each model (using cross validation so that I would train 4/5 of the data to predict the last 1/5, then swapping out so that there would be no data leakage).
* run a binary classifier on the new dataset which includes those extra features
* get 100% accuracy. 

I tried 5-fold cross-validation on the new model and it performed at around 96%+ accuracy.

Do I need to check something else to make sure that this model is legit? 
comment_no.=000--> Make sure you are evaluating the ensemble on totally untouched data. By untouched I mean that this data hasn't been used for fitting the base or "stacker" models.

edit: and also not used for selecting hyperparameters for your stacker model
-------------
Question=000322  Need ML Algorithm suggesstion ?
Hi, 

I have read ML long time ago. I've crunch time and in need to choose the algorithm to complete my following task:

Traveller, is visiting my website. I make them fill the form and have alll the necessary signal (attributes) with me like whether they have booked flight or not, whether email is guenine is not, phone no is given or not, trip date is fixed, destination location is fixed or not.

But along with that I have many visitor who don't fill the form completely or just uses fake phone number. 

I again re-iterate, I have lot of signal available with me, and I need to filter out the traveller who is certain to go for travelling so that I can personally contact them.  I also need some score as well on the scale of 10.

Which ML algorithm is best suited for this job and why ?
comment_no.=000--> Regularized logistic regression.

Because you need a starting point, you can't expect to find the perfect method on the first attempt. 
-------------
Question=000323  Probability distribution upto a normalizing constant
I have always had trouble understanding what it means when we say: we know a probability distribution upto a normalizing constant. What do we exactly mean here? Do we mean that we know the probability distribution p(x) = p~(x)/Z as a whole with the normalizing constant Z? Or do we mean that we know the probability distribution only in terms of p~(x) and do not know the normalizing constant Z?

Please help.
-------------
Question=000324  Gradients turn zero after a few epochs
-------------
Question=000325  Day of the week with generic backward prop NN's
I have been with temporal data and have been representing Monday as "00001" and Friday as "10000".

In general is this a good idea or is it better to represent the data as the day of week in binary? (000,001,010,011,100,101)

I realize this is a pretty generic question.  I am just looking for a "rule of thumb".


comment_no.=000--> From what I know, first method is common for categorical data (days of the week), named  "one-hot encoding". 
I don't see any advantages in second method besides slightly smaller number of inputs.
-------------
Question=000326  What is the most common model for acoustic classification?
I'm writing a few labs on the subject and I'd like to get some opinions on what I should teach.
comment_no.=000--> This paper here might be useful :)
http://cs229.stanford.edu/proj2013/CamenzindGoel-jazz_Automatic_Music_Genre_Detection.pdf
-------------
Question=000327  How to train lstm layer of deep network
-------------
Question=000328  How important is performing cross validation on your algorithm's parameters?

comment_no.=000--> a lot.

If you have a parameter you are changing it is just like another feature. 
If you run your algorithm with 100's or 1000's of parameter combinations.. you will have results which are 'significant' just due to chance.
-------------
Question=000329  Is there an accessible implementation of any of the image captioning neural nets that have recently gained attention?
-------------
Question=000330  Would you benefit from Deep Learning examples & tutorials?
I'm wanting to build a sort of blog that dives into different aspects of Deep Learning,  a project based/case study style learning experience.

Assuming the content was good, would you subscribe to a blog like that? Would you be interested in working through the problems?
comment_no.=000--> Definitely ! It sounds like it could be fun 
comment_no.=001--> I would definitely subscribe. Go ahead please do it
comment_no.=002--> If it's python, if it provides some math - not too hard - not too dumbed down - something that a probability/linear-algebra background can figure out - pretty pictures - links to further reading - links to foundational reading, if the projects have varied lengths so some are short and some are medium (100 lines) and few are large (200+). 

Yes. 
comment_no.=003--> Pseudocode with clear description of every variable would be a goldmine

Trying to detangle other's code on github is very cumbersome
comment_no.=004--> Yes please, I would definitely benefit from tutorials/guided learning!
-------------
Question=000331  How to represent multi-dimensional data as colors in Self-Organizing Map?
-------------
Question=000332  Long boolean vectors as data set - looking for suitable model / algorithm
Hey! I am new to this topic, therefore any help is greatly appreciated!

My data set consists of two pieces:

* One factor ranging roughly from -1 to +1

* A rather long (say 100-dim.) boolean array

In the training process, I want to find correlations between the factor and what values in the boolean array are set. Runtime, the algorithm should be able to predict a value for the factor when analyzing an array.

The previous solution used some kind of SVM. Unfortunately I don't have access to the source code and can't really think of a way to re-implement this.

Thank you for any kind of help!
comment_no.=000--> My uninformed opinion:

One-hot encoding for the array boolean values, assuming you have enough data for that dimension size. Then run a logistic regression on it for some interpretable results (by reading the coefficients), or run a random forest for high accuracy. Maybe you can determine the importance of features with tree-feature selection... ah I can't link to it because sourceforge is down at the moment (they host scikit-learn). 

Then if you cut down on the features, your logistic regression model may become more interpretable. 
-------------
Question=000333  I've already implemented a Naive Bayes classifier to help me assess the sentiment of tweets. What other algorithms can I use?
I have a data set of tweets, and I need to classify them as either pro, anti, or neutral with respect to some topic.

I've already used the Mulitnomial Naive Bayes Classifier on sklearn, and would like a second algorithm to compare the classification against.

My first thought would be k-nearest neighbours ( I can use  `countvectorizer` to turn the tweet into a vector of characteristics, and then run it through the alg).

Any other suggestions? 
comment_no.=000--> I've tried using a nb classifier like that for sentiment analysis on larger passages, but it didn't work too well.

You could build a long short-term neural network and then classify it's output, which is how sentiment analysis is often done with neural-networks.

Take a look at: http://deeplearning.net/tutorial/lstm.html
-------------
Question=000334  Problem understanding Bias, Variance, and cost/learning curves for train set, cross validation set and test set.
-------------
Question=000335  Trying to decide on algorithm for my data
Hi. I've began reading a lot about Machine Learning, but having no advanced background in statistics/maths, I find it hard to relate to most of the online information for selecting the proper algorithm on my data. I do programming for a living. This is a pet project as will be clear from my example, so I don't need an absolute best fit or optimal result. With this said, here is my problem:


I play Path of Exile, a free game. It's extremely similar to Diablo. It's got one of the most complex set of items attributes. The game allows players to have shops online (so list items with prices) as well as give the information about what the players are wearing. I spent months of time writing code to extract this data, so I have historical data about what items were worn, what they were replaced with, what items were listed for sale, which ones sold, how long it took to sell, etc.

This data is rather massive, were talking a few hundred gigabytes. My data is split in 2 big data sets:

* Inventories *(what players wear)*
* Items for sale *(pricing data is very noisy, as there are no consistency in pricing)*

Heres a breakdown of how items work, as far as stats are concerned:

* There are ~250 stats to chose from. Those stats never change. *(ex: +40% fire resistance)* 
* An item is a combination of 4-6 of those stats.
* Every stat has one category. When an item has one stat, it cannot get another stat within the same category. *(ex: if the item has +40% fire resistance, it cannot have the 2nd stat '+20% fire resistance'. There are ~30 categories)*
* Every stat has a known chance to occur. *(ex: 0.25% chance to get the stat +40% fire resistance)*
* If the item is listed for sale, it *may* have a listed price.

The system isn't much more complicated than that. I simplified a lot the details, but this is what matters. I have 2 questions I wish to answer with machine learning:

* **Predict an item pricing**
* **Try to find how strongly related are stats together (ie: out of the stats that players wear, estimate the demand)**

I tried the Andrew Ng online course, as well as reading a lot of documentation online, the weka tool, and none seem to make it clear what algorithm I should pick. This is what I am planning to use as input neurons:

* Every category (ie: set of stats) is an input neuron, with the currently selected stat within the category being the value. 
* The 'weight' of every stat in the category is simply its probability to occur. 
* The stats within the category are ordered from least probable to most probable (ie: best to worst).
* Proportionally adjust the categories ranges within 0-1.

ex: (one category)

Stat | Range (of fire resistance) | Probability
---|---|----
of the Magma | 42-45 | 0.10%
of the Volcano | 36-41 | 0.25%
of the Furnace | 30-35 | 0.75%
of the Kiln | 24-29 | 1%
of the Drake | 18-23 | 1%
of the Salamander | 12-17 | 1%
of the Whelpling | 6-11 | 1%

So if my item has "of the Magma", for that neuron, I would give it 0.10 value to that input in the NN. If it had "of the Volcano", I would give it 0.35.

Every category is given an input accordingly, with 0 denoting none was chosen.

So with this said, I am not sure which algorithm to pick to feed it the output neuron (pricing?). My data is very noisy for prices. Should I use a classifier with the output neurons being slices of prices (ie: neuron 1 = 0-1$, neuron 2 = 1-5$, neuron 3 = 5-10$, etc.) ? Or should I have only one output neuron? I thought the slices would effectively 'fix' the issue of very variable pricing and at least give a good idea of the price range to expect. And as far as detecting the strength of connections between stats for items that are worn, I do not see what output neuron I could map, so I was wondering if that was even possible. I though I should rank negatively stats that are being removed (ie: player stopped equipping item with stats x,y,z, so input those values but as negatives), but that still leaves me wondering how to extract the correlation between the stats. 

I would welcome any help as such. I don't expect any hand-holding, I just want a nudge in the proper direction! 

Thanks again
comment_no.=000--> Let me preface this by saying that I'm no ML expert, I've only part gone through Hinton's Coursera class.  I do play PoE though, and this is an interesting project.

I would say that classifying the items in price ranges would be easier, then your output could be something like "90% chance of it being in range X-Y, 7% chance of it being in range W-Z and 3% chance of it being in range U-V".  This seems like it would be easier to train than a single output of the expected price.  If you used one output of expected price, you would also need to convert all prices to one currency, whereas with the ranges it isn't the case (you could have outputs of 10-20 chaos and 30-40 exalts no problem - the NN only puts it in a class)

If it were me, my inputs to the NN would be as follows:

* Item Type (Helm/Chest/Amulet/Ring/etc - give this a number so ItemType=1 is for Helms for example)

* Defensive Stat Type (0 for None, 1 for Armour, 2 for ES, 3 for Evasion, 4 for Ar/ES, 5 for Ar/Ev, 6 for Ev/ES, 7 for Ar/Ev/ES - Sacrificial Garbs)

Then the rest of the input nodes would be all possible affixes.  Assuming it's a NN, your outputs would be the ranges of prices.  The price estimation is more straightforward to me, it's just a simple classifier.  The strengths of the connections come through your training cases, you're looking to do supervised learning (I think).

I don't see why you would start by specifying weights.  That comes through learning.  Just have the stat (e.g. Fire Resistance) as an input neuron, and that input value as the stat's value.  I think that's a better system because of hybrid affixes (e.g. Emperor's) don't show up directly - you need to infer their existence.

For flat damage, use the "average" value (e.g. "adds 6 to 15 physical damage" would be "Flat Phys neuron" with value of "10.5").  

For weapons, you would also need to look at the pDPS and DPS, on top of the APS and crit chance.

I think this is a very interesting project though!  You could use it to work out the best master crafting option to maximise the expected sale price.

I've no idea how you would work out the whole "items worn and their progression" for demand estimation.



comment_no.=001--> First, have you tried just taking something like scikit-learn and just fitting a simpler regression model, like linear regression or random forest regression, to the data first?  Often those will get you 90%+ of the accuracy of a more complex model in about 1% of the time, and help you figure out which sets of attributes and outputs will work well.  If you just use the 'raw' price as the output, you can use a regression model, and if you want to lump the prices into ranges, then use a classifier (where each price range is a separate class).

If you're determined to use a neural network, then for managing the output there's two simple ways you can do it.  The easiest is to have a single output neuron for price that uses a rectified linear activation function and squared error loss (in other words, treat it as a regression problem).  This will drive your model to try to estimate the average price across that set of inputs.  The second way to do it is to break the price into ranges and treat it as a classification problem: stick a softmax layer at the end with categorical cross-entropy as the loss function. This will make it estimate the probability that an item will sell in a given range.  You do have to be a bit careful to 1) have a 'bin' for every possible price, and 2) not have too many outputs, since this can make the model harder to fit.

Advantages of the linear output is that you only need one output neuron, it should fit quickly, and it will give you a flexible output.  Disadvantage is that it won't give you much in the way understanding the variance of the price (i.e. the average might be $1, but is it $1 +/- $0.01, or $1 +/- $0.99?)  Advantages of the 'binned' output is that it will give you some measure of uncertainty, so it might tell you that it's 95% sure that it will sell for $1-2, or it might tell you that it's 25% likely to sell for $1-2, 25% for $2-3, etc.  Disadvantages are that it will probably be slower to train, and if you slice the prices too small, then you could get odd results if you have very rare combinations of attributes that sell for unusual prices.

For analyzing the combinations of items, you might be able to use something like t-SNE on your trained network to get some idea of if or how particular combinations of traits cluster, but I'm less familiar with that sort of thing.  It might be easier to fit a separate model, either by filtering to only examine items that were actually purchased and then doing a (non neural network) clustering on those, or maybe through building a model that tries to predict the Nth attribute given the other N-1 on the item.
-------------
Question=000336  Trying to scale output of Mahout logistic regression to p(event), but it's not working. Help?
-------------
Question=000337  Question on Kernels
-------------
Question=000338  How do I handle large CSV datasets?
I'm a beginner messing around with some basic application of ML, applying simple classifiers and stuff, and I stumbled along a CTR challenge Avazu posted a while back on Kaggle: https://www.kaggle.com/c/avazu-ctr-prediction. My problem is that the training set here is a single CSV ~2GB in size, and possibly a few hundred million entries. I tried opening this dataset in the WEKA viewer, and even after increasing the stack size to 4GB, it stopped responding after some time, forcing me to close it. This brings me back to the question, what tools/libraries do you guys use to handle such large datasets?
comment_no.=000--> Let's denote the problem straight: it doesn't matter to you if the dataset size is 4GB/20GB/3TB if you can hold only 1GB dataset. There is no other way to shrink down your dataset other than to take a subset of it. Every line-by-line approach to get this subset would work for you. 

Well, there are some more questions - like how to choose the entries of subset - randomly, proportionally or sets of same size for every factor. That depends on the task, there is plenty of information about subsampling.

If this subset is not enough for you to reach stability in model, there are still options of making a few subset and juggling with them in various ways.

Generally question "How do I just load my enormous dataset straight into my R/WEKA/Excel/sklearn/spintowintoolbox without cleaning it or taking subset of features and observations?" has no answer, you should handle this yourself.
comment_no.=001--> My uninformed opinion:

Read it chunk by chunk and consider using an on-line learner. 
comment_no.=002--> You could simply read it line by line with csv reader, put it in list then convert it to numpy array.
-------------
Question=000339  Help a beginner out?
Hi! I am a college student trying to diversify my resume with some projects that are a little different than a standard CRUD application and have always been interested in ML. I am going through 'Mahout in Action' right now. 

I am just curious if anyone could suggest a decent data clustering project ? I am completely unable to think of anything interest right now :/

Any advice or suggestions will be appreciated. 

Thanks

comment_no.=000--> Anyone here?? 
-------------
Question=000340  Nearest Neighbor Help
-------------
Question=000341  Are there any datasets freely available on which I can test the autocomplete algorithm I have developed

comment_no.=000--> There is /r/datasets -- also ask there.
-------------
Question=000342  I need help with Reinforcement Learning
I'm trying to do an AI with Machine Learning, so I started to learn how to do Machine Learning, but I don't think I understand enough of it to do one. I tried learning by watching some courses, but what they say doesn't seem to be related with what I'm trying to do.
If anyone could help me, I'm trying to do it with PyBrain and Python 3.4 at the moment. I'm currently trying to code the Environment class.
I am doing this just for fun and to learn how it works.
comment_no.=000--> So what *are* you trying to do? Without a concrete question, I doubt anyone can really help you. What are your problems with the Environment class (from [this tutorial](http://pybrain.org/docs/tutorial/reinforcement-learning.html) I assume)?

For a general introduction to RL, Sutton and Barto's famous RL book is freely available [here](http://webdocs.cs.ualberta.ca/~sutton/book/ebook/the-book.html). You might also like [this Udacity course](https://www.udacity.com/course/machine-learning-reinforcement-learning--ud820) by Charles Isbell and Michael Littman, or Olivier Georgeon's [IDEAL MOOC](http://liris.cnrs.fr/ideal/mooc/) although it's a bit more advanced.
-------------
Question=000343  Markov Decision Process in R for a song suggestion software?
Okay, so I'm not exactly sure if this belongs here, but this is my problem:
We have a music player that has different playlists and automatically suggests songs from the current playlist I'm in. What I want the program to learn is, that if I skip the song, it should decrease the probability to be played in this playlist again.
I think this is what's called reinforcement learning and I've read a bit about the algorithms, decidin that MDP seems to be exactly what we have here. I know that in MDP there are more than one state, so I figured for this case it would mean the different playlists.
Like depending on the state (playlist) I'm in, it chooses the songs that it thinks fits the best and get "punished" (by skipping) if it has chosen wrongly.

So what I'm asking is, if you guys think this is the right approach? Or would you suggest a different algorithm? Does all of this even make any sense, should I provide more information?

If it does sound right, I'd like to ask for some tutorials or starting points getting about MDP in R. I've searched online but have only found the MDP Toolbox in R and it kind of doesn't really make sense to me. Do you have any suggestions? I'm really helpful for any kind of advice. :)
comment_no.=000--> MDP researcher here. I would avoid the MDP formulation for this problem since what you would have here is a partially observable (in the state of the person selecting songs) markov decision process, which are *really* difficult. Depending on your requirements, I think this could be a good way to proceed that doesn't take a lot of advanced ML education:

* Confine your worldview to a single playlist. Switching playlists is a separate and likely more complicated problem that you don't need to deal with.
* If you have the resources to build song features that describe the attributes of each song (think of these like Pandora's genres) then you can build a model where an action (punishing) for one song can inform whether you want another song to be played. If you don't have the ability to create song features, then your modeling choices become very simple.
* Assuming you have song features, you should start with a logistic regression. From there, many tweaks and improvements are possible.
-------------
Question=000344  Can I use SKlearn's Naive Bayes to classify tweets?
I have a data set of tweets with keywords relating to vaccine perception.  I want to be able to classify a new tweet as either pro-vaccine, anti-vaccine, or neither.

I hear Naive Bayes is a way to do this, but I can not find any documentation on how  can implement the classifier with words as feautres rather than numbers.

Can anyone provide some insight?
comment_no.=000--> Yes you can, but you have to figure out a way of representing tweets so that the classifier can do its work. [These slides](https://web.stanford.edu/class/cs124/lec/naivebayes.pdf) go over the Bag-of-Words representation and how you use it to fit the multinomial Naive Bayes model. Other groups have done this kind of stuff with so-called word vectors, where individual words are given a representation in an arbitrary vector space learnt from a huge text corpus via neural network models (see [word2vec](https://code.google.com/p/word2vec/)). 
 
If you're using Python, the SKLearn library has automatic functions for this kind of feature extraction, see [here](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html). 
 

 

-------------
Question=000345  Optimization of SVM -- how to interpret model order giving best cross validated results
-------------
Question=000346  Subsequence Identification in Time-Series Data?
-------------
Question=000347  Trouble pre-processing large volumes of image data
This [Kaggle competition](https://www.kaggle.com/c/diabetic-retinopathy-detection/data) is my first time applying my limited understanding of ML. My knowledge is mainly from Andrew Ng's Coursera class.

Any how, I've drafted up a [class](https://gist.github.com/Aweeeezy/164ee8be0464ec79df04) to get started with the preprocessing of these images. The basic function of the code is to create a data abstraction for each image that has attributes like its id and RGB values for each pixel (found using `skimage.io.imread()`).

Now, my computer is pretty old and only has 2GB of ram...I'll be getting a new computer soon with 16GB and nearly double the processing speed, but in the mean time, I'd like to find a way to more efficiently generate, store, and represent this data so my ML scripts can work with it reasonably at a later stage of the project.

The method you see in [the code](https://gist.github.com/Aweeeezy/164ee8be0464ec79df04) bogs up my computer and doesn't finish execution. I thought of throwing a condition statement in the suite of `process_images()` that checks if I'm at, let's just say, a 5% interval of the total data set --> if so, then write the binary of the dictionary, `image_data`, to file and then set it to None so I can free up memory --> continue where I left off, repeatedly appending to the same binary file until I've gone through all 44,000 training examples.

The problem with this is that after testing this out on just 5 images, my binary file is about 161M (32.2M each, which accounting for all 44,000 images scales up to a 1.5TB!).

How the hell can I possibly work with this much data efficiently?! What do ML experts usually do to pre-process high res image data? How do I normalize/standardize the RGB values of each pixel in an image for every image in this huge of a data set?

I know that's more than a couple questions, but I'm not really sure where to go from here. Advice, please and thank you!
comment_no.=000--> I'm sorry to say it more or less boils down to resources.

A dataset that big really requires that faster pc you're talking about.

2GB of RAM just isn't enough to do what you need.

I just took a look at your code and the competition. For a start you want to use [cPickle](https://docs.python.org/2/library/pickle.html#module-cPickle) over pickle. It is many, many, many times more performant than standard pickle.

That alone may solve a lot of your issues.
-------------
Question=000348  Recommended sources for ML-related probabilities
-------------
Question=000349  RNN Library for generating handwriting samples
-------------
Question=000350  Standardization of Input Variables
-------------
Question=000351  Classification algorithms - a discussion
Hi there,

I've been tasked with writing my own implementation of any classification algorithm for an assignment. I was going to pick the c4.5 algorithm but I thought I'd ask you guys what you think (as I thought it'd be interesting to see all the different opinions).

I chose the c4.5 because I am quite familiar with how it works and feel that it would be ok for a beginner to attempt it (I have never tried to write my own implementation of a classification algorithm before). My data deals with a bunch of measurements and I have to decide if the next entry is a certain type of animal. Just in case you were wondering! :)

So if you were a beginner, what algorithm would you choose? Better yet, if you had to decide on an algorithm with your current knowledge, would it be different than if you were just starting out? What pitfalls did you encounter when you were first starting? 

Also, if you could think of any resources that may be beneficial to my *learning* (not direct answers as I'd like to learn from my assignment!).....post away! :P

Sorry if this is the wrong place, I got a notion and I thought it'd be pretty cool to hear form an expert or two!

Thanks!
comment_no.=000--> The first classifier I implemented was logistic regression. Maybe that's too simple for your assignment though.
comment_no.=001--> I think C4.5 will be just fine. I guess you could also start with ID3 and then extend it to learn more about the differences. To prevent overfitting, you may want to look into some pruning algorithms. Then if you want to go further, you could look into Random Forests were really popular a few years ago I think (maybe they still are, but ML isn't my main field and I got the feeling their popularity has decreased now that deep learning works so well). 

In my earlier days I mostly implemented neural networks and genetic algorithms. You could do those too, although genetic algorithms may be slightly harder to apply to your problem. 
-------------
Question=000352  Pre-empting failures with log file based prediction
-------------
Question=000353  RF Model w/ ~15000S x ~150000F barely outperforming univariate model. Suggestions?
-------------
Question=000354  How do I scale non-normal (NB) features?
Hello /r/MLQuestions! Thanks for your time, I'll be brief. I have a labeled dataset 8million x 62. About half are discrete [NGS-read counts](http://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1003494), approximately Negative Binomial or log-concave Poisson according to bioinformatics literature. These counts vary over several orders of magnitude and are not log-normal. I am hoping to use SVM or NN for cross validation.

1. Do I need to scale these data? It is frequently suggested (Ng, Mostafa, etc.) that features should be scaled to the range -1:1 or 0:1 for SVMs. Does this apply for NN as well?

2. If scaling is required, what would you suggest for these count variables? Is it appropriate to use the empirical cumulative distribution function (ecdf) to scale the values to 0:1?
comment_no.=000--> 1. Yes (scale for NN). 
For SVMs, (especially if using a kernel) - you'll want to scale them as well. (Many methods expect the features to have been normalized in fact). 
Easy way to do this is scikit learn - Standard Scaler. 

2. Just normalize and/or scale to values to between 0-1 I think.. 
-------------
Question=000355  help needed for summarization of Amazon.com reviews
x-post from http://www.reddit.com/r/LanguageTechnology/comments/2xqeoq/help_needed_for_summarization_of_amazoncom_reviews/ (please post answers there if you can)

Hi,
A few friends and I have an idea: summarize the reviews of Amazon.com products to show concise pros and cons of the product. This stemmed out of the time I waste reading reviews before buying a product.

We're starting with NLP, and need to know where to start looking to search for answers to (components of) this problem.

We've read "Jointly Learning to Extract and Compress by Taylor Berg-Kirkpatrick, Dan Gillick and Dan Klein", and have a dataset: https://snap.stanford.edu/data/web-Amazon.html

Thanks :)

P.S.: this is my first post on reddit.
comment_no.=000--> Found some useful suggestions on: http://www.quora.com/What-is-the-algorithm-behind-Amazons-Customer-Reviews-summary-feature

Could still use your opinion! :)
-------------
Question=000356  Row vs column vectors
Is there a convention as to whether features are represented as row or column vectors? Or is it a free for all on that front?
comment_no.=000--> From what I've seen (using sklearn, matlab, Caffe...) features are always columns (each sample is row in matrix). However, it is only convention as far as I know.
-------------
Question=000357  AskML: Stacked Denoising Autoencoder - PEBKAC?
-------------
Question=000358  What does the ℝ symbol mean when constructing neural networks?
I'm trying to read up on some papers about neural networks, and without a background in math or computer science, some terms and syntax escape me.

Several papers I have come across express that "we associate a column vector in ℝ^d" or  similar (where d is a dimensionality). But the only meaning of ℝ that I know of is the conventional one. Does it mean that every vector element is a real number, or does it mean something else?
comment_no.=000--> yes, your understanding is correct
comment_no.=001--> R usually denotes Real numbers so you are correct.
-------------
Question=000359  Number of parameters in multi class and two class logistic regression
If you have f number of features, then in 2 class LR you have f parameters (ignoring the bias).  However in multiclass LR with k classes you have f*k parameters, correct?  

Which means that using multiclass (softmax) LR for a 2 class problem would have double the parameters of using 2 class (sigmoid) LR.

What is bothering me is why is it not f*(k-1) parameters for multiclass LR? 
comment_no.=000--> Actually it is f*(k-1)
[wiki](http://en.wikipedia.org/wiki/Multinomial_logistic_regression)
-------------
Question=000360  Propensity scores
-------------
Question=000361  Question about SVM solver algorithms, in matlab specifically (X post from machineLearning post faux-pas)
-------------
Question=000362  Predicting when a resource will run out?
I am trying to figure out what approach would be best for predicting when a resource will run out or be low and needs to be repleneished?

For example, let's say I have a salt shaker at a restaurant and am able to tell the amount of salt left in the shaker (by weight, mass, count, or whatever means is best).  How could I use this data shown over time to predict when it will be empty and will need to be filled back up?

I need to be able to see that the shaker may get used more on weekends, or maybe during summer months, so taking time into account is pretty important.

I have been thinking about using a neural network with cos(time) or sin(time) as a time input variable, but I feel stumped as in my machine learning class two semesters ago we covered identification problems more than these kinds of problems.
comment_no.=000--> Why would you need a neural network for this? This can be easily modeled with a linear equation based on the number of customer, then a Poisson or Gamma distribution can be used to represent the use of the salt shaker.
-------------
Question=000363  Predicting customer purchasing behavior
I work at a dating site and I have a TON of data at my disposal.  We want to be able to predict if a user will purchase or not, and for users that are likely to not purchase, offer them a free trial.

I am pretty well-versed in python (pandas, matplotlib, etc.) and working with large datasets, but not so much with ML.  Where do I start?
comment_no.=000--> Perhaps with [Coursera ML course (link to reddit ML thread)](http://www.reddit.com/r/MachineLearning/comments/2rv8zb/join_the_free_machine_learning_course_on_coursera/)
-------------
Question=000364  Random Forests
I'm a Machine learning newbie. To learn something in ML, I have implemented a random forest (scikit) based classifier for activity prediction based on accelerometer data. i.e The classifier predicts states like standing, running, etc based on tri-axial accelerometer output.

The RF classifier performs reasonably well in predicting state, however, I would like to add something akin to a feedback loop to it.

What I mean is that, if the classifier misclassified and I hand correct it. For the next run, on same dataset, there shouldn't be a misclassification. I'm using scikit.

Any pointers?
comment_no.=000--> With RF you should simply include that sample with correct (by hand) prediction in your training set and then retrain your classifier. 
Alternative is to use some [online learning](http://en.wikipedia.org/wiki/Online_machine_learning) alghoritam.
-------------
Question=000365  Why does UFLDL consider -1 to be an "inactive" tanh hidden neuron output?
From the [Stanford UFLDL tutorial](http://ufldl.stanford.edu/tutorial/unsupervised/Autoencoders/):

> Informally, we will think of a neuron as being “active” (or as “firing”) if its output value is close to 1, or as being “inactive” if its output value is close to 0. We would like to constrain the neurons to be inactive most of the time. This discussion assumes a sigmoid activation function. If you are using a tanh activation function, then we think of a neuron as being inactive when it outputs values close to -1.


Tanh is symmetric about the X-axis.  If you flip the signs of the incident weights then an "inactive" neuron is effectively active again.  This piece of advice s even repeated in the older version of their tutorial.  Is it a mistake or is there a good reason that an output of -1 is somehow less "active" than 1?


comment_no.=000--> Sigmoids were originally devised as continuous versions of binary threshold neurons. Then people started using Tanh because it's faster than sigmoids and has the same shape. It's just a sigmoid with a bias of -0.5, multiplied by 2, basically. So essentially they are still thinking of tanh's as approximations of binary threshold neurons.

However it's not even true for binary neurons. NNs don't care whether a neuron is "active" or "inactive". They can just multiply by a negative weight and adjust the bias, and a zero becomes 1 and a one becomes 0.
-------------
Question=000366  Multiple Kernel Learning - Regression
-------------
Question=000367  Difference between Implementation and Research
-------------
Question=000368  [Baum-Welch algorithm for Hidden Markov Models]
I was going over the Baum-Welch algorithm for updating and ran into a puzzling question about the new values for the symbol distribution -- basically that, unless every observation contains every symbol in the model, it looks like the algorithm will update the missing symbol probabilities to zero for all states. I'm probably just missing something incredibly simple here, but I'd really appreciate it if someone would point it out to me. For the sake of a common notation, [here's](http://en.wikipedia.org/wiki/Baum%E2%80%93Welch_algorithm) the wiki. 

Any updated probability of symbol O in state i [b i(O)] can be expressed as the sum of a disjoint subset of the gammas for that observation and state, so the sum overall the updated values of b i (O) will always be equal to one.

But unless every symbol in B is in every observation the model updates on, at least one of the symbols won't have a gamma that corresponds to it for any state, so it'll get updated to zero across all states (the numerator for b* i (k) is zero).

Assuming that the model updates on a sequence like that (say 'a,b', where each state has a distribution over 'a', 'b', and 'c'), then it can't update on a new sequence like 'b,c' with the discounted symbol in it. It'll also estimate the probability of such a sequence as zero. 

After updating on 'a,b', the probability of seeing c in any state is zero, which makes the the alpha/beta estimates corresponding to it also zero for all states. The denominator for gamma is the sum over states i, of [alpha i (t) * beta i (t)]. All of those are zero, so the divisor is zero.

Just estimating the probability of 'b,c' runs into a similar problem -- the probability of seeing c is zero; which takes the probability of an observation with c in it to zero. 

HMMs don't just work on permutations of their symbol sets, so I'm clearly missing something. Could someone please explain what?
comment_no.=000--> You should have non-zero initial emission probabilities for all symbols. This way, within each observation, the probability of the hidden states for that observation are used *with* the probability of observing that sequence in the first place.

When your pseudo probabilities are later normalized, the unobserved symbols and states will remain non-zero.
-------------
Question=000369  Noob question, how to deal with unequal amount of data for each class, using deep networks
Hi I am trying to do classification using a cnn, but the amount of training data i have varies for each class. What is the correct method in order to deal with this?
comment_no.=000--> I don't think it matters. It could possibly learn incorrect prior probabilities, and possibly affect performance on cases where it is very uncertain. Try weighing the cases of the classes you want more of, more. I.e. putting multiple copies of them in the training set or multiplying the error so those cases matter more.
comment_no.=001--> There is no one easy way to do this and it's in no way specific to CNNs.

First, are you classes imbalanced *only* in your training data with respect to the underlying population?
Or is the underlying distribution imbalanced in the same way?

Having a handle on the class distribution is very helpful here.

If you Google for things like "class imbalance" or "covariate shift" you may get some ideas.

I've tested this briefly: http://blog.smola.org/post/4110255196/real-simple-covariate-shift-correction but it didn't appear to have much effect.
-------------
Question=000370  question regarding design of a cnn
Hi i want to predict emotions in faces using cnn's. I downloaded a canned architecture, rigged it for output as the basic emotions, and ran through my emotion data. However a strange thing is, even when i give it a non-face image it predicts something. This is to be expected as i never told it to expliclty look for a face first, it expects all its inputs to be faces. So, how can i have a system where it predicts emotions only if it is looking at a face?

What I could think of is to add an extra "non face" category to the prediction, and put in non-face data with the labels as this. But this seems a little daunting as I expect I shall have to provide it a lot of non-face images for it to understand to classify only faces. Am I correct in assuming this? Also Can I somehow use the krizhevsky-net as the starting point for this?

Would appreciate your suggestions
-------------
Question=000371  Searching for a suitable distance measure.
-------------
Question=000372  Stochastic variational inference question
-------------
Question=000373  Dropping out for convolutional RBMs and Convolutional Autoencoders
If I implement dropout in convolutional RBMs or Convolutional Autoencoders, do I dropout entire bases (i.e. consider say only 30 out of 60 kernels for a minibatch update), or do I dropout individual hidden units from the kernel activations. (i.e. I use all 60 kernels, but within each kernel's own hidden layer I dropout half of units).
comment_no.=000--> The latter is the more common approach. But, experiment and see what works better for your problem.
-------------
Question=000374  How to get data from a site?
I don't know if this is the right place to post this, so please correct me if I am wrong.
Id like to make a program that can search for recipes by the name of food item. there is a big repository of recipes at http://www.recipesource.com/
I have never worked with getting data from the net, but from what I understand people generally use api provided by the sites?
So how can i search for recipes from this site automatically, Do i need them to provide some "usable API", or can is there some all-purpose program people use to automate such tasks?
comment_no.=000--> when there is no api i normally use C#/F#/Python with their standard http client libs for downloading and some regex to extract urls. you might have to play with some http headers / cookies to get sites to talk to you.

sometimes selenium works well if some website tries to prevent automatic web scraping.. but it's much slower.

comment_no.=001--> Most likely too late for you, but it could be useful to others:

Udacity offers a self-paced course on MongoDB. Lesson 2 is on screen scraping, with example code in python + BeautifulSoup library. There is no MongoDB in this lesson.
-------------
Question=000375  Dealing with imbalanced data set
I'm working on a course project to classify Tweets as either "interesting" or "not interesting". I have hand labeled about 2000 tweets, and ended up with a ratio of about 1:10 (interesting : not interesting).

I downsampled the "not interesting" tweets and got 200/200 distribution. I use k-fold cross validation on this set and get very good performance with a naive bayes classifier. However, I feel it is a terrible waste to discard 1500 of the not-interesting tweets. 

So the question: could I still train my classifier using the imbalanced data set, as long as my validation set is balanced ? Or am I overlooking some issues with this?
comment_no.=000--> There's a lot of literature on imbalanced data sets.

First and foremost, what performance metric are you considering? You'll likely want to be considering Area Under the ROC or precision or recall rather than accuracy (you could have a 90% accurate classifier trivially).

The biggest issue with what you're proposing is that the class balance you're enforcing during training is not likely to persist when applied to a real stream of tweets. I'd be interested to know what happens if you split your data into two even sets with stratified classes (i.e. 100/900 in two sets), train on a 100/100 set and then apply your classifier to the other full 100/900 set.
comment_no.=001--> If you know the operating context (i.e. the class imbalance when you are deploying the model) then you can use ROC analysis to select an optimal classifier on the convex hull. The isometric for accuracy will be a line with a 45 degree angle for balanced classes and will change for unbalanced classes. (Steeper or shallower). The AUC gives a good measure if you do not know the operating context the model would be deployed in, but bare in mind some areas under the curve might not be realistic. In particular if you are doing an information retrieval type task (sounds like it) then you will be more interested in having a steep accent on the left hand side of the ROC curve than the AUC score.

This is a good paper:
http://bib.oxfordjournals.org/content/13/1/83.full.pdf+html
-------------
Question=000376  How can you predict the next frame of video?
Lets say I have a very large series of sequential images. The images are all broadly similar, and sequential images are particularly similar. There exists some inscrutable rule that creates each image from the previous few images.

I want to create a program that can approximate that rule. I want to feed my program the last few sequential images, and have it guess the next.

How would I go about this? As a novice I have been reading around trying to puzzle things out for myself, but I feel I understand less the more I read.
comment_no.=000--> [This talk](http://research.microsoft.com/apps/video/default.aspx?id=180609) has a demonstration of Deep Gaussian Processes doing exactly this.

It's a very hard task though.
-------------
Question=000377  Recommended way to convert a set of probability distributions into a feature vector?
I splitted a number of short texts into its sentences and ran Stanford's CoreNLP sentiment analysis on each one. This discards the words and works with the POS trees only. As a result, for each text now I have a number of probability distributions of being in one out of five possible sentiment classes (from very negative to very positive). Now I want to define the feature vector for the texts.

My first approach was to add five elements to the feature vector for each unique sentence tree in the output of the sentiment classifier, then just populate it with the values of the probability distributions. Does this make sense? Wouldn't I be somehow losing the representativity of the sentiment magnitude?

Thanks!
comment_no.=000--> "I splitted a number of short texts into its sentences and ran Stanford's CoreNLP sentiment analysis on each one. This discards the words and works with the POS trees only. As a result, for each text now I have a number of probability distributions of being in one out of five possible sentiment classes (from very negative to very positive). Now I want to define the feature vector for the texts."

In addition to what you suggest, it might make sense to also include the mean as a feature.  

"My first approach was to add five elements to the feature vector for each unique sentence tree in the output of the sentiment classifier, then just populate it with the values of the probability distributions. Does this make sense? Wouldn't I be somehow losing the representativity of the sentiment magnitude?"

Do you mean representing the probabilities as a vector (i.e. [0.1, 0.3, 0.2, 0.2, 0.2]) fails to take into account the fact that the scores are ordered - that 1 and 2 are more similar than 1 and 5?  

While this is a valid concern, I don't think that it will be a big deal if you have enough data.  It is common to put numerical features into buckets when using linear models.  This has the same issue but it works well in practice.  
-------------
Question=000378  Would anyone be willing to look over my simple NN Octave code?
I'm currently taking the Stanford ML course on Coursera and have been more or less breezing through all the programming assignments. This includes neural networks and the back-propagation algorithm. However, I tried re-creating it on my own from memory and I just can't get it to work right. First I tried in Java, then in Octave, and I'm getting similar results - a bit of learning at first and then it levels off at a low success percentage. Obviously something is wrong (probably in my back-propagation algorithm), but I've been over and over it and can't figure out what. There are no errors when I run the code, but I'm thinking it has to be something obvious that I'm overlooking. Would anyone be willing to take a look at some heavily commented octave code to see if anything looks wrong?

Code is [here](https://gist.github.com/jumpy89/e7d5197cd7c9e39092e7).
comment_no.=000--> If you still need to fix it, have a look at:
http://ufldl.stanford.edu/wiki/index.php/Gradient_checking_and_advanced
_optimization

-------------
Question=000379  Problem: simulate a mechanical device that outputs numbers from 1-100.
So i ve recently gotten into statistical modeling and this problem popped into my head. So here s the rest of the setup:

the device outputs a list like: 5,34,65,88, 07, 32 without replacements and no repetition

So according to my readings this can be thought of as the classical ball and urn model and so the whole theory of hidden markov models can be used to develop a solution to this problem. So what I'd like is reassurance that yes I m on the right track with a solution to this and also what software should i use to actually try and solve this.
comment_no.=000--> When I first read your problem I thought of hidden markov models.  Another approach may be recurrent neural networks.
-------------
Question=000380  Common mistakes for beginners? My ANN is converging poorly.
I'm getting into machine learning, and tried implementing a neural net in Java based off of [this](http://neuralnetworksanddeeplearning.com/chap1) online book. I've tested two different training scenarios - one was simply adding two numbers between 0 and 9 together, and the other was recognizing handwritten digits from the MNIST data set. The problem is convergence is slow and tops out at a fairly low level (~70-80% correct answers for addition and 25-35% for MNIST data). It's obviously working somewhat because I'm getting more correct answers than I would by chance alone, but I feel like I should be getting a lot more. The addition problem is easy and the book shows a python implementation getting over 90% correct after a single training epoch. Varying learning rate and size/number of hidden layers hasn't helped much (also I feel like I need to have a pretty high learning rate for things to get anywhere, like 1-10). I've been over and over my backpropagation algorithm many times and can't find any mistakes. Is there anything obvious I may have missed or should recheck? Any help would be much appreciated.
comment_no.=000--> You may want to try the UFLDL tutorial's technique for checking your feed-forward network code:

http://deeplearning.stanford.edu/wiki/index.php/Gradient_checking_and_advanced_optimization


comment_no.=001--> You should be getting a lot more.

If you are doing the same number of iterations as that online book, you should be getting the same results. For a similar architecture, you can get up to 98.4% accuracy on MNIST (eg http://deeplearning.net/tutorial/mlp.html).

You clearly have a bug. I would write unit tests for the pieces until you find it. Good luck!
-------------
Question=000381  What class or resource do you recommend after Coursera Stanford ML course.
The main question is in the title. For the interest of keeping the post generic it's really all you need read or address. It would help if you said why recommend and what background you recommend having before starting. below the line I give some personal background.

----------------------------------------------
If you read this and want to offer suggestions specific to me that would be great!

I'll be completing stanford's ML learning course through Coursera in the next couple weeks. I'm curious what would be the next logical thing for me to learn to deepen my knowledge of the subject.

Here is a break down of my background:

* Bs in CS by the end of the year
* Proficient in python
* Familiar with ruby, octave, R, python-pandas, python numpy, C, C++
* Math:  linear algebra and Calc 2.

Here is a list of online courses i'm familiar with:

* [Gerogia tech ML through Udacity](https://www.udacity.com/)
* [Caltech ML course](http://work.caltech.edu/telecourse.html)
* [Carnegie Mellon](https://www.cs.cmu.edu/~tom/10701_sp11/hws.shtml)

p.s, I'm sure this question has been addressed somewhere else, feel free to re-direct me. I'll also keep searching. Thanks for the help
comment_no.=000--> The CMU course was my favorite
comment_no.=001--> I think the [Unsupervised Feature Learning and Deep Learning Tutorial](http://ufldl.stanford.edu/wiki/index.php/UFLDL_Tutorial) is a good follow up.
-------------
Question=000382  Study note automation
Hi everybody. Should start out by saying that I am new to ML. I have a strong background in stats and calculus, but none in programming (I'll be taking the coursera class in programming this June).

For a while now, I've had an idea for a program thats been bouncing around in my head. Specifically, I want to create a program that takes text (likely in .pdf or .doc formats) and convert it into a series of questions and answers.

For example, given the following:
>text- The capital of Canada is Ottawa. 
>converted into Q&A format- Q: What is the capital of Canada? A: What capital of Canada is Ottawa.

Any suggestions for starting this would be GREATLY appreciated. I know there are a lot of courses out there, but if someone could narrow down what areas I should focus on, then I could take a targeted approach to learning about what I need to know.

Thanks!

comment_no.=000--> Two quick comments.

First, you would probably want to deal with raw text rather than proprietary formats. There are tools to extract text from doc and pdf files, but they are error prone.

Second, this isn't so much a machine learning task as it is a syntax task. See any good NLP/linguistics/syntax textbook on the topic of Wh- movement, and you'll see that there's most likely a simple rule-based solution to this problem.
-------------
Question=000383  Question about clustering of word vectors (X-post from /r/MachineLearning)
-------------
Question=000384  Difficulty training simple XOR function.
I've created a neural network, with the following structure:

Input1 - Input2 - **Input layer**.

N0 - N1 - **Hidden layer**. 3 Weights per node (one for bias).

N2 - **Output layer**. 3 Weights (one for bias).


I am trying to train it the XOR function with the following test data:

* **0 1** - desired result: **1**
* **1 0** - desired result: **1**
* **0 0** - desired result: **0**
* **1 1** - desired result: **0**

After training, the **mean square error** of test (when looking for a 1 result) {0, 1} = 0, which is good I presume. However the mean square error of test (when looking for a 0 result) {1, 1} = 0.5, which surely needs to be zero?

 During the train stage I notice the MSE of true results drops to zero very quickly, whereas MSE of false results lingers around 0.5.

I'm using back propagation to train the network, with a sigmoid function. The issue is that when I test any combination after the training, I get a ouput result of **1.000014 for true**, and **1.000104 for false**. 

I'm trying to get 1 for true, 0 for false. The network seems to learn very fast, even with an extremely small learning rate.

If it helps, here is the weights that are produced, with a learning rate of **0.1**:

N0-W0 = **-0.999**, N0-W1 = **0.655**, N0-W2 = **0.304** (**bias weight**) - Hidden Layer

N1-W0 = **0.674**, N1-W1 = **-0.893**, N1-W2 = **0.516** (**bias weight**) - Hidden Layer

N2-W0 = **2.135**, N2-W1 = **2.442**, N3-W2 = **1.543** (**bias weight**) - Output node

Back propagation steps:

1. **Feed forward a feature set, summing the weights x input set. Calculating sigmoid per node.**

2. **Apply bias.**

3. **Calculate output node error, desired output - actual output (sigmoid).**

4. **Back propagate error, layer above error x connecting weight. Per node.**

5. **Adjust weights, repeat till MSE low enough.**

 
Apologies for the long post, but I've been scratching my header over this for awhile now, and I can't determine what is wrong with my back propagation algorithm. Thanks in advance.
comment_no.=000--> Can you show me your code?  Its unclear what order you're doing some of the steps in the feed-forward process.  And also you need to backpropogate through the whole network.
comment_no.=001--> The bias is just another input to the neuron.  Don't treat it special (besides always getting an input value = 1).  Also, does your code work for the OR and AND cases?
comment_no.=002--> I just ran XOR on a 2-2-1 NN and got these weights

            h1        h2
      w0    1.4933    7.1091    //bias weight
      w1    2.2253   -4.3301    //weight to input 1
      w2   -0.6427   -5.3821    //weight to input 2

            o1
      w3   -10.3734    //bias weight
      w4    -5.2024    //weight to h1
      w5    10.0029    //weight to h2

Throw these numbers into your code to see if your forward propagation code works.  Results that I get using sigmoid units are:
    
        0.0097    // Result for 0 0
        0.9922    // Result for 0 1
        0.9871    // Result for 1 0
        0.0136    // Result for 1 1
-------------
Question=000385  Neural network learning fast, giving me false positives.
I've recently started implementing a feed-forward neural network and I'm using back-propagation as the learning method. I've been using http://galaxy.agh.edu.pl/~vlsi/AI/backp_t_en/backprop.html as a guide.

However, after just the first epoch, my **error is 0**. Before using the network for my real purpose I've tried with the simple network structure:

* 4 binary inputs, 1, 1, 0, 0.
* 2 hidden layers, 4 neurons each.
* 1 output neuron, 1.0 should = valid input.

Each training epoch runs the test input (1, 1, 0, 0), calculates the output error (sigmoid derivative * (1.0 - sigmoid)), back propagates the error and finally adjusts the weights.

Each neuron's new weight = **weight + learning_rate * the neuron's error * the input to the weight.**

Each hidden neuron's error = **(sum of all output neuron's error * connected weight) * the neuron's sigmoid derivative.**

The issue is that my **learning rate has to be 0.0001** for me to see any sort of 'progress' between the epochs in terms of lowering the error. In this case, the error starts around ~30.0. Any greater learning rate and the error results in 0 after the first pass, and thus results in **false positives**.

Also when I try this network with my real data (a set of 32 audio features from sample - 32 neurons per hidden layer) - I get the same issue. To the point where any noise will trigger a false positive. Possibly this could be an input feature issue, but as I'm testing using a high pitch note I can clearly see the raw data differs from a low pitch one.

I'm a neural networks newbie, so I'm almost positive the issue is with my network. Any help would be greatly appreciated.
comment_no.=000--> I really don't know enough to say for sure but it sounds like you are just overfitting. NNs are very good at that. Try using a smaller network or some other generalization method.
comment_no.=001--> Randomize your initial weights.  I've had good results with random numbers between +/- 0.5

Give training sets of positive, and negative examples e.g. all instances that will produce 1, and all instances that will produce 0.

Also, each layer needs a bias weight with a unity input.


comment_no.=002--> sounds like it's overfitting the training data... have you tried adding a regularization term to your cost function?
-------------
Question=000386  How much data do you need to do ML?
I'm personally most familiar with split testing on websites.

But to really do some ML, what kind of data sets do you need?
comment_no.=000--> The non-answer answer is that it depends. The more complicated your model is the more data you need in order to avoid over-fitting. I've [read](http://www.amazon.com/Learning-From-Data-Yaser-Abu-Mostafa/dp/1600490069/ref=sr_1_1?ie=UTF8&qid=1396934536&sr=8-1&keywords=learning+from+data) that a good rule of thumb for a supervised linear model is ten training examples for every parameter of the model. It is my understanding that linear models are generally considered relatively simple models so that number should probably increase for other sorts of models.
-------------
Question=000387  8-3-8 Neural Network Does Not Converge.
Hello,

I'm trying to recreate the 8-3-8 neural network.  I'm having trouble getting my network to converge.  It'll get close, but there is always one output that won't converge.

For example:

output =

    0.9740    0.0000    0.0147    0.0000    0.0000    0.0628    0.0002    0.0002
    0.0000    0.9755    0.0041    0.0000    0.0000    0.0681    0.0000    0.0000
    0.0141    0.0002    0.9756    0.0000    0.0008    0.0630    0.0000    0.0000
    0.0000    0.0001    0.0000    0.9685    0.0000    0.0889    0.0000    0.0000
    0.0000    0.0000    0.0018    0.0001    0.9798    0.0563    0.0000    0.0000
    0.0568    0.0736    0.0535    0.0906    0.0617    0.3661    0.0768    0.0673
    0.0019    0.0000    0.0000    0.0000    0.0000    0.0780    0.9745    0.0000
    0.0080    0.0000    0.0000    0.0000    0.0000    0.0648    0.0000    0.9789

I'm using sigmoid threshold units, and I have one input bias node.  I run 100,000 epochs, and I always get one or two outputs that will not converge.

[Here is the Matlab code](http://pastebin.com/jaLGTnyx)

Any advice to troubleshoot a NN would be helpful.

Thanks,
Max

edit: I found my mistake.  Every layer needs a bias unit.
[
updated Matlab code](http://pastebin.com/z1WXkssp)

    0.9813    0.0148    0.0006    0.0000    0.0101    0.0000    0.0145    0.0000
    0.0117    0.9762    0.0029    0.0077    0.0000    0.0000    0.0000    0.0013
    0.0024    0.0151    0.9758    0.0000    0.0000    0.0000    0.0142    0.0140
    0.0000    0.0168    0.0000    0.9805    0.0103    0.0002    0.0000    0.0110
    0.0122    0.0000    0.0000    0.0160    0.9834    0.0083    0.0031    0.0000
    0.0000    0.0000    0.0001    0.0012    0.0069    0.9739    0.0191    0.0180
    0.0127    0.0000    0.0220    0.0000    0.0039    0.0172    0.9712    0.0000
    0.0000    0.0066    0.0165    0.0133    0.0000    0.0159    0.0000    0.9770
comment_no.=000--> I answered my own question :)
-------------
Question=000388  How could I beat 2048 with ML? (x-post: /r/MachineLearning)
x-post from: http://www.reddit.com/r/MachineLearning/comments/20fmym/how_could_i_beat_2048_with_ml/

There is already an AI for playing the game 2048 here. It uses minimax, but I was wondering about the possibility of using machine learning to make an AI. How would I set up a neural network or something similar to play the game?

I am not very experienced with ML, so I'm looking for advice on how to set up neural network (nodes per layer) as well as what algorithms to use for adjusting weights. Or maybe neural networks aren't the best approach?
comment_no.=000--> I'm currently working on doing exactly this. Did anything come of your project?
comment_no.=001--> I tried an approach with reinforcement learning using a genetic algorithm to optimize a decision tree forest (https://github.com/Underflow/reinforcement-2048).

I'm really not conviced of my method, and it doesn't do better than 1024.
-------------
Question=000389  Is there any way to find the 'real/effective bandwidth' of an audio file?
I have some previously recorded audio that I'm using for some ML projects.    The audio is currently encoded with a certain compression/codec.  But, before that, it may have gone through a number of other codecs/compression algorithms.  For instance, maybe GSM for a cell phone call, then ulaw, then g729. etc.   

Is there a way to calculate the how much useful information is still contained in the audio, as compared to a reference, (like say pcm16).  I don't have access to the original non-compressed files of course.

Seems like there should be some information theoretic approach that would work....

comment_no.=000--> I've never worked with this, but my understanding is that useful information is synonymous with Shannon Entropy.

http://en.wikipedia.org/wiki/Shannon_entropy#Definition

If I were doing this, I would start with the negative sum over i of P(xi) * log_b[ P(xi) ] expression.

Here are a couple of approaches which do this:

http://www.kennethghartman.com/calculate-file-entropy/

http://www.kennethghartman.com/shannon-entropy-of-file-formats/

and:

http://stackoverflow.com/questions/990477/how-to-calculate-the-entropy-of-a-file
-------------
Question=000390  Pattern Recognition: Have method, need name.
This seems like it might be the best place to post this. I don't know any of the pattern recognition lingo in any real depth, but I need to know if there is a name for the algorithm in a program I've already finished. If there isn't, then I need to know the closest thing to compare it to. 

I have a simple signal of one variable vs. another. I fit a number of polynomials at various places in the signal to extract some points, and simplify the signal by some fundamental simple shapes which pass through these points. This seems to be something like a Hough Transform.

I then calculate a few hundred attributes, based on these shapes for the set of all signals. For instance length of a side or angle between two sides and so on. I have categorized by eye a subset of these signals into two categories, good or bad. I use this training set to find upper and lower limits acceptable for these attributes, and apply a boolean test for all the attributes of all the signals using the test set limits.

In other words finding the box of dimension (# of attributes) that the training set lives in and reporting which events of the total set are within it. This defines the output set of good signals for which there are no false negatives, and very very few false positives (with these signals anyhow). This seems to be something like Linear Discriminant Analysis with hard limits instead of dealing with probabilities. 

I then do various operations to find the smallest set of these cuts which return the exact same output. This seems to be like Feature Selection.

What do you think? Is there possibly a name for this exact thing overall? Have I gotten close with the sub-method names?
comment_no.=000--> Your description is a bit too vague. You say that 

> I have a simple signal of one variable vs. another.

Which makes it sound like you have 2 variables in total (i.e, you could plot all of your dataset in a 2D plane) but the rest of your text absolutely doesn't sound like that. I haven't got the foggiest what you mean by "fitting polynomials at various places in the signal", so I'm just going to ignore that part and assume  that you are dealing with a set of geometric shapes, because that's what it sounds like.

Then, you obviously totally confuse training and testset (maybe just a typo), because apparently you find some thresholds for some attributes on a trianingset, and then apply a boolean test "using the *test set* limits"???


> This defines the output set of good signals for which there are no false negatives, and very very few false positives (with these signals anyhow).

I don't know why you think there are no false negatives in that set


> This seems to be something like Linear Discriminant Analysis with hard limits instead of dealing with probabilities. 

absolutely not. First of, LDA isn't dealing with probabilities either, but more importantly: what you're doing sounds nothing like LDA. 


From what I've understood of your method, it is probably most similar to a decision tree.



-------------
Question=000391  Pros and cons of user-based vs item-based collaborative filtering
So let's say you have a bunch of nondescript users and a bunch of nondescript items. You want to perform collaborative filtering in order to recommend user A a new item. No metadata is used in any case.

I'm trying to better understand when you'd do user-based vs item-based collaborative filtering. User-based collaborative filtering makes sense to me - given a user, you match him/her up with all other users and come up with a similarity-weighted average rating of all songs, at which point you can find the most highly-rated thing that the user hasn't seen yet.

What are the pros and cons of using one vs the other? If you construct the matrix of ratings, with let's say users as columns and items as rows, both end up being extremely similar - in user-based CF, you look at correlations between user columns (using cosine similarity or something similar), and in item-based CF, you look at correlations between item rows (using the same thing). What applications make sense for using one vs the other?
comment_no.=000--> It seems to me that user-based or item-based are just inverses of each other.

In terms of the approach that makes the most sense to me -- clustering -- you would have two possibilities.  Given *m* users and *n* items:

* Users are a point in *n*-dimensional item space, where each of the *n* dimensions is that user's rating for the *n*th item

* Items are a point in *m*-dimensional user space, where each of the *m* dimensions is that item's rating given by the *m*th user

The former lets you find users who are similar to each other based upon their item ratings, allowing you to make predictions for the unrated items by comparing this user to their most similar users and how they rated that item, and making the assumption that similar users will continue to make similar ratings.  i.e. "Here are the users most like you", or going one-level deeper, "Users most similar to you prefer these items", or for the NSA: "person A is a terrorist; he is most similar to these people in his buying or rating patterns, so check them out as persons of interest".

The latter lets you easily find items who are similar to each other based upon their ratings by users, allowing you to make predictions for the users who haven't rated that item by comparing this item to the most similar items and how the user rated that item (which is just the inverse of the previous paragraph), and making the assumption that similar items would be ranked similarly by similar users.  i.e. "Here are the items most similar to this item", or going one-level deeper, "items similar to this were bought by these users", or for narcotics police: "item A is discovered to be a mind-altering substance; these items are most similar based on user buying patterns, so investigate them as potential mind-altering substances as well", or perhaps "item A is a mind-altering substance; items similar to this were bought by these users, so check them out as potential drug users".

**It's just two sides of the same coin.**

EDIT: removed Amazon examples -- they weren't working out for me.  :-P
-------------
Question=000392  nonparametric sequential hypothesis testing
-------------
Question=000393  Are Markov-chain models of language used for anything productive?
I've encountered Markov models of language before, and they are very entertaining because they can be used to generate sequences of realistic sounding nonsense, but do they have any other uses?
Are these models ever employed in machine translation or speech recognition, or have they been superseded by something better?

Also, I recall there being some kind of markov chain tool in Ubuntu but I can't remember the name. If you know it, please share :)
comment_no.=000--> Yes, those language generation tools essentially sample from an [n-gram markov model](http://en.wikipedia.org/wiki/N-gram).
n-grams are used to model the prior probability of a target sentence in speech recognition, handwriting recognition, machine translation, etc.
Often some kind of discounting is used to assign some probability to n-grams that have not been observed during training.

There are of course alternative approaches to language modelling, mainly neural networks. Especially recurrent neural networks should be suited for this.
comment_no.=001--> They are used in word prediction. For example, autocorrect on mobile keyboards. There was a really cool keyboard app I saw awhile ago that could fit in a very small space and relied on predicting what you meant to type. Google search uses them to make suggestions and detect misspellings. I *think* they might use something like them in Google translate but I'm not certain.

I don't know if they are used in speech recognition but it's certainly a good application. E.g. is it more likely to user said "I recognize *beach*" or "I recognize *speech*." It's also theoretically possible to use them to get good text compression, but I don't know if anyone actually does that.
-------------
Question=000394  SVR advice wanted.
I am trying my hand at a [Hackerrank](https://www.hackerrank.com/challenges/predicting-house-prices "Challenge link") challenge. After determining after a couple of days that writing my own linear regression in C was going to be rife with trouble I decided I would try to use an SVR from libSVM.  

After playing with SVM toy for a couple of minutes I felt like I had a few configurations worth giving a shot. But when I try them on the test data the resulting plane seems to be fairly horizontal and near the average of all the training values. 

For more specifics I am using the libSVM nu-SVR and experimenting with both the various kernels and tweaking gamma. Pointers and links to explanations to help me better understand what I'm missing would be most appreciated. Thanks.
comment_no.=000--> What parameters for the nu-SVR are you using?  Its possible you're regularizing too much.  Have you gotten r^2 or mean absolute errors of your cross-validation outputs?  

EDIT: it would also be worthwhile just to find an implementation of OLS and put your data through that to see what the results look like.
comment_no.=001--> Just replying to let any passers by know that It was a piece of cake to solve this problem with Scikit-learn. The hardest part was using Python with nearly no previous experience. If Scikit-learn continues to be this straightforward I look forward to being very grateful for the recommendation. For now consider this problem solved.

-------------
Question=000395  Why is machine learning important and how is it applied in business?
Background: I am an business analyst. My analysis to this point has been mainly restricted to excel and access, with some use of business objects and SQL. In my personal life, I am learning more about computer science and practicing python. My degree was in Anthropology and Poli Sci.

Complete ML noob.
comment_no.=000--> It becomes more difficult to see trends, and to make intelligent decisions from data, when the amount of data increases beyond what a person or team of people can handle. In order to make the most of a large amount of data, it is necessary to automate the processes we use to analyze, and make decisions from it.


comment_no.=001--> I'm not sure what sector of business you're in, but I know finance uses a lot of machine learning.

Consider the idea of stock prediction.  Say you want to predict the stock price at the end of the day.  What types of information may be useful?  The price at the previous day would be useful, the price the day before would be as well.  Stocks in similar industries may also be useful.  You can then use these pieces of information together to build a predictive model of what the stock price at the end of the day will be. 

In general more information is used as well and I can get you some links for that if you would be interested.
-------------
