how do i scale non-normal ( nb ) features ? hello /r/mlquestions ! thanks for your time , i 'll be brief . i have a labeled dataset 8million x 62. about half are discrete [ ngs-read counts ] ( http : //journals.plos.org/ploscompbiol/article ? id=10.1371/journal.pcbi.1003494 ) , approximately negative binomial or log-concave poisson according to bioinformatics literature . these counts vary over several orders of magnitude and are not log-normal . i am hoping to use svm or nn for cross validation . 1. do i need to scale these data ? it is frequently suggested ( ng , mostafa , etc . ) that features should be scaled to the range -1:1 or 0:1 for svms . does this apply for nn as well ? 2. if scaling is required , what would you suggest for these count variables ? is it appropriate to use the empirical cumulative distribution function ( ecdf ) to scale the values to 0:1 ? <eoq> 1. yes ( scale for nn ) . for svms , ( especially if using a kernel ) - you 'll want to scale them as well . ( many methods expect the features to have been normalized in fact ) . easy way to do this is scikit learn - standard scaler . 2. just normalize and/or scale to values to between 0-1 i think.. <eoa> 
help needed for summarization of amazon.com reviews x-post from http : //www.reddit.com/r/languagetechnology/comments/2xqeoq/help_needed_for_summarization_of_amazoncom_reviews/ ( please post answers there if you can ) hi , a few friends and i have an idea : summarize the reviews of amazon.com products to show concise pros and cons of the product . this stemmed out of the time i waste reading reviews before buying a product . we 're starting with nlp , and need to know where to start looking to search for answers to ( components of ) this problem . we 've read `` jointly learning to extract and compress by taylor berg-kirkpatrick , dan gillick and dan klein '' , and have a dataset : https : //snap.stanford.edu/data/web-amazon.html thanks : ) p.s . : this is my first post on reddit . <eoq> found some useful suggestions on : http : //www.quora.com/what-is-the-algorithm-behind-amazons-customer-reviews-summary-feature could still use your opinion ! : ) <eoa> 
row vs column vectors is there a convention as to whether features are represented as row or column vectors ? or is it a free for all on that front ? <eoq> from what i 've seen ( using sklearn , matlab , caffe ... ) features are always columns ( each sample is row in matrix ) . however , it is only convention as far as i know . <eoa> 
what does the ℝ symbol mean when constructing neural networks ? i 'm trying to read up on some papers about neural networks , and without a background in math or computer science , some terms and syntax escape me . several papers i have come across express that `` we associate a column vector in ℝ^d '' or similar ( where d is a dimensionality ) . but the only meaning of ℝ that i know of is the conventional one . does it mean that every vector element is a real number , or does it mean something else ? <eoq> yes , your understanding is correct <eoa> 
what does the ℝ symbol mean when constructing neural networks ? i 'm trying to read up on some papers about neural networks , and without a background in math or computer science , some terms and syntax escape me . several papers i have come across express that `` we associate a column vector in ℝ^d '' or similar ( where d is a dimensionality ) . but the only meaning of ℝ that i know of is the conventional one . does it mean that every vector element is a real number , or does it mean something else ? <eoq> r usually denotes real numbers so you are correct . <eoa> 
number of parameters in multi class and two class logistic regression if you have f number of features , then in 2 class lr you have f parameters ( ignoring the bias ) . however in multiclass lr with k classes you have f*k parameters , correct ? which means that using multiclass ( softmax ) lr for a 2 class problem would have double the parameters of using 2 class ( sigmoid ) lr . what is bothering me is why is it not f* ( k-1 ) parameters for multiclass lr ? <eoq> actually it is f* ( k-1 ) [ wiki ] ( http : //en.wikipedia.org/wiki/multinomial_logistic_regression ) <eoa> 
predicting when a resource will run out ? i am trying to figure out what approach would be best for predicting when a resource will run out or be low and needs to be repleneished ? for example , let 's say i have a salt shaker at a restaurant and am able to tell the amount of salt left in the shaker ( by weight , mass , count , or whatever means is best ) . how could i use this data shown over time to predict when it will be empty and will need to be filled back up ? i need to be able to see that the shaker may get used more on weekends , or maybe during summer months , so taking time into account is pretty important . i have been thinking about using a neural network with cos ( time ) or sin ( time ) as a time input variable , but i feel stumped as in my machine learning class two semesters ago we covered identification problems more than these kinds of problems . <eoq> why would you need a neural network for this ? this can be easily modeled with a linear equation based on the number of customer , then a poisson or gamma distribution can be used to represent the use of the salt shaker . <eoa> 
predicting customer purchasing behavior i work at a dating site and i have a ton of data at my disposal . we want to be able to predict if a user will purchase or not , and for users that are likely to not purchase , offer them a free trial . i am pretty well-versed in python ( pandas , matplotlib , etc . ) and working with large datasets , but not so much with ml . where do i start ? <eoq> perhaps with [ coursera ml course ( link to reddit ml thread ) ] ( http : //www.reddit.com/r/machinelearning/comments/2rv8zb/join_the_free_machine_learning_course_on_coursera/ ) <eoa> 
random forests i 'm a machine learning newbie . to learn something in ml , i have implemented a random forest ( scikit ) based classifier for activity prediction based on accelerometer data . i.e the classifier predicts states like standing , running , etc based on tri-axial accelerometer output . the rf classifier performs reasonably well in predicting state , however , i would like to add something akin to a feedback loop to it . what i mean is that , if the classifier misclassified and i hand correct it . for the next run , on same dataset , there should n't be a misclassification . i 'm using scikit . any pointers ? <eoq> with rf you should simply include that sample with correct ( by hand ) prediction in your training set and then retrain your classifier . alternative is to use some [ online learning ] ( http : //en.wikipedia.org/wiki/online_machine_learning ) alghoritam . <eoa> 
why does ufldl consider -1 to be an `` inactive '' tanh hidden neuron output ? from the [ stanford ufldl tutorial ] ( http : //ufldl.stanford.edu/tutorial/unsupervised/autoencoders/ ) : > informally , we will think of a neuron as being “active” ( or as “firing” ) if its output value is close to 1 , or as being “inactive” if its output value is close to 0. we would like to constrain the neurons to be inactive most of the time . this discussion assumes a sigmoid activation function . if you are using a tanh activation function , then we think of a neuron as being inactive when it outputs values close to -1 . tanh is symmetric about the x-axis . if you flip the signs of the incident weights then an `` inactive '' neuron is effectively active again . this piece of advice s even repeated in the older version of their tutorial . is it a mistake or is there a good reason that an output of -1 is somehow less `` active '' than 1 ? <eoq> sigmoids were originally devised as continuous versions of binary threshold neurons . then people started using tanh because it 's faster than sigmoids and has the same shape . it 's just a sigmoid with a bias of -0.5 , multiplied by 2 , basically . so essentially they are still thinking of tanh 's as approximations of binary threshold neurons . however it 's not even true for binary neurons . nns do n't care whether a neuron is `` active '' or `` inactive '' . they can just multiply by a negative weight and adjust the bias , and a zero becomes 1 and a one becomes 0 . <eoa> 
[ baum-welch algorithm for hidden markov models ] i was going over the baum-welch algorithm for updating and ran into a puzzling question about the new values for the symbol distribution -- basically that , unless every observation contains every symbol in the model , it looks like the algorithm will update the missing symbol probabilities to zero for all states . i 'm probably just missing something incredibly simple here , but i 'd really appreciate it if someone would point it out to me . for the sake of a common notation , [ here 's ] ( http : //en.wikipedia.org/wiki/baum % e2 % 80 % 93welch_algorithm ) the wiki . any updated probability of symbol o in state i [ b i ( o ) ] can be expressed as the sum of a disjoint subset of the gammas for that observation and state , so the sum overall the updated values of b i ( o ) will always be equal to one . but unless every symbol in b is in every observation the model updates on , at least one of the symbols wo n't have a gamma that corresponds to it for any state , so it 'll get updated to zero across all states ( the numerator for b* i ( k ) is zero ) . assuming that the model updates on a sequence like that ( say 'a , b ' , where each state has a distribution over 'a ' , 'b ' , and 'c ' ) , then it ca n't update on a new sequence like 'b , c ' with the discounted symbol in it . it 'll also estimate the probability of such a sequence as zero . after updating on 'a , b ' , the probability of seeing c in any state is zero , which makes the the alpha/beta estimates corresponding to it also zero for all states . the denominator for gamma is the sum over states i , of [ alpha i ( t ) * beta i ( t ) ] . all of those are zero , so the divisor is zero . just estimating the probability of 'b , c ' runs into a similar problem -- the probability of seeing c is zero ; which takes the probability of an observation with c in it to zero . hmms do n't just work on permutations of their symbol sets , so i 'm clearly missing something . could someone please explain what ? <eoq> you should have non-zero initial emission probabilities for all symbols . this way , within each observation , the probability of the hidden states for that observation are used *with* the probability of observing that sequence in the first place . when your pseudo probabilities are later normalized , the unobserved symbols and states will remain non-zero . <eoa> 
noob question , how to deal with unequal amount of data for each class , using deep networks hi i am trying to do classification using a cnn , but the amount of training data i have varies for each class . what is the correct method in order to deal with this ? <eoq> i do n't think it matters . it could possibly learn incorrect prior probabilities , and possibly affect performance on cases where it is very uncertain . try weighing the cases of the classes you want more of , more . i.e . putting multiple copies of them in the training set or multiplying the error so those cases matter more . <eoa> 
noob question , how to deal with unequal amount of data for each class , using deep networks hi i am trying to do classification using a cnn , but the amount of training data i have varies for each class . what is the correct method in order to deal with this ? <eoq> there is no one easy way to do this and it 's in no way specific to cnns . first , are you classes imbalanced *only* in your training data with respect to the underlying population ? or is the underlying distribution imbalanced in the same way ? having a handle on the class distribution is very helpful here . if you google for things like `` class imbalance '' or `` covariate shift '' you may get some ideas . i 've tested this briefly : http : //blog.smola.org/post/4110255196/real-simple-covariate-shift-correction but it did n't appear to have much effect . <eoa> 
dropping out for convolutional rbms and convolutional autoencoders if i implement dropout in convolutional rbms or convolutional autoencoders , do i dropout entire bases ( i.e . consider say only 30 out of 60 kernels for a minibatch update ) , or do i dropout individual hidden units from the kernel activations . ( i.e . i use all 60 kernels , but within each kernel 's own hidden layer i dropout half of units ) . <eoq> the latter is the more common approach . but , experiment and see what works better for your problem . <eoa> 
how to get data from a site ? i do n't know if this is the right place to post this , so please correct me if i am wrong . id like to make a program that can search for recipes by the name of food item . there is a big repository of recipes at http : //www.recipesource.com/ i have never worked with getting data from the net , but from what i understand people generally use api provided by the sites ? so how can i search for recipes from this site automatically , do i need them to provide some `` usable api '' , or can is there some all-purpose program people use to automate such tasks ? <eoq> when there is no api i normally use c # /f # /python with their standard http client libs for downloading and some regex to extract urls . you might have to play with some http headers / cookies to get sites to talk to you . <eoa> 
how to get data from a site ? i do n't know if this is the right place to post this , so please correct me if i am wrong . id like to make a program that can search for recipes by the name of food item . there is a big repository of recipes at http : //www.recipesource.com/ i have never worked with getting data from the net , but from what i understand people generally use api provided by the sites ? so how can i search for recipes from this site automatically , do i need them to provide some `` usable api '' , or can is there some all-purpose program people use to automate such tasks ? <eoq> most likely too late for you , but it could be useful to others : udacity offers a self-paced course on mongodb . lesson 2 is on screen scraping , with example code in python + beautifulsoup library . there is no mongodb in this lesson . <eoa> 
dealing with imbalanced data set i 'm working on a course project to classify tweets as either `` interesting '' or `` not interesting '' . i have hand labeled about 2000 tweets , and ended up with a ratio of about 1:10 ( interesting : not interesting ) . i downsampled the `` not interesting '' tweets and got 200/200 distribution . i use k-fold cross validation on this set and get very good performance with a naive bayes classifier . however , i feel it is a terrible waste to discard 1500 of the not-interesting tweets . so the question : could i still train my classifier using the imbalanced data set , as long as my validation set is balanced ? or am i overlooking some issues with this ? <eoq> there 's a lot of literature on imbalanced data sets . <eoa> 
dealing with imbalanced data set i 'm working on a course project to classify tweets as either `` interesting '' or `` not interesting '' . i have hand labeled about 2000 tweets , and ended up with a ratio of about 1:10 ( interesting : not interesting ) . i downsampled the `` not interesting '' tweets and got 200/200 distribution . i use k-fold cross validation on this set and get very good performance with a naive bayes classifier . however , i feel it is a terrible waste to discard 1500 of the not-interesting tweets . so the question : could i still train my classifier using the imbalanced data set , as long as my validation set is balanced ? or am i overlooking some issues with this ? <eoq> if you know the operating context ( i.e . the class imbalance when you are deploying the model ) then you can use roc analysis to select an optimal classifier on the convex hull . the isometric for accuracy will be a line with a 45 degree angle for balanced classes and will change for unbalanced classes . ( steeper or shallower ) . the auc gives a good measure if you do not know the operating context the model would be deployed in , but bare in mind some areas under the curve might not be realistic . in particular if you are doing an information retrieval type task ( sounds like it ) then you will be more interested in having a steep accent on the left hand side of the roc curve than the auc score . this is a good paper : http : //bib.oxfordjournals.org/content/13/1/83.full.pdf+html <eoa> 
how can you predict the next frame of video ? lets say i have a very large series of sequential images . the images are all broadly similar , and sequential images are particularly similar . there exists some inscrutable rule that creates each image from the previous few images . i want to create a program that can approximate that rule . i want to feed my program the last few sequential images , and have it guess the next . how would i go about this ? as a novice i have been reading around trying to puzzle things out for myself , but i feel i understand less the more i read . <eoq> [ this talk ] ( http : //research.microsoft.com/apps/video/default.aspx ? id=180609 ) has a demonstration of deep gaussian processes doing exactly this . it 's a very hard task though . <eoa> 
recommended way to convert a set of probability distributions into a feature vector ? i splitted a number of short texts into its sentences and ran stanford 's corenlp sentiment analysis on each one . this discards the words and works with the pos trees only . as a result , for each text now i have a number of probability distributions of being in one out of five possible sentiment classes ( from very negative to very positive ) . now i want to define the feature vector for the texts . my first approach was to add five elements to the feature vector for each unique sentence tree in the output of the sentiment classifier , then just populate it with the values of the probability distributions . does this make sense ? would n't i be somehow losing the representativity of the sentiment magnitude ? thanks ! <eoq> `` i splitted a number of short texts into its sentences and ran stanford 's corenlp sentiment analysis on each one . this discards the words and works with the pos trees only . as a result , for each text now i have a number of probability distributions of being in one out of five possible sentiment classes ( from very negative to very positive ) . now i want to define the feature vector for the texts . '' in addition to what you suggest , it might make sense to also include the mean as a feature . `` my first approach was to add five elements to the feature vector for each unique sentence tree in the output of the sentiment classifier , then just populate it with the values of the probability distributions . does this make sense ? would n't i be somehow losing the representativity of the sentiment magnitude ? '' do you mean representing the probabilities as a vector ( i.e . [ 0.1 , 0.3 , 0.2 , 0.2 , 0.2 ] ) fails to take into account the fact that the scores are ordered - that 1 and 2 are more similar than 1 and 5 ? while this is a valid concern , i do n't think that it will be a big deal if you have enough data . it is common to put numerical features into buckets when using linear models . this has the same issue but it works well in practice . <eoa> 
