can this problem be solved using ml ? i do <unk> analytics and reporting at a company that <unk> a wide range of products . we record sales transactions , and <unk> a item is sold , the following is recorded : * customer id ( each customer has a unique id ) * product id ( each product has a unique id ) * sale date ( other fields are recorded too - location of purchase , <unk> , <unk> type , etc . ) we sell a few big ticket items , and what i 'm wondering is if it 's possible to predict whether a customer will buy one of the big ticket items based on their purchase history , using <unk> data as described above . we have about 2 million rows of sales data <unk> <unk> years , and in that time maybe <unk> big ticket items have been sold to 5,000 out of <unk> <unk> . is this a problem that ml <unk> to ? i realize ml is n't something you learn <unk> but i would love to know more about how something like this can be <unk> . <eoq> you might want to look into <unk> analysis : https : <unk> you might also look at `` <unk> systems '' . as an example , if most people who buy <unk> also buy <unk> , you might see someone buying <unk> who has n't yet bought a <unk> . in that case , they might be more likely to buy a <unk> , especially if you recommend it to them . something like : http : <unk> <eoa> 
started to implement svm from scratch just to see if i can ... turns out it does not go so well , could anyone care to take a look ? i 'm following <unk> and decided to implement support vector machines from scratch in python . turns out , it does not work so well - i made up sets that are clearly separable , yet my algorithm has a lot of trouble finding accurate line . am i doing something wrong or my failure was <unk> since algorithm itself requires much more <unk> math to even work correctly ? i <unk> source code to github , please forgive <unk> variable names here and there . https : <unk> <eoq> i just <unk> at it and <unk> you were not explicitly plotting the support vectors . that should be the first thing you try . <eoa> 
started to implement svm from scratch just to see if i can ... turns out it does not go so well , could anyone care to take a look ? i 'm following <unk> and decided to implement support vector machines from scratch in python . turns out , it does not work so well - i made up sets that are clearly separable , yet my algorithm has a lot of trouble finding accurate line . am i doing something wrong or my failure was <unk> since algorithm itself requires much more <unk> math to even work correctly ? i <unk> source code to github , please forgive <unk> variable names here and there . https : <unk> <eoq> make sure the objective you are <unk> decreases . <eoa> 
what is the most popular file type for datasets ? arff ? why they do n't use database ? as my question , i doubt that what is popular file type for datasets ? arff ? <eoq> i 've been wondering this , too ? anyone know the fastest file format for reading from <unk> into memory ? <eoa> 
what is the most popular file type for datasets ? arff ? why they do n't use database ? as my question , i doubt that what is popular file type for datasets ? arff ? <eoq> should i ask this on machine learning subreddit ? <eoa> 
transition from econometrics to machine learning ? hello everyone i 'm currently an economics major and would like to know if there is a way to transition from economics , particularly econometrics to a <unk> in machine learning . i 've found out that i do n't like <unk> theory and want to look for an alternative without changing <unk> . the school im in has <unk> of being very good in econometrics , compared to the other <unk> in my <unk> , but i have n't <unk> those classes yet so i do n't really know what are they about . thank you for your answers in advance ( and sorry for bad english ) <eoq> machine learning is a <unk> of math and programming , so if you have n't started already , i would recommend starting to learn r. it 's a statistical language and an easier transition from other math languages like matlab . we teach it at our [ bootcamp ] ( http : <unk> ) because it is fast to learn . this book is also recommended : [ r for everyone ] ( http : <unk> ) <eoa> 
how is weight initialization done today ? i 've just read [ understanding the difficulty of training deep feedforward neural networks ] ( http : <unk> ) . it seems to me that no regularization is used . today , now that we have [ batch normalization ] ( http : //arxiv.org/abs/1502.03167 ) , [ dropout ] ( https : <unk> ) and [ deep networks with stochastic depth ] ( https : <unk> ) as well as typically relu activation functions , i wonder how weights are <unk> . was there a <unk> paper which checked the importance of initialization / how much of a difference the initialization still makes ? what are common ways to initialize weights in <unk> ? for example , is rbm initialization still done ? ( why / why not ? ) <eoq> yes , there was indeed a follow up paper <unk> initialization when using relu <unk> : [ <unk> deep into <unk> : <unk> <unk> performance on imagenet classification ] ( https : <unk> ) [ he ] . <eoa> 
how is weight initialization done today ? i 've just read [ understanding the difficulty of training deep feedforward neural networks ] ( http : <unk> ) . it seems to me that no regularization is used . today , now that we have [ batch normalization ] ( http : //arxiv.org/abs/1502.03167 ) , [ dropout ] ( https : <unk> ) and [ deep networks with stochastic depth ] ( https : <unk> ) as well as typically relu activation functions , i wonder how weights are <unk> . was there a <unk> paper which checked the importance of initialization / how much of a difference the initialization still makes ? what are common ways to initialize weights in <unk> ? for example , is rbm initialization still done ? ( why / why not ? ) <eoq> ( more questions to this paper are in [ my <unk> ] ( http : <unk> ? <unk> # <unk> ) ) <eoa> 
basic python machine learning - why use <unk> and <unk> ? i am going through a book called python machine learning . i am working through how to use scikit and there is an example in the book where we write a method called <unk> . i am having a hard time understanding the reasoning behind some of the code and would love some help . here is my notebook from github : https : <unk> there are two lines that i am particularly interested in : > <unk> = <unk> ( ( <unk> , <unk> ) ) and > <unk> = <unk> ( ( <unk> , <unk> ) ) why do we want a <unk> stacked array in one case and a <unk> stacked array in another case ? any guidance on <unk> material is also appreciated . <eoq> x is input , usually a matrix of shape ( batch_size , <unk> ) . <unk> and <unk> are two <unk> of data , so you want to combine them to be a shape of ( <unk> , <unk> ) . y is <unk> , usually a vector of shape ( batch_size , ) . you want to combine <unk> and <unk> to a shape of ( batch_size <unk> , ) <eoa> 
vector space models and support vector machines i 'm applying vector space modelling and support vector machines to my thesis ( specifically to see how frequently certain terms appear in documents within a particular corpus ) . i was wondering whether i could ask some specific questions about the relationship between <unk> and svm . following <unk> , <unk> and <unk> 's paper , i 'm guessing that first each of the terms within the document are <unk> as vectors using the tf-idf ( so that terms are weighted higher if they appear more frequently in a document and weighted lower if they appear less frequently in the entire <unk> ) . svm could then be applied to each of these vectors in order to <unk> them depending on the terms which appear most frequently in each . is this the correct way of understanding the relationship between the two ? thank you . <eoq> the vector space model is simply a way to represent your document , while a support vector machine is a classification algorithm that is going to work with that representation . they <unk> on different <unk> of the machine learning process , and you could <unk> any of them with an other method . if it can help , imagine that you are working with documents which only have 2 words ( for <unk> purposes ) . that means that each document can be <unk> as a point on a plane ( that 's your vector space ) . your tf-idf <unk> is going to <unk> where you are going to put those points on the plane . your svm is going to try and <unk> a line ( the <unk> <unk> ) to separate ( as well as possible ) the points in the plane . <eoa> 
how to use create simple linear regression model with sklearn.dataset using pymc3 ? <eoq> `` how to create simple linear regression model with sklearn.dataset using pymc3 ? '' is what i meant to say <unk> . <eoa> 
how to use create simple linear regression model with sklearn.dataset using pymc3 ? <eoq> i really want to learn how to create predictive bayesian models so i can move away from <unk> and <unk> my understanding of statistics . i am having trouble taking the theoretical concepts and applying them real data ( i.e . bayesian linear model <unk> supervised data ) . <eoa> 
thousands of humans , dozens of decisions each , best clustering method to use for <unk> increasing certain decisions i have thousands of records ( of humans ) that each have dozens of percentage attributes ( of their decisions ) where for each human all their decision percentages add up to 1 . the assumption is that these records should be able to be <unk> based on their percentages , and then i should be able to get a new record that matches <unk> to one of these clusters and i would provide recommendations on how to better <unk> to one or more of the clusters by increasing the percentages of one or more ( but hopefully not too many ) decisions . also i 'd prefer it did n't recommend <unk> a decision percentage , because the only way that is possible is by increasing many other percentages . i 'm currently learning methods and scikit with the udacity course but have n't gotten to the section on clustering yet , and i need someone with a high level understanding to make sure i 'm going down the right path . i tried to <unk> away the actual problem , and hope i 've done a good job , but i would be happy to explain that if curious . <eoq> sounds like you are on the right path . to get <unk> the nearest cluster you may need to increase / decrease many percentages like you said - this depends on your data and clusters and clustering method . possibly you could only suggest 3 percentages to change in your <unk> ' of the application , and then you only suggest the most important changes . finally , you will have to think about how many clusters you want , and you will need to somehow gain some knowledge what these clusters are about . you will need to somehow <unk> a bit that your clusters you find using some algorithm ( for example kmeans ) are <unk> . good luck ! <eoa> 
why does lstm need 4 inputs per node , compared to sigmoid nodes which only use 1 var a sum ? could something simpler get the job done ? https : <unk> defines a node as ... <unk> < - <unk> + <unk> out = <unk> <unk> , <unk> , <unk> , and remember are <unk> sigmoid neural nodes <unk> input is a weighted sum of many other nodes . the purpose is for a network to self <unk> its patterns of how long to remember things . but <unk> it be done with something simpler like ... out < - <unk> ( <unk> ) + <unk> in , out , and decay are all <unk> . in and decay are normal sigmoid nodes , and out is <unk> by that equation and can be used the same way . for example , any existing <unk> could have a decay var added so it changes its state slower as chosen by some other node . training any of these is a harder problem than <unk> a <unk> model of data flow of a node type . how are rnns normally trained ? how are lstm normally trained ? how could a rnn with decay var per node be trained ? <eoq> hey . so the lstm can remember , forget and output a <unk> value ( that it <unk> <unk> ) . all these operations are dynamic ( decided based on the inputs , with <unk> weights ) . think of it as a <unk> memory unit . your net ( if i understand it correctly ) would output just a weighted average ( or <unk> sum ) of the previous output and current input <unk> ( you forgot to <unk> input with a weight matrix , bias and nonlinearity ) . what would that give ? well , it would be quite similar to deep <unk> networks <unk> . they work well , but check out their <unk> architecture in the original paper . they do not use a <unk> decay parameter though , just sum the output . how to train both an lstm or your rnn ? you 'd use backpropagation through time - you <unk> your rnn in time by k <unk> , giving a deep feedforward net , and then apply <unk> . if you 're <unk> with this , i would suggest trying to work with an <unk> rnn <unk> as a feed-forward net . just assume you have a constant number of inputs and <unk> your sequences if they 're <unk> . happy experimenting ! edit : deep <unk> networks http : <unk> ( code on github ) <eoa> 
<unk> networks and deep <unk> connected networks on graphs has anybody read this <unk> have been reading it and with all <unk> i am pretty lost <unk> anybody has read this paper please can you please let me know how the network <unk> is taking place . <eoq> [ article ] ( http : <unk> ) i have n't read this but it looks really hard ... what 's your background with ml ? <eoa> 
working with credit <unk> data i want to find a potentially better classifier than what i have right now so i 'm working with this data set in r http : <unk> i want to predict credit ( <unk> or <unk> ) using the other variables . most of them are categorical or binary , only about 3 are <unk> . <unk> , i 've built quite a few models for my class project and the best one that i made used <unk> training points and 200 test points and achieved about 15 % error rate , it was a decision tree that <unk> all the variables . most of the other models were in the 20 % error rate . i wan na know if you guys have any suggestions for lowering this as much as possible ? is it possible that there is a <unk> as to what can be extracted from the data ? things i 've tried : all the variables , best & <unk> subset selection applied to - > logistic regression , knn , lda , <unk> , <unk> methods , svm and decision trees , simple , <unk> & random forest . i also tried a neural network in r but the results were not as good as my simple decision tree . what model do you think will work best for this type of data ? <eoq> be sure to try some <unk> , such as pca , before trying a classifier , that might improve results . <eoa> 
experience with amazon aws ? just wondering if anyone here has any experience working with amazon aws ? looking through the website it felt like it was written more to convince management , and less geared to someone who would actually be `` doing '' it . anyone have any thoughts/experience on how it is ? its multiclass , binary and regression description seems a little sparse to call it ml in my ( unexperienced ) opinion <eoq> set up a spark cluster <unk> ! <eoa> 
experience with amazon aws ? just wondering if anyone here has any experience working with amazon aws ? looking through the website it felt like it was written more to convince management , and less geared to someone who would actually be `` doing '' it . anyone have any thoughts/experience on how it is ? its multiclass , binary and regression description seems a little sparse to call it ml in my ( unexperienced ) opinion <eoq> i use torch on their <unk> . once you get it setup it 's really nice . there are other services that offer much faster <unk> clusters though <eoa> 
experience with amazon aws ? just wondering if anyone here has any experience working with amazon aws ? looking through the website it felt like it was written more to convince management , and less geared to someone who would actually be `` doing '' it . anyone have any thoughts/experience on how it is ? its multiclass , binary and regression description seems a little sparse to call it ml in my ( unexperienced ) opinion <eoq> aws is set of more than <unk> services . are you asking about amazon machine learning ? if yes , that <unk> is more <unk> at ml beginners , i do n't think you will get the same results as a hand <unk> model . if no the main benefit is that you run whatever software you want on their <unk> and only <unk> per hour . <eoa> 
experience with amazon aws ? just wondering if anyone here has any experience working with amazon aws ? looking through the website it felt like it was written more to convince management , and less geared to someone who would actually be `` doing '' it . anyone have any thoughts/experience on how it is ? its multiclass , binary and regression description seems a little sparse to call it ml in my ( unexperienced ) opinion <eoq> at [ data science <unk> ] ( http : <unk> ) , we 've started <unk> aws ml , but there are many less options than in azure ml . azure has more algorithms and parameters to <unk> ( and the way it 's set up is easier for beginners to understand ) . aws ml is still pretty new - but it works for simple modeling if you already have all your data in <unk> or <unk> . <eoa> 
using jupyter for machine learning ? hey all ! i was just doing some reading and came across jupyter . it looks like it 's pretty useful for data analysis but i was wondering if it 's used in industry for machine learning ? or is a more traditional <unk> what is <unk> used ? <eoq> i 've never really known for <unk> to be <unk> . most of my <unk> <unk> use <unk> . most of them also mainly use c++ . <eoa> 
using jupyter for machine learning ? hey all ! i was just doing some reading and came across jupyter . it looks like it 's pretty useful for data analysis but i was wondering if it 's used in industry for machine learning ? or is a more traditional <unk> what is <unk> used ? <eoq> i work in an r & d <unk> and we use jupyter a lot for any python work -- when you 're <unk> and idea , or <unk> an analysis its excellent . of course once everything has been <unk> down we move to <unk> for the <unk> <unk> ( <unk> , <unk> , and <unk> are all popular ) of actually coding up a complete solution in <unk> . <eoa> 
when is probabilistic clustering applied ? the em algorithm seems to be the only example of probabilistic clustering . it seems to me that it is very similar to k-means , but deals better with noise . however , what is the advantage of <unk> clustering compared to <unk> based clustering like dbscan / optics ? <eoq> if you have some expectation that your data has clusters with a particular <unk> distribution then obviously an em approach is going to be the right answer . dbscan and optics and <unk> ( a further <unk> over optics ) can actually be <unk> as a <unk> probabilistic clustering where these exists some pdf that generated the data , and the goal is to find suitable level sets of that pdf -- this is most clear in <unk> and its obvious relationship with <unk> single <unk> ( which takes <unk> this probabilistic <unk> and even <unk> convergence to the level set tree of the pdf with <unk> data ) . <eoa> 
how to calculate the performance of a neural network ? i made a neural network using python and numpy , and was wondering how i could calculate the performance of my classifier on a given test set . any <unk> , or link would be appreciated . <eoq> by performance do you mean accuracy or speed ? in either case you should build two <unk> nns , one with your code and the other with an existing nn solution . train them both on the same dataset ( perhaps mnist ) and see what happens . it 's very likely that the nn you 've built will be significantly slower and if there are <unk> in your code you could get diff results . solutions like <unk> flow or theano do a lot of <unk> to make their <unk> very fast . <eoa> 
intuition for using matrices instead of vectors as gate parameter in an lstm when dealing with word <unk> . if i am correct then the <unk> of an lstm always take the same shape as the object , on which they are used to either forget , update or output the cell state . but i am wondering whether it is a stupid idea to just use for example a scalar to forget about a cell state which is a vector . since i did n't find any such implementations if there is a intuition related to word <unk> why this is not recommended . <eoq> so you might want a different scalar to decide which dimensions of the cell state you want to forget <unk> so we need a vector of <unk> to predict which will be <unk> . <unk> but we want this vector to depend on the rnn input , previous time point output , ( and possibly cell state ) ... so how can we transform those things into this vector of forget <unk> ? we use a matrix and a nonlinearity <eoa> 
point me in the right direction for simple machine learning project ? hello , i have a simple project where people can text a phone number and i need to determine what category of sentence they are saying . the sentences do follow a pattern , so much so that i could just write a logic tree or if statement . however , i do this this falls into the category of <unk> ? and i saw this library : https : <unk> i 'm a good developer , but new to machine learning . id like to use my project as a way of going a bit deeper , but not too deep : ) <unk> ? am i on the right track ? <eoq> if the sentences follow a fixed pattern you are likely better of writing the if statement . machine learning only makes sense for problems that ca n't be solved with a simple computer program or algorithm . in your case it would be a bad <unk> of the if statement . <eoa> 
is using a relu activation function always better then using using sigmoid/tanh ? ( neural networks ) from what i 've read online and from my limited experience training neural networks , it seems like relu is always a better choice than using a sigmoid/tanh activation function . i know one advantage relu has is that it does not <unk> from the vanishing gradient problem ( both sigmoid and tanh are <unk> by this ) . on http : <unk> , using relu always seems to get to a better result faster than the other functions . are there cases where sigmoid/tanh work better then relu ? if so , what are those cases and how would i identify them ? <eoq> if you wan na to learn an input representation , like sparse coding , then sigmoid activation is a good choice . <eoa> 
handling multiple feature types in an <unk> stump classifier [ matlab or python ] hi , i 'm working on a project where i 'm using an <unk> decision stump classifier to predict a binary label ( 1 or -1 ) . i have working code that works on feature vectors that are exclusively binary , and am now trying to handle certain elements of the feature vector that are not binary . for example , some of them are numeric ( i.e . <unk> ) , or categorical ( i.e . `` <unk> '' ) . they are <unk> <unk> the feature vector . i 'm considering <unk> the data set provided to put categorical variables at the end and handle them later , or expanding each categorical feature to multiple <unk> binary features ( which seems very <unk> ) . how should i go about dealing with categorical features , given that i 'm trying to use the adaboost formula on decision <unk> that have primarily <unk> with binary features only ? thank you ! <eoq> numeric features should be no problem of the original adaboost formulation ( see wikipedia / google ) , your decision stump should just be able to handle them , and then you can <unk> it into your adaboost code . categorical features should be <unk> with <unk> variables or one hot encoding : https : <unk> <eoa> 
what is the current state of the art in speech recognition ? sorry for <unk> a question straight from the <unk> , but it is one that <unk> me quite a bit , and i did not find it having been asked here recently . so what is the current state of the art in speech recognition ? open source and/or otherwise ? <eoq> [ deep speech <unk> ] ( http : <unk> ) should be in that <unk> . <eoa> 
need course recommendation hey /r/mlquestions ! i want to get into machine learning , and i 've been following andrew ng 's machine learning course on coursera . however , i 'm finding it kinda hard to follow , i 'm finding the math kinda shallow . i 'm <unk> on my math skills , and thus i 'm looking for a <unk> path recommendation which will also <unk> the math side by side , or at least point me in the right direction . any recommendations would be helpful ! thanks ! <eoq> hi ! i work at data science <unk> . besides our bootcamp , i recommend the books that we use in the bootcamp : 1- a <unk> guide to statistics ( easy intro to hard math ) 2- predictive analytics by <unk> <unk> 3 - r for everyone 4 - doing data science : straight talk from the <unk> good luck ! <eoa> 
what are advantages of ensemble learning compared to a single model ? ensembles ( e.g . multiple decision trees ) can reduce overfitting and give a better classifier than only a single system . is there another advantage ? <eoq> the very statement of your question suggests a pretty good advantage right away . but it 's also the case that various model classes ( or hypothesis classes ) can be <unk> in ensembles , <unk> a classifier ( or regression ) <unk> not possible with any single model . another advantage is that certain methods of training , such as what you find in random forests , provide a form of <unk> feature selection , as any <unk> <unk> by <unk> , local feature selection ( e.g . information gain ) in any individual model are often <unk> by choosing from a random subset of features from model to model . ensembles can also be <unk> more easily , leading to more efficient performance . there is also the issue of <unk> , which allows one to create a classifier that is <unk> accurate from a set of models that are individually <unk> . there are more advantages , but these are some of the big ones that spring to mind . <eoa> 
theano vs tensorflow ? in your opinion which of the above two is easier to learn and prototype new neural networks . i have little experience in theano and it was very hard to get a grasp on how things are working . how do these two compare ? <eoq> i 'm no expert but <unk> flow is still <unk> new where 's theano has been out for a while so there may be a bigger <unk> around theano for when you need help ? ! ? <eoa> 
theano vs tensorflow ? in your opinion which of the above two is easier to learn and prototype new neural networks . i have little experience in theano and it was very hard to get a grasp on how things are working . how do these two compare ? <eoq> in my course we had used torch and it was really easy to get started it had lot of the built in stuff and learning <unk> is <unk> i prefer python so <unk> to theano recently and i must say it 's too <unk> but it gives you a much better understanding of what you are doing <unk> variable , scan , function these are things you have to get a hold of before you start theano . <eoa> 
theano vs tensorflow ? in your opinion which of the above two is easier to learn and prototype new neural networks . i have little experience in theano and it was very hard to get a grasp on how things are working . how do these two compare ? <eoq> i recently decided to go with tensorflow . the reason was simply because it is open source , developed by a big company of which i 'm sure it will continue development for a while . due to this fact i expect more and more people to <unk> and other nice software to be written around tensorflow ( e.g . [ <unk> ] ( https : <unk> ) ) . <eoa> 
theano vs tensorflow ? in your opinion which of the above two is easier to learn and prototype new neural networks . i have little experience in theano and it was very hard to get a grasp on how things are working . how do these two compare ? <eoq> this is actually a very complex question with no correct answer ... it 's also worth <unk> that with tensorflow and theano ( and probably the other ml frameworks as well ) there are some great libraries that <unk> on top of them that make development far simpler . such as <unk> for <unk> and lasagne for theano . <eoa> 
theano vs tensorflow ? in your opinion which of the above two is easier to learn and prototype new neural networks . i have little experience in theano and it was very hard to get a grasp on how things are working . how do these two compare ? <eoq> tensorflow is the preferred because it supports more <unk> and is faster . <eoa> 
knn without predictions ? excuse my ignorance , i am very new to all this : ) . for reference : * using python 3 ( numpy , pandas , ( hypothetically ) scikit ) * a small-ish data set , ( 4k rows , 15 columns ) . i know you can use knn to find the nearest neighbor ( s ) and it predicts something ( the example i 've seen in a bunch of tutorials is iris type ) . my question is : what if i do n't want to predict anything ? i 've got all my data , i just want to input one row ( with all 15 categories known ) and find the most similar data point . i am using a simple euclidean distance measurement ( after normalization of the data ) . but is there a good way to do this with ml ? are there other options i am missing ? perhaps you have a suggestion for something else ? thanks ! <eoq> unsupervised learning <eoa> 
knn without predictions ? excuse my ignorance , i am very new to all this : ) . for reference : * using python 3 ( numpy , pandas , ( hypothetically ) scikit ) * a small-ish data set , ( 4k rows , 15 columns ) . i know you can use knn to find the nearest neighbor ( s ) and it predicts something ( the example i 've seen in a bunch of tutorials is iris type ) . my question is : what if i do n't want to predict anything ? i 've got all my data , i just want to input one row ( with all 15 categories known ) and find the most similar data point . i am using a simple euclidean distance measurement ( after normalization of the data ) . but is there a good way to do this with ml ? are there other options i am missing ? perhaps you have a suggestion for something else ? thanks ! <eoq> are you interested in clustering your data into groups ? if so sklearn provides a bunch of methods for doing so : http : <unk> <eoa> 
knn without predictions ? excuse my ignorance , i am very new to all this : ) . for reference : * using python 3 ( numpy , pandas , ( hypothetically ) scikit ) * a small-ish data set , ( 4k rows , 15 columns ) . i know you can use knn to find the nearest neighbor ( s ) and it predicts something ( the example i 've seen in a bunch of tutorials is iris type ) . my question is : what if i do n't want to predict anything ? i 've got all my data , i just want to input one row ( with all 15 categories known ) and find the most similar data point . i am using a simple euclidean distance measurement ( after normalization of the data ) . but is there a good way to do this with ml ? are there other options i am missing ? perhaps you have a suggestion for something else ? thanks ! <eoq> if you have one row and want to find the most <unk> point in your data , simply calculate all <unk> of all rows to your new row , and find the one with the minimum distance , that will be the most <unk> row . <eoa> 
need help understanding the maths behind gradient descent using <unk> book i have <unk> a wall at an exercise ( part http : <unk> # <unk> ) . i think i have understood most of the ideas of the gradient descent for training a neural network , however i ca n't understand the first exercise of the topic : prove that the choice of δv which <unk> <unk> is <unk> , where <unk> is determined by the size <unk> <unk> . where c is the cost function ( objective function ) , v is a variable of c . the question also suggests : <unk> : if you 're not already familiar with the <unk> <unk> , you may find it helpful to <unk> yourself with it . i have tried <unk> around with the <unk> like so : <unk> then <unk> but i get stuck at this point . am i going in the right direction ? <eoq> what you really have to use is the equality case of <unk> . cs states that for two vectors x and y and the scalar product < , > , you have : | < x , y > | < = <unk> <unk> with equality if and only if x and y are <unk> e.g there exists some constant ( let 's call it - η ) such that x = - η y . as you want to minimize < x , y > , the best you can hope to achieve is < x , y > = - <unk> <unk> . for x = δv and y = <unk> you get the result because the equality case is a upper <unk> . then as δv = - η <unk> , you have η = <unk> / <unk> if the gradient is non-zero . <eoa> 
plotting hypothesis function in linear regression hi guys ! sorry in advance for what i assume is a very stupid question . i am taking the machine learning course on coursera , and i 'm <unk> a <unk> on the <unk> linear regression hypothesis function plot . [ image ] ( http : <unk> ) the image linked above is the plotting i 'm <unk> to . i 'm unable to understand how <unk> 1 being 0.5 <unk> a <unk> to the line , and what the <unk> of 2 is . also [ this ] ( https : <unk> ? <unk> ) is the video in question where the plot is being <unk> about . once again sorry for being <unk> about this . <eoq> hi , h ( x ) = <unk> + <unk> just realize that h ( x ) is your y value . in the first example : <unk> = <unk> <unk> = 0 so y will always be <unk> no matter what . in the second example , <unk> is now 0.5 , so whatever x is , y = <unk> . i 'm sure you get it by now , too <unk> to do example points . cheers . <eoa> 
classification of <unk> i have an <unk> device that will give me a 200 <unk> signal from 8 different <unk> <unk> around the <unk> . they come to me as a <unk> array . i 'd like to classify these , however i 'm not sure where to start . since these are signals , i 'm assuming just <unk> in the raw array for each time sample is n't a <unk> option . i 've tried using a knn on them and it 's results were around <unk> % accurate . is my use case right for ml or am i missing another algorithm that can do this for me ? <eoq> the scenario is correct but <unk> the knn with the raw signals is not the right way to proceed . you 'll need some pre-processing step on the signals like filtering , than you have to extract features from the signals probably <unk> them , than you have to identify the more relevant features for your problem and use those features ( predictors ) to train your ml algorithm . this is a very very general overview , i 'm not an expert as well but i suggest you to search for `` times series , machine learning '' , i think that would be a good start , but as said , i 'm not an expert . <eoa> 
how to choose an unsupervised classification algo ? food <unk> dataset i have a dataset with <unk> information for <unk> <unk> . for each food i have the number of <unk> of <unk> , <unk> , <unk> that are contained in <unk> of that food . when i plot the data , i can identify a few clusters <unk> : http : <unk> how do i go about choosing a unsupervised classification algo for this problem ? using the dbscan algorithm i was able to get this result : http : <unk> is there a diff algo that would allow me to do better ? <eoq> you could try <unk> . it effectively runs dbscan for all possible <unk> values and tries to identify the best possible <unk> clustering from that . this makes parameter selection alot easier and gets you to a good clustering quickly . <eoa> 
how to choose an unsupervised classification algo ? food <unk> dataset i have a dataset with <unk> information for <unk> <unk> . for each food i have the number of <unk> of <unk> , <unk> , <unk> that are contained in <unk> of that food . when i plot the data , i can identify a few clusters <unk> : http : <unk> how do i go about choosing a unsupervised classification algo for this problem ? using the dbscan algorithm i was able to get this result : http : <unk> is there a diff algo that would allow me to do better ? <eoq> you could also try to use hierarchical clustering for your data and see if it <unk> better . http : <unk> # <unk> <eoa> 
how can i get a remote , beginner level job in ml ? hi , i am a <unk> programmer from <unk> . if you want more background on my question , you can check out my blog post called [ the tools of a <unk> programmer ] ( https : <unk> ) . i 've been developing the <unk> for web applications for more than 5 years . however , with web development being <unk> more and more toward <unk> , i 'm thinking of changing my field . i have <unk> in love with machine learning . however , here are the issues i have with it : 1. there are no machine learning jobs in <unk> . the <unk> has n't gotten big enough to have big data . this will probably change in the next 2-3 years , but right now , it 's <unk> . 2. all jobs require you to already be <unk> in the field , <unk> remote jobs , because they are not looking for people they will have to <unk> . i understand that point of <unk> , but i have no way of connecting with these people in person and asking them to give me a chance . 3. i am not strong in mathematics . in fact , i 'm not strong in any kind of theory that i ca n't <unk> apply . while i will definitely get around to learning the <unk> that <unk> machine learning once i get more into it , i do n't want to start with theory and then learn how i can bring it into the real world . so , is there a way of getting a remote job in my <unk> ? if not , how can i get myself to a place where i would be <unk> , while keeping my day job of writing <unk> systems with <unk> ? thank you guys in advance for your help . you 're awesome ! <eoq> i do n't know why you would think it 's easier <unk> else . most of these jobs require a <unk> ( <unk> or phd ) degree . not being strong in mathematics means your resume is in the <unk> before i even finish reading it because i do n't want to spend 5 years training someone if i need someone now . your <unk> # 3 has all kinds of <unk> flags , even if you wrote the opposite of the first sentence . your best bet is to take some statistics courses on <unk> . you 'll probably need prerequisite math up to calculus <unk> to get through it . then , start working with some open source libraries and <unk> data to prove you have the <unk> . i 'd make it a 5-10 <unk> plan so not having anything in your home <unk> for the next couple years wo n't matter if you really want to get into the field . <eoa> 
does there exist any ml algorithm to detect pivotal input parameter ? i want to know if there exist any neural network algorithm that allows you to determine which out of all the input parameters is the pivotal factor . or in other words can i detect the input parameter , changing the values of which yields maximum variation in the output value . for instance , i am doing a project to determine the air quality index of any given city using neural networks . as input parameters i have considered a variety of meteorological factors ( like wind speed , temperature , air pressure , humidity etc ) + air pollution from cars + air pollution from industries and my output parameter is pm 2.5 value . now i want to deduce , out of all these input factors which is the major contributor to the pm 2.5 value or changes in the values of which of the input parameters changes the output value significantly . <eoq> you may want to look at traditional statistical techniques like regression if you need an interpretable model . <eoa> 
does there exist any ml algorithm to detect pivotal input parameter ? i want to know if there exist any neural network algorithm that allows you to determine which out of all the input parameters is the pivotal factor . or in other words can i detect the input parameter , changing the values of which yields maximum variation in the output value . for instance , i am doing a project to determine the air quality index of any given city using neural networks . as input parameters i have considered a variety of meteorological factors ( like wind speed , temperature , air pressure , humidity etc ) + air pollution from cars + air pollution from industries and my output parameter is pm 2.5 value . now i want to deduce , out of all these input factors which is the major contributor to the pm 2.5 value or changes in the values of which of the input parameters changes the output value significantly . <eoq> i <unk> with <unk> . there will be statistical methods that are n't `` machine learning '' that can answer this - and <unk> that 's the case , i always recommend going with the standard statistical method . <eoa> 
does there exist any ml algorithm to detect pivotal input parameter ? i want to know if there exist any neural network algorithm that allows you to determine which out of all the input parameters is the pivotal factor . or in other words can i detect the input parameter , changing the values of which yields maximum variation in the output value . for instance , i am doing a project to determine the air quality index of any given city using neural networks . as input parameters i have considered a variety of meteorological factors ( like wind speed , temperature , air pressure , humidity etc ) + air pollution from cars + air pollution from industries and my output parameter is pm 2.5 value . now i want to deduce , out of all these input factors which is the major contributor to the pm 2.5 value or changes in the values of which of the input parameters changes the output value significantly . <eoq> as a <unk> rule of thumb , you can look at the size of an input 's <unk> weights to estimate it 's importance . ( and you could multiply it by the importance of the node a weight is connected to , which you determine in the same way . ) you can also do `` <unk> tests '' where you <unk> each input ( or combination of inputs ) and see how much this changes the output , either in absolute terms or in terms of error . you can do this after training to find out the important of a ( set of ) parameter ( s ) in the final network , or before training to find out how important it is to begin with . <eoa> 
does having a degree from a ranked university ultimately matter ? i started taking cs courses at a local university ( university of <unk> <unk> <unk> ) and <unk> up really <unk> it . a lot of the <unk> here do research in <unk> <unk> and i ended up taking some graduate level courses too . do you all think it would be worth it to get the <unk> at an <unk> university ( though its <unk> school in <unk> is [ ranked # 101 ] ( http : <unk> ) ) , or would it be <unk> for me to just <unk> the <unk> and apply <unk> , such as <unk> <unk> or <unk> else of the like ? thanks ! ( edit : <unk> ) <eoq> you 're in one of the fastest growing fields in the world . do n't worry about <unk> and do what you like . be interested , have fun and the rest will happen automatically . <eoa> 
good language for introduction to <unk> algorithms ? hello , so i am trying to find a language with which i can write code to <unk> through <unk> reasoning <unk> ' , as well as <unk> it 's search algorithms based on information learned from these nets . i also want a language that i can use to write scripts for a 2d game engine , as i would like to build visual models of my projects for a web page ( to help my <unk> <unk> ) . so far i am only really familiar with <unk> ( been working full time as a <unk> developer for about 7 months ) , but i have spent quite a bit of time developing relatively <unk> models for problem solving that i would like to attempt to put into code . any <unk> would be greatly appreciated , thank you ! <eoq> well . what kind models ? <eoa> 
good language for introduction to <unk> algorithms ? hello , so i am trying to find a language with which i can write code to <unk> through <unk> reasoning <unk> ' , as well as <unk> it 's search algorithms based on information learned from these nets . i also want a language that i can use to write scripts for a 2d game engine , as i would like to build visual models of my projects for a web page ( to help my <unk> <unk> ) . so far i am only really familiar with <unk> ( been working full time as a <unk> developer for about 7 months ) , but i have spent quite a bit of time developing relatively <unk> models for problem solving that i would like to attempt to put into code . any <unk> would be greatly appreciated , thank you ! <eoq> python is very popular for <unk> stuff in general and it 's pretty <unk> used to make websites . that means there 's potentially a lot of support ( frameworks etc. ) . i would n't say it 's <unk> suited for self <unk> <unk> though . for that i 'd go with ( some <unk> of ) <unk> . <eoa> 
resources for text <unk> writing ? there seems to be a lot of data on generalized numeric data , or vision systems . but where are there resources on text <unk> applications ? i 've had a lot of difficulty finding resources to learn about these fields . do you have any to share ? most frameworks seem to focus <unk> on numeric data for analyzing <unk> as first class <unk> . <eoq> i 've <unk> searching so far i 've found [ this ] ( https : <unk> ) <eoa> 
back propagation dnn , question about layers when you are using a dnn is a good <unk> to make the number of neurons in a layer always <unk> , from the input to the output ? <eoq> no . easy example : <unk> <unk> <eoa> 
help with svm math i 'm trying to do support vector machine classification . i understand in general how it works , although not all of the <unk> <unk> math behind it . there 's one thing i 'm confused about in particular . with reference to this : https : <unk> ... <unk> it says we want to minimize 1/2 * <unk> * w or <unk> 1/2 * <unk> i know that the <unk> of the <unk> is <unk> so we want to minimize <unk> . can someone explain to me where the 1/2 and square term come in ? <eoq> the 1/2 is added because it leaves the solution <unk> but it gives a better looking solution after <unk> . <eoa> 
how to <unk> a nn with more data ? if i have a trained nn with a training set of <unk> inputs with <unk> features , and i want to <unk> the nn with lets say <unk> new inputs , how can i train the nn , given that it was trained with <unk> data and now i have only <unk> . i hope you understand me . <eoq> by inputs you mean the number of samples in your data sets , right ? not the number of input nodes of your network . if the inputs and outputs in both data sets follow the same format , then you can basically do whatever you want . you can now start training on your new data set exclusively , or you can <unk> them together into a single <unk> sample data set . or if you want to keep training on both , but you want to make the second data set more important , you can <unk> it a few times . for instance , you can make 10 <unk> of each sample in the second data set and combine it with the first to make a <unk> sample data set . then you just continue training on that . <eoa> 
can you train a network to recognize a pattern , then produce a function that can be used independently of the network ? for example : i want to make an app that detects when i say `` hello '' . from my understanding , i could train a neural network with a bunch of <unk> of me saying `` hello '' . would it be possible to extract the network 's `` knowledge '' of what my hello sounds like ? <unk> i know next to nothing about machine learning , just curious about its <unk> and uses . <eoq> the short answer is yes , that is totally possible ! a great example would be the mnist dataset , which contains thousands of handwritten digits as <unk> images . you can train a neural network , which <unk> patterns in these images and learns what the numbers <unk> look like . once your network has been fed a lot of example images and learned these different patterns , you can the then output what the nn <unk> a certain <unk> looks like . i 'm on <unk> <unk> , will provide links when i get home . if this sort of stuff <unk> you , there some great resources to get started online , just have a look about in this subreddit : ) edit : links as <unk> 1. <unk> of features learned in the mnist dataset mentioned above https : <unk> 2. get started with some literature https : <unk> 3. online courses https : <unk> i would definitely recommend you start easy , do some linear regression with <unk> and get a feel for the <unk> and the frameworks available out there ! <eoa> 
<unk> large image data to cnn hello all , i am currently working on a project to recognize features on mars ' <unk> using a cnn . an example would be to recognize sand dunes from a large <unk> image of an area of interest . the only dataset i have is a large <unk> by <unk> <unk> image of an area that has sand dunes and the same image with the sand dunes being <unk> in white . i plan to use the <unk> image as training and the untouched image as testing . how would i approach <unk> the data to the cnn ? <unk> the image in smaller <unk> seems ideal , but that 's still a lot of images i think , and would require a lot of memory . what is the best way to go about this ? edit : i should also mention that i 'm using <unk> and python <unk> <eoq> 1. you do n't have to use all of the patches , use the amount your <unk> allow you to . 2. you could split the patches and read them from the <unk> when needed i would go with option 1 as reading from the <unk> will slow you down quite a bit . and probably lots of the <unk> patches are <unk> anyway . <eoa> 
<unk> <unk> on lasagne , or lasagne for <unk> . hi , i have being studying nn 's for <unk> , and now i decided to move to <unk> instead of coding simple mlp with sigmoid activations . i found lasagne , the idea of stacking layers was to me very interesting , but while reading the <unk> 's a couple of things seems confusing to me ( probably the part of theano ) , and that is the <unk> of this post . since i did n't find a better way to present my <unk> , i 'll post the code and make questions along the way . let 's say that we have a <unk> , which has 10000 entries , each row with 15 attributes and one label ( 1 or 0 ) . the the program would probably begin with : import pandas as <unk> import <unk> as t from <unk> import <unk> , denselayer , get_output from <unk> import <unk> , softmax from <unk> import categorical_crossentropy from <unk> import <unk> train = <unk> ( <unk> ' ) <unk> = <unk> ( ( <unk> ) ) <unk> = denselayer ( <unk> , <unk> = 500 , nonlinearity = <unk> ) l_out = denselayer ( <unk> , <unk> = 2 , nonlinearity = softmax ) pred = get_output ( l_out ) so far , so good . the problem start below target_var = <unk> ( <unk> ' ) loss = categorical_crossentropy ( pred , target_var ) if i understood theano correctly , all this is some sort of memory allocation without `` real '' <unk> been made . but , trying to run this piece of code <unk> the <unk> error : <unk> ( most <unk> call last ) : file `` <unk> '' , line 20 , in < module > loss = categorical_crossentropy ( pred , target_var ) file `` <unk> '' , line <unk> , in categorical_crossentropy return <unk> ( predictions , targets ) file `` <unk> '' , line <unk> , in categorical_crossentropy <unk> typeerror ( <unk> mismatch between coding and true distributions ' ) typeerror : rank mismatch between coding and true distributions since the label is <unk> ' or <unk> ' , should n't <unk> ' be an scalar ? loss = <unk> ( ) bonus question , is it possible to <unk> the values of the loss function in each epoch , to plot a graph ? params = <unk> ( l_out , <unk> ) bonus question 2 , been the network trained , params can be saved in on file and be read later ? updates = <unk> ( loss , params , <unk> , <unk> ) test_pred = get_output ( l_out , <unk> ) <unk> = categorical_crossentropy ( test_pred , target_var ) <unk> = <unk> ( ) <unk> = <unk> ( <unk> ( <unk> ( test_pred , <unk> ) , target_var ) , <unk> ) input_var = <unk> ( <unk> ' ) # correct type ? train_fn = <unk> ( [ input_var , target_var ] , loss , <unk> ) and , for the training part , is it the correct procedure ? for i in range ( len ( train ) ) : input = <unk> [ range ( 15 ) ] target = train [ <unk> ' ] train ( input , target ) to finish , if i just want to evaluate the output of the network , can i just use output = get_output ( l_out ) or it has to be a theano function ? note , that the code above ( with the exception of the code till the error <unk> ) is not a `` real '' code i just <unk> ( based on the tutorial ) to try to understand better the <unk> of lasagne . <unk> : it might be a little confusing because it is <unk> and i have the <unk> at <unk> . still , any help would be appreciated . <eoq> > loss = categorical_crossentropy ( pred , target_var ) this is wrong . if you have 5 predictions , you should have 5 target variables . they should match shape . <unk> , both be ( 1 , k ) or ( <unk> ) or ( k , ) . -- > <unk> typeerror ( <unk> mismatch between coding and true distributions ' ) that 's why you have this error . the shapes do n't match . -- > bonus question , is it possible to <unk> the values of the loss function in each epoch , to plot a graph ? loss is <unk> from your <unk> function . > train_fn = <unk> ( [ input_var , target_var ] , loss , <unk> ) this is your <unk> function . it 's basically like <unk> : def train_fn ( input_var , target_var ) : return loss -- > train ( input , target ) you should have loss = train_fn ( inputs , targets ) -- in general , i find <unk> theano code via a <unk> or notebook very <unk> . most of your <unk> would go away if you did this and tested your questions <unk> . <eoa> 
probably not a great question . i do n't know that this is the proper <unk> for this question but it 's here that my searching has <unk> me so it 's here that i 'll stage myself . i <unk> out of college 3 years ago and have recently <unk> learning programming again . i have a decent grasp on basic programming <unk> and am <unk> a lot of college math . currently working through a college algebra mooc then moving on to bigger and better things . so as someone with relatively little background , where should i start learning about machine learning . i 'm not looking to get a job or anything like that , it 's more of a <unk> . i 'd like to do a project of some sort using it but first i need to actually learn how to use it . i understand that there is a lot of math , so any direction toward online resources or books or what have you would be immensely helpful . thank you in advance for your time . <eoq> machine learning - coursera , stanford university is your next stop after you know the basics of linear algebra <eoa> 
probably not a great question . i do n't know that this is the proper <unk> for this question but it 's here that my searching has <unk> me so it 's here that i 'll stage myself . i <unk> out of college 3 years ago and have recently <unk> learning programming again . i have a decent grasp on basic programming <unk> and am <unk> a lot of college math . currently working through a college algebra mooc then moving on to bigger and better things . so as someone with relatively little background , where should i start learning about machine learning . i 'm not looking to get a job or anything like that , it 's more of a <unk> . i 'd like to do a project of some sort using it but first i need to actually learn how to use it . i understand that there is a lot of math , so any direction toward online resources or books or what have you would be immensely helpful . thank you in advance for your time . <eoq> the three most important math classes for machine learning are probability theory , <unk> <unk> , and linear algebra . those are all roughly <unk> level applied math classes . after you 're <unk> with those topic check out the coursera mooc . <eoa> 
<unk> or <unk> for ml ? <eoq> <unk> edit , since you asked for it : because there is almost no argument for 2 anymore . it still has some libraries that are not available for 3 , but those are <unk> and you can probably find good , if not better <unk> . * python 2 will <unk> it 's support eventually , while 3 continues to be <unk> ; it is the <unk> version . * new libraries are primarily developed for 3 , although some still support both versions . * python 3 <unk> supports <unk> , which <unk> some problems , especially for <unk> . * python 3 comes <unk> with <unk> , a package <unk> . while people <unk> that ( <unk> ) <unk> is a better package <unk> , <unk> often <unk> . * python 3 is ( in my opinion ) more <unk> . things that were done in python 2 <unk> now have to be done explicitly , which <unk> quite some confusion . also have a look at : https : <unk> note that the <unk> of library support is not really relevant anymore . i have never found a library that i needed that was available in 2 , but not in 3 . <eoa> 
code for image segmentation ? <unk> segmentation ? anyone know a good github <unk> or other code source for <unk> <unk> <unk> or <unk> segmentation implementations ? any language or library is fine , i 'm just looking for source to compare with and maybe run as a <unk> . <eoq> [ here 's one ] ( https : <unk> # <unk> ) credit for finding it goes to <unk> for <unk> my question on the <unk> simple questions thread ' on /r/machinelearning <eoa> 
convolutional network : how to determine filter size and number ? hi , say you have an <unk> image , how does one determine the filter size and the number of filters in the first <unk> layer ? <eoq> rules of thumb + trial and error ( cross-validation ) https : <unk> https : <unk> https : <unk> <eoa> 
what are some good concepts/algorithms for this problem ? disclaimer : i 'm not a ml person , and this is not a homework assignment . there is a deterministic two-player full-information game . ( tic-tac-toe , chess , go , diplomacy , etc . etc. ) . i do know what the game is , but it 's pretty silly , so let 's just pretend its chess instead . so the task that i have to complete is simply to write a heuristic that takes a state of my game and evaluates to some number-value . for example , for chess , maybe i can write a program that just counts the number of white pieces on the board . a naive heuristic , but that 's the type of program that i could use . i was thinking it would be pretty cool if i could use some machine learning algorithms to learn a heuristic and train it on some data of game records . what machine learning algorithm is best for this sort of task ? taking a state - > return a number value describing how good the state is for example , i was taking to my friend about making a neural network . but he said that would be totally inappropriate for my kind of task . if that 's bad . what 's good ? <eoq> have a look at the <unk> design https : <unk> ? <unk> <eoa> 
what are some good concepts/algorithms for this problem ? disclaimer : i 'm not a ml person , and this is not a homework assignment . there is a deterministic two-player full-information game . ( tic-tac-toe , chess , go , diplomacy , etc . etc. ) . i do know what the game is , but it 's pretty silly , so let 's just pretend its chess instead . so the task that i have to complete is simply to write a heuristic that takes a state of my game and evaluates to some number-value . for example , for chess , maybe i can write a program that just counts the number of white pieces on the board . a naive heuristic , but that 's the type of program that i could use . i was thinking it would be pretty cool if i could use some machine learning algorithms to learn a heuristic and train it on some data of game records . what machine learning algorithm is best for this sort of task ? taking a state - > return a number value describing how good the state is for example , i was taking to my friend about making a neural network . but he said that would be totally inappropriate for my kind of task . if that 's bad . what 's good ? <eoq> i tried to do something like this with <unk> once . <eoa> 
what are some good concepts/algorithms for this problem ? disclaimer : i 'm not a ml person , and this is not a homework assignment . there is a deterministic two-player full-information game . ( tic-tac-toe , chess , go , diplomacy , etc . etc. ) . i do know what the game is , but it 's pretty silly , so let 's just pretend its chess instead . so the task that i have to complete is simply to write a heuristic that takes a state of my game and evaluates to some number-value . for example , for chess , maybe i can write a program that just counts the number of white pieces on the board . a naive heuristic , but that 's the type of program that i could use . i was thinking it would be pretty cool if i could use some machine learning algorithms to learn a heuristic and train it on some data of game records . what machine learning algorithm is best for this sort of task ? taking a state - > return a number value describing how good the state is for example , i was taking to my friend about making a neural network . but he said that would be totally inappropriate for my kind of task . if that 's bad . what 's good ? <eoq> <unk> , for this task i have used monte carlo tree search and suggest it . it requires only <unk> <unk> for each game you apply it to . training a <unk> state <unk> is going to be very dependent upon the specifics of the game . i mean you can just use a svm or ann or what have you on your state vector but for it to really work well you want to figure out what is the relevant information to do your regression on ... this can be very <unk> . what is the game ? i mean , why are you <unk> because you are trying to solve a silly game ? look at the cool work these guys did with <unk> & <unk> : http : <unk> edit : monte carlo tree search ( <unk> ) is a simple algorithm . it is n't a machine learning technique per <unk> , but it can be <unk> with them . in its simplest form : from a single game state assume all <unk> <unk> are made randomly . play many random games starting at that state . record the <unk> of wins to <unk> . there are many , many ways to improve on this , and the most important is to bias the random <unk> towards ones that are more <unk> ' or <unk> ' . <eoa> 
`` are you sure you should be doing that ? '' hey people , i have a non-critical system i 'm working on that logs when people decide to do certain things . i want to flag when users do things that do n't match what they 've done before . an analogy would be `` joe posts on /r/machinelearning at noon every tuesday '' . here is how i 'm thinking about this : * output : which subreddit joe posted on * inputs : time of day , day of week over time , the system `` learns '' when joe is likely to post on what subreddit with the ultimate goal being something like this . one day , joe posts on /r/cars on tuesday at noon . the system notices this and says `` hey joe , are you sure you should be posting on /r/cars ? usually , you post on /r/machinelearning at this time . '' another case would be that tuesday at noon rolls around and joe does not post on anything . the system notices this and flags it for joe : `` hey joe , should you be posting on /r/machinelearning ? '' the system gets to watch joe 's posting history as long as is necessary for there to be some confidence in the flags that are being generated . in reality , joe is not actually watching the flags , these are flags being aggregated across many different people and the flags are just reports generated for the phbs to decide whether to investigate or not . then those decisions are fed back into the system . besides reading a book on neural networks many many years ago , i am a noob to machine learning . however , this screams to me to be some sort of machine learning project . am i way off ? how would i approach it ? thanks . <eoq> i think what you 're looking for is `` anomaly detection '' . <eoa> 
`` are you sure you should be doing that ? '' hey people , i have a non-critical system i 'm working on that logs when people decide to do certain things . i want to flag when users do things that do n't match what they 've done before . an analogy would be `` joe posts on /r/machinelearning at noon every tuesday '' . here is how i 'm thinking about this : * output : which subreddit joe posted on * inputs : time of day , day of week over time , the system `` learns '' when joe is likely to post on what subreddit with the ultimate goal being something like this . one day , joe posts on /r/cars on tuesday at noon . the system notices this and says `` hey joe , are you sure you should be posting on /r/cars ? usually , you post on /r/machinelearning at this time . '' another case would be that tuesday at noon rolls around and joe does not post on anything . the system notices this and flags it for joe : `` hey joe , should you be posting on /r/machinelearning ? '' the system gets to watch joe 's posting history as long as is necessary for there to be some confidence in the flags that are being generated . in reality , joe is not actually watching the flags , these are flags being aggregated across many different people and the flags are just reports generated for the phbs to decide whether to investigate or not . then those decisions are fed back into the system . besides reading a book on neural networks many many years ago , i am a noob to machine learning . however , this screams to me to be some sort of machine learning project . am i way off ? how would i approach it ? thanks . <eoq> if joe always posts on <unk> on tuesday , and then one week he posts to <unk> on <unk> , should n't that also be a <unk> flag ? if joe is a <unk> <unk> , then why would the time be important at all ? <eoa> 
`` are you sure you should be doing that ? '' hey people , i have a non-critical system i 'm working on that logs when people decide to do certain things . i want to flag when users do things that do n't match what they 've done before . an analogy would be `` joe posts on /r/machinelearning at noon every tuesday '' . here is how i 'm thinking about this : * output : which subreddit joe posted on * inputs : time of day , day of week over time , the system `` learns '' when joe is likely to post on what subreddit with the ultimate goal being something like this . one day , joe posts on /r/cars on tuesday at noon . the system notices this and says `` hey joe , are you sure you should be posting on /r/cars ? usually , you post on /r/machinelearning at this time . '' another case would be that tuesday at noon rolls around and joe does not post on anything . the system notices this and flags it for joe : `` hey joe , should you be posting on /r/machinelearning ? '' the system gets to watch joe 's posting history as long as is necessary for there to be some confidence in the flags that are being generated . in reality , joe is not actually watching the flags , these are flags being aggregated across many different people and the flags are just reports generated for the phbs to decide whether to investigate or not . then those decisions are fed back into the system . besides reading a book on neural networks many many years ago , i am a noob to machine learning . however , this screams to me to be some sort of machine learning project . am i way off ? how would i approach it ? thanks . <eoq> disclaimer : i 'm not a computer scientist . <unk> <unk> , if you have a <unk> of <unk> representing [ day of week ] and [ time of day ] , then predicting the <unk> that follow ( representing subreddit ) is like an <unk> problem in natural language processing . if you express the input as a <unk> input sequence then you can indeed use a simple feed-forward neural network to learn the <unk> output sequence representing subreddit . nn is n't necessary though ( it 's just my <unk> ) . if you just take the data points of [ time of week , subreddit ] , and plot them in 2d , then you will see clusters around certain <unk> . this is assuming you have <unk> a number with each subreddit ( which others might point out as being a bad practice unless the number assignments are not arbitrary ) . checking if an action is out of place is then like checking to see if a new point is within some distance of an existing cluster . i do n't know anything about cluster analysis ... from the perspective of finding an <unk> , i would just fit the data to something like a <unk> polynomial curve and check how far away new <unk> are from that line , relative to the standard deviation of the fitting . if you <unk> the [ time of week ] parameter , then each subreddit can simply have a probability <unk> with it based on previous statistics . so if i 've <unk> <unk> 5 times in time-slot 1 and <unk> 10 times in time-slot 1 in the past , then there 's a <unk> % chance that i 'll <unk> <unk> during the next time-slot 1 , where `` time-slot 1 '' would represent a <unk> bin like `` monday <unk> '' . the <unk> can be made less <unk> as more data is collected . <eoa> 
building an ml team for <unk> <unk> i hope this is the correct place to post and apologies <unk> if not . i am a <unk> <unk> and i run my own business . we have a large unique data set and so i am trying to build an ml team to sift through and look for possible <unk> / trends etc . i am however - not a <unk> person . what is the easiest and minimum path to <unk> <unk> understanding of <unk> to be able to have a <unk> understanding of what we should be doing as a <unk> and to be able to <unk> <unk> with the <unk> of my team . i have a <unk> in economics but i have not done math for a long time - so assume i would be starting from zero ... ... <eoq> start with this . https : <unk> <eoa> 
how to explain <unk> jump in precision i was training a single layer lstm model to do sentiment analysis on <unk> <unk> with a binary bucket . i use <unk> word <unk> and <unk> the sentence to the max and softmax to determine the loss and binary result . i set the learning rate at a pretty high level , and the <unk> <unk> around 50 % for a while , and then suddenly <unk> to 98 % in one single epoch and then <unk> at 98 % . however , <unk> does n't change much . how would someone explain such a jump ? all the training process i have seen involve <unk> increase in precision , not like this . is it possible that the prediction just shift over 0.5 line , and thus change the precision a great deal , but not that much for loss ? <eoq> did you mean corpus ? <eoa> 
how to explain <unk> jump in precision i was training a single layer lstm model to do sentiment analysis on <unk> <unk> with a binary bucket . i use <unk> word <unk> and <unk> the sentence to the max and softmax to determine the loss and binary result . i set the learning rate at a pretty high level , and the <unk> <unk> around 50 % for a while , and then suddenly <unk> to 98 % in one single epoch and then <unk> at 98 % . however , <unk> does n't change much . how would someone explain such a jump ? all the training process i have seen involve <unk> increase in precision , not like this . is it possible that the prediction just shift over 0.5 line , and thus change the precision a great deal , but not that much for loss ? <eoq> could you share any graphs or code ? <eoa> 
what is the difference between biological and artificial neural networks ? <eoq> the main difference is that a biological <unk> is <unk> <unk> . basically , if we think of <unk> as water , and a neuron as a bucket , biological information flow happens like this : imagine 4 empty buckets arranged as <unk> as possible together . now , <unk> over the <unk> of those buckets , imagine another empty bucket <unk> somehow . imagine you start to <unk> cups of water into the top bucket . in the beginning , none of that water goes into the bottom buckets . the bucket is filling , or the neuron is <unk> . imagine that each bucket is set up to tip over and <unk> all its water at once as soon as it <unk> a certain <unk> of water . so you keep pouring cups of water in , still nothing goes into the buckets below . you 're pouring and pouring . suddenly , as soon as you reach the <unk> point , the bucket tips over and <unk> all of its water into the <unk> buckets . it slowly <unk> back to normal ( i.e . <unk> and empty ) <unk> all the <unk> buckets , are only partially <unk> and none of their water gets to the buckets below them and so it continues . the cups of water you <unk> into the first neuron represent each time the neuron was <unk> from <unk> . the size of the buckets are usually the same . ( the amount of water it takes for them to tip ) . but the amount of water they <unk> is not always the same . the buckets <unk> can not <unk> with the buckets above usually . so that 's how biological neurons work in a really simplified <unk> . <unk> of buckets arranged on top of other buckets on top of other buckets . when it comes to artificial networks backpropagation maybe sort of works like the initial <unk> of the biological network in the developing mind . but it does n't really work that way once the neurons are <unk> out . also in the animal brain , there is a network for each thing our brain can do . one for <unk> , within <unk> one for <unk> , one for contrast etc , one for <unk> , one for words , one for <unk> , one for <unk> , etc , and they 're all <unk> <unk> . there is a memory <unk> that the <unk> <unk> <unk> to , it gets <unk> during <unk> , etc , there <unk> networks for <unk> , <unk> , love , <unk> <unk> , tools , actions , names , etc . the 6 <unk> <unk> in humans <unk> like a 6 layer convergence pattern . you just throw an entire <unk> <unk> of data at a sensor . the first layer just <unk> how the sensor <unk> without making any sense of it . the next layer detects patterns in that noise , the next detects patterns in that layer and so on and so forth . the result at the end is often fed into a network for concepts and then <unk> of any language . the network for <unk> is <unk> with the network for <unk> <unk> , sound etc and so forth , and that 's how we create <unk> . <eoa> 
using regression to solve for multiple variables i 'm just getting started in my first ml project . i wanted to choose something i am interested in for my first project so i decided to try to predict <unk> <unk> sports outcomes using ml . i read this paper http : <unk> and i have a few questions that are probably pretty basic . can linear regression be used to predict more than 1 variable at once ? we care about multiple variables after a game which make up a players `` score '' but only have a small amount of information <unk> to feed for a prediction . before the game we have features such as player name , <unk> , <unk> , team , team <unk> % , <unk> <unk> % , <unk> <unk> , etc . but we want to predict multiple variables such as points , <unk> , <unk> , etc . do we predict each one individually or am i missing something obvious ? <eoq> in <unk> proper , you have y = x <unk> + <unk> , with y being the <unk> vector , x being the n x p matrix of <unk> x predictors , <unk> the regression coefficients , <unk> the error vector . if you care about multiple variables that are n't <unk> of your original y , then no , because you 'd be fitting the same regression coefficients for different variables . <unk> , unless you 're certain that all predictors are useful for predicting all the different variables ( due to domain knowledge for example ) , then it may not even make sense to fit the same model for all variables . <eoa> 
is there any <unk> <unk> for unsupervised learning ? <unk> from k-means clustering and some gaussian stuff , i could not find much on unsupervised learning . are there any set of tutorials to get started with unsupervised learning ? or a book or a thesis of a phd <unk> ? <unk> . i did my <unk> share of searching before asking . <eoq> try the mooc on udacity . it is great for beginners and it is really fun ! <eoa> 
what activation function to use for convolutional layers ? i 've read that relu activation functions are used in convolutional layers , but i 'm wondering why . from playing around with a <unk> implementation , i 'm seeing that the sigmoid activation function works great but the relu and <unk> activation functions are resulting in an exploding gradient . <eoq> <unk> are the best <unk> choice , they should <unk> sigmoid if you are just doing standard cnn stuff . if you use <unk> in your case and it fails to converge , you should check your implementation . <eoa> 
i do not understand a step in neural networks , am i missing something ? so i am a programmer , getting into ai/neural networks . kind of started reading into it and a few tutorials . mostly basic stuff ( teach a few neurons to simulate/figure out how to be an and/or gate etc . ) now i get how those neural networks work , they have input , then neurons with gain and all ( ? ) inputs on them with a weight . then they provide an output , which you put through another set of neurons which connects to all of the first lines of neurons , and they generate output for all your outputs . given this thing , and say a car ( output : forward , steer left , steer right ) and input : ( distance to wall forward , distance to wall left of nose , distance to wall right of nose , distance to goal ) then write a feedback loop to reward the neural network for not colliding ( or punish collisions ) and reward it got getting closer to the goal . ( using back propagation ) now we give it an area , set it in a simple maze , simple , but required backtracking or going away from the goal to get to the goal . we let this neutal network run , give it random values , tons of iterations and generations , whatever way you like it . no matter how i imagine this thing working , i ca n't imagine any other situation then ending up with a car , that will steer straight to the goal , and stops just in front of wall , and then just sits there . doing nothing at all , because it ca n't for the life of him identify or work with the maze in order to go through . am i missing something here ? does the neural network have hidden magic i do n't get . or am i simply lacking inputs to complete the task ? i ca n't really find any good examples online to look at either . they either do basic stuff , mayor hand holding ( for the ai ) , or are insanely difficult to understand . i want to understand if a neural network can actually figure out a complicated problem like this given enough time/processing power and limited inputs and fixed outputs like described . <eoq> i think you 're correct in assuming that the system you 've described would n't really work . it 's not because of any <unk> <unk> of neural networks - i think you 've just chosen a poor objective function . <eoa> 
i do not understand a step in neural networks , am i missing something ? so i am a programmer , getting into ai/neural networks . kind of started reading into it and a few tutorials . mostly basic stuff ( teach a few neurons to simulate/figure out how to be an and/or gate etc . ) now i get how those neural networks work , they have input , then neurons with gain and all ( ? ) inputs on them with a weight . then they provide an output , which you put through another set of neurons which connects to all of the first lines of neurons , and they generate output for all your outputs . given this thing , and say a car ( output : forward , steer left , steer right ) and input : ( distance to wall forward , distance to wall left of nose , distance to wall right of nose , distance to goal ) then write a feedback loop to reward the neural network for not colliding ( or punish collisions ) and reward it got getting closer to the goal . ( using back propagation ) now we give it an area , set it in a simple maze , simple , but required backtracking or going away from the goal to get to the goal . we let this neutal network run , give it random values , tons of iterations and generations , whatever way you like it . no matter how i imagine this thing working , i ca n't imagine any other situation then ending up with a car , that will steer straight to the goal , and stops just in front of wall , and then just sits there . doing nothing at all , because it ca n't for the life of him identify or work with the maze in order to go through . am i missing something here ? does the neural network have hidden magic i do n't get . or am i simply lacking inputs to complete the task ? i ca n't really find any good examples online to look at either . they either do basic stuff , mayor hand holding ( for the ai ) , or are insanely difficult to understand . i want to understand if a neural network can actually figure out a complicated problem like this given enough time/processing power and limited inputs and fixed outputs like described . <eoq> i think understanding some more basic machine learning algorithms would help . <eoa> 
i do not understand a step in neural networks , am i missing something ? so i am a programmer , getting into ai/neural networks . kind of started reading into it and a few tutorials . mostly basic stuff ( teach a few neurons to simulate/figure out how to be an and/or gate etc . ) now i get how those neural networks work , they have input , then neurons with gain and all ( ? ) inputs on them with a weight . then they provide an output , which you put through another set of neurons which connects to all of the first lines of neurons , and they generate output for all your outputs . given this thing , and say a car ( output : forward , steer left , steer right ) and input : ( distance to wall forward , distance to wall left of nose , distance to wall right of nose , distance to goal ) then write a feedback loop to reward the neural network for not colliding ( or punish collisions ) and reward it got getting closer to the goal . ( using back propagation ) now we give it an area , set it in a simple maze , simple , but required backtracking or going away from the goal to get to the goal . we let this neutal network run , give it random values , tons of iterations and generations , whatever way you like it . no matter how i imagine this thing working , i ca n't imagine any other situation then ending up with a car , that will steer straight to the goal , and stops just in front of wall , and then just sits there . doing nothing at all , because it ca n't for the life of him identify or work with the maze in order to go through . am i missing something here ? does the neural network have hidden magic i do n't get . or am i simply lacking inputs to complete the task ? i ca n't really find any good examples online to look at either . they either do basic stuff , mayor hand holding ( for the ai ) , or are insanely difficult to understand . i want to understand if a neural network can actually figure out a complicated problem like this given enough time/processing power and limited inputs and fixed outputs like described . <eoq> try experimenting with [ genetic algorithms ] ( http : <unk> ) and [ reinforcement learning ] ( http : <unk> ) . <eoa> 
ml options for <unk> <unk> ? i am currently experimenting with being a <unk> on amazon . one of the features they have is to allow for <unk> <unk> . they allow for you to <unk> ' on certain keywords so your product appears when you search for that particular <unk> . for a particular <unk> , you can have many keywords and give them a <unk> <unk> . at the end of each day , you see how these performed through various <unk> , <unk> in the <unk> for that <unk> . i wanted to use an algorithm to help figure out after every week which keywords were worth <unk> more <unk> into . for these types of problems , what algorithms would be best suited ? are there any services ( azure , aws , etc ) that would <unk> in this research ? <eoq> i do n't want to <unk> ml , but would n't this seem to be something that could more <unk> be <unk> by just comparing things such as <unk> <unk> . ? just having some kind of <unk> of <unk> to cost would seem to do exactly what you want , and is a lot less <unk> to implement ! <eoa> 
is donald trump an artifact of overfitting ? <eoq> <unk> on noisy data , leading to regression to mean . <eoa> 
is donald trump an artifact of overfitting ? <eoq> i 'm more thinking some kind of deep <unk> in the optimization algorithm , leading to a <unk> but <unk> solution . <eoa> 
is donald trump an artifact of overfitting ? <eoq> his <unk> and changing opinions would point to it yes . <eoa> 
is donald trump an artifact of overfitting ? <eoq> does this thread <unk> in this subreddit ? <eoa> 
<unk> neural style it seems a lot of people have been <unk> the results of this paper http : <unk> , training a cnn to recreate an image in the style of another . i would like to try doing this , but i 'm not <unk> clear on how it 's done . from what i understand , the cnn is essentially trained to map randomly generated pixels to the pixels of the desired picture ? or is it much more complicated than that ? <eoq> have you checked something like this : https : <unk> ? or this : https : <unk> ? <eoa> 
validation accuracy higher when feed-forward with dropout than without i am training with dropout . if i feed the validation set forward with dropout still on the validation error is lower . if i turn it off , the validation error is far higher . ideas ? note that this issue is n't present for a shallow net , but it is for a very deep net . <eoq> are you <unk> the weights by half ? at rest time or at validation , you need to <unk> the weights by half if you 're using 0.5 as dropout probability . <eoa> 
hoping for some guidance in selecting models for feature extraction hi , i 've been studying machine learning over the last couple of months with the hope of solving a specific problem . i 'm hoping someone can help me with advice on what approach to take . i have a large <unk> of labeled , connected graphs on which i would like to do unsupervised feature extraction . ideally i want to discover higher level features present in the graphs , ( similar to these features described by <unk> ng in this [ video ] ( https : <unk> ? <unk> ) ) and be able to generate random graphs <unk> of only those features . from my research , autoencoders seem to be used for similar problems . does that sound right ? any advice or suggestions would be much appreciated . thanks ! <eoq> okay , so <unk> `` features '' of a graph and <unk> graphs can be difficult . the best way i know about to do that is based on the `` kronecker graph '' idea . you suppose your graph can be generated by kronecker product from a small graph . this actually yields excellent results , close to natural graphs . now what you want to do is estimate the generator of your graph , and the simulate other graphs from it . here is the paper that <unk> kronecker graphs , and how to estimate our generator . https : <unk> <eoa> 
get dataset used in learning to execute <unk> put up his learning to execute code . https : <unk> i want to use data from this program , but i do not know torch so i do n't know how to <unk> the data . what lines should i add to get the data , or does anyone have a set saved anywhere ? <eoq> did you read his paper ? he uses the <unk> tree bank data set . its a common <unk> . i believe the paper explicitly states how to get it from <unk> 's website . <eoa> 
need some direction for doing shared parameter regression . i have some y vs x data for a number of sub-populations in a population . for the <unk> of this question , the data for each <unk> can be <unk> using linear regression , y = m x + <unk> there are only a <unk> of unique m and b values that are shared <unk> the different sub-populations , and there are typically many <unk> unique m 's and b 's than there are sub-populations . we do n't always know a <unk> which <unk> matches which m and b values , <unk> do we always know how many unique values of each there are . for this problem it is <unk> as important to get the correct number of unique values as it is to determine what those values are . i have tried two ways to do this so far : * the first way is to decide a <unk> which sub-populations will share which parameters ( for our data this works some of the time ) . this way is very fast , but it is n't always a great model because there is no general rule for <unk> this . * the second way is to find m and b for all sub-populations , then find the unique values by using some clustering methods . this way takes much more <unk> time , which is n't great for our setup but is n't the end of the world ; and sometimes we do n't have enough data for all sub-populations to make reasonable <unk> of the parameters . ideally there is a <unk> way that both <unk> on which parameters are shared by which sub-populations and what the unique values of those parameters are . what i am looking for is some literature , or a specific topic to research on . i have prior distributions for the model parameters , so bonus made up internet points to you if you can suggest a bayesian method . <eoq> how about gradient descent https : <unk> <eoa> 
giving probability distribution as label i want to implement a neural net that will recognize images but get a <unk> probability distribution as labels while training . right now , i have it for fixed labels . now , i want to change this such that i give it the probability distribution instead of the one fixed label . can i simply do this by changing the label to a sequence of labels , where each label is one probability ? i ca n't find any information on this online . <eoq> why do n't you use your softmax layer to directly learn the distribution instead of the labels ? that 's one way to go about it . <eoa> 
what model should i use for predicting a certain value ( task <unk> time ) ? details inside . hi all , i am new to this and i recently finished a beginner course on <unk> [ this ] ( https : <unk> ) . and i <unk> that at my <unk> , i have a process which takes quite long time to complete and lot of other <unk> are dependent on this process to complete . i have the historic data of that process which gives information like it 's start date , end date and the start time and end time of the <unk> that it is <unk> of . how can use this data to predict the estimated end time of any <unk> instance of this process ? which algorithm should i use ? any pointers are highly appreciated . <eoq> does the process itself have any input ? how variable is the <unk> of the process ? <eoa> 
what model should i use for predicting a certain value ( task <unk> time ) ? details inside . hi all , i am new to this and i recently finished a beginner course on <unk> [ this ] ( https : <unk> ) . and i <unk> that at my <unk> , i have a process which takes quite long time to complete and lot of other <unk> are dependent on this process to complete . i have the historic data of that process which gives information like it 's start date , end date and the start time and end time of the <unk> that it is <unk> of . how can use this data to predict the estimated end time of any <unk> instance of this process ? which algorithm should i use ? any pointers are highly appreciated . <eoq> <unk> what is your <unk> distribution of what you want to predict ? what is the business value of the thing you are going to predict , and what representation would be most useful to those people ? what would they do with the answer ? <eoa> 
rnn - vanishing or exploding problem i 'm trying to understand an exercise from [ hinton 's course on neural networks ] ( https : <unk> ) . [ complete exercise ] ( http : <unk> ) basically , i need to know if it 's a vanishing or exploding problem . so what i did was first calculate all the partial derivatives corresponding to `` chain <unk> '' ∂e/∂wxy and search for some light there : ∂e/∂wxy = ∂e/∂y * ∂y/∂h3 * <unk> * ∂z3/∂h2 * ∂h2/∂z2 * ∂z2/∂h1 * ∂h1/∂z1 * ∂z1/∂wxh calculating each component : ∂e/∂y = - ( <unk> - y ) = - ( 0.5 - y ) ∂y/∂h3 = why = 1 ∂h3/∂z3 = h3 ( 1 - h3 ) ∂z3/∂h2 = whh = -2 ∂h2/∂z2 = h2 ( 1 - h2 ) ∂z2/∂h1 = whh = -2 ∂h1/∂z1 = h1 ( 1 - h1 ) ∂z1/∂wxh = x1 so : ∂e/∂wxy = - ( 0.5 - y ) * 1 * h3 ( 1 - h3 ) * ( -2 ) * h2 ( 1 - h2 ) * ( -2 ) * h1 ( 1 - h1 ) * x1 i saw nothing there . so i <unk> <unk> to <unk> as the exercise says , and this is what i found : ∂y/∂z3 = ∂y/∂h3 * ∂h3/∂z3 ∂y/∂h3 = why = 1 ∂h3/∂z3 = h3 ( 1 - h3 ) so : ∂y/∂z3 = 1 * h3 ( 1 - h3 ) what i see there : - h3 is the logistic function , output always will be between ( 0 ; 1 ) - 1 minus something between ( 0 ; 1 ) is something between ( 0 ; 1 ) - and something between ( 0 ; 1 ) multiplied by something between ( 0 ; 1 ) is something between ( 0 ; 1 ) so in <unk> , not only it will be something between ( 0 ; 1 ) , but it will be pretty small because the multiplication in ∂y/∂z3 , and then it will be much smaller because in ∂e/∂wxy , that logistic function appears 3 times ( one for each logistic hidden unit ) multiplying together , and that will shrink the whole gradient a lot , independently of the other terms . my question is , am i correct ? and when can it be an <unk> problem ? because as i see here and with that logic ( that maybe is wrong ) , it 'll always be a vanishing problem . <eoq> to my knowledge , as long as you 're using the sigmoid activation function then you 'll only have the vanishing gradient problem . notice that the the derivative of the sigmoid produces outputs on the range ( 0 , <unk> ] . every time you <unk> you 're multiplying by the derivative of the sigmoid , and since it 's always a <unk> you 're always making it smaller . <eoa> 
rnn - vanishing or exploding problem i 'm trying to understand an exercise from [ hinton 's course on neural networks ] ( https : <unk> ) . [ complete exercise ] ( http : <unk> ) basically , i need to know if it 's a vanishing or exploding problem . so what i did was first calculate all the partial derivatives corresponding to `` chain <unk> '' ∂e/∂wxy and search for some light there : ∂e/∂wxy = ∂e/∂y * ∂y/∂h3 * <unk> * ∂z3/∂h2 * ∂h2/∂z2 * ∂z2/∂h1 * ∂h1/∂z1 * ∂z1/∂wxh calculating each component : ∂e/∂y = - ( <unk> - y ) = - ( 0.5 - y ) ∂y/∂h3 = why = 1 ∂h3/∂z3 = h3 ( 1 - h3 ) ∂z3/∂h2 = whh = -2 ∂h2/∂z2 = h2 ( 1 - h2 ) ∂z2/∂h1 = whh = -2 ∂h1/∂z1 = h1 ( 1 - h1 ) ∂z1/∂wxh = x1 so : ∂e/∂wxy = - ( 0.5 - y ) * 1 * h3 ( 1 - h3 ) * ( -2 ) * h2 ( 1 - h2 ) * ( -2 ) * h1 ( 1 - h1 ) * x1 i saw nothing there . so i <unk> <unk> to <unk> as the exercise says , and this is what i found : ∂y/∂z3 = ∂y/∂h3 * ∂h3/∂z3 ∂y/∂h3 = why = 1 ∂h3/∂z3 = h3 ( 1 - h3 ) so : ∂y/∂z3 = 1 * h3 ( 1 - h3 ) what i see there : - h3 is the logistic function , output always will be between ( 0 ; 1 ) - 1 minus something between ( 0 ; 1 ) is something between ( 0 ; 1 ) - and something between ( 0 ; 1 ) multiplied by something between ( 0 ; 1 ) is something between ( 0 ; 1 ) so in <unk> , not only it will be something between ( 0 ; 1 ) , but it will be pretty small because the multiplication in ∂y/∂z3 , and then it will be much smaller because in ∂e/∂wxy , that logistic function appears 3 times ( one for each logistic hidden unit ) multiplying together , and that will shrink the whole gradient a lot , independently of the other terms . my question is , am i correct ? and when can it be an <unk> problem ? because as i see here and with that logic ( that maybe is wrong ) , it 'll always be a vanishing problem . <eoq> i 'm <unk> na add this here just in case someone has a similar question . so yes , that <unk> was correctly , and could find it [ on wikipedia ] ( https : <unk> ) ( <unk> ... ) : > <unk> activation functions such as the <unk> <unk> function have gradients in the range ( -1 , 1 ) or [ 0 , 1 ) , and backpropagation <unk> gradients by the chain rule . this has the effect of multiplying n of these small numbers to compute gradients of the `` front '' layers in an <unk> network , meaning that the gradient ( error signal ) decreases <unk> with n and the front layers train very slowly . * i suppose that the exploding problem <unk> only when other kind of activation functions are used , that allow values over 1 , that multiplying together over <unk> can `` <unk> '' into some really big numbers . <eoa> 
<unk> formulation of svm model - question i 'm going through the <unk> lectures on machine learning and am confused by one step in the calculating the solution to the svm model . in <unk> <unk> ( link below ) , why is there a minus <unk> in front of the <unk> ? i thought that for <unk> of the form g ( x ) > =0 , you add a <unk> * g ( x ) term to the <unk> , not <unk> * g ( x ) . <unk> 9 shows that the <unk> is of the form g ( x ) > =0 . i must be missing something small . http : <unk> <eoq> that 's because if you have an optimization problem <unk> f ( x ) subject to h ( x ) < = 0 , the <unk> will be l = f + lambda * h where the lambda ( i ) are positive . here with h = - g , he used - lambda with lambda ( i ) > = 0 instead of <unk> with lambda ( i ) < = 0 for <unk> . <eoa> 
using bayes classifier in home <unk> to build an <unk> like <unk> hi , i have a lot of connected <unk> at home , i can <unk> everything from my phone on a <unk> i made , but i would like to go further . my goal is to be able to text or speak with my home system , like a kind of <unk> ( with very basic commands to begin ) . i think machine learning can be a good idea to implement that , using a bayes classifier ( i found this awesome library to do that : https : <unk> # classifiers ) my goal is to train the system with sentence in input ( `` turn on the light '' for example ) , and in output the action ( `` <unk> '' for example ) , and then use the trained system to detect what i want to do when i speak to my <unk> . do you think that 's a good way of doing this ? thanks a lot , <eoq> <unk> ? <eoa> 
why is this nn so <unk> ? <eoq> it 's an lstm put together with jupyter , <unk> and <unk> . the <unk> ( <unk> lines ) should have distinct steps in them , like the y ( <unk> ) but it does n't seem to matter what <unk> are , it just <unk> fit <unk> . [ code is here ] ( https : <unk> ) and [ sample data generator here ] ( https : <unk> ) <eoa> 
python <unk> <unk> <unk> when reading from a csv file . i 've created an empty <unk> and want the most <unk> values of a <unk> column ( attribute ) to populate the <unk> . could someone please explain why this is <unk> ? def gain ( data , attr , <unk> ) : `` `` '' calculates the information gain ( reduction in entropy ) that would result by splitting the data on the chosen attribute ( attr ) . `` `` '' val_freq = { } <unk> = <unk> # calculate the frequency of each of the values in the target attribute for record in data : if record in val_freq [ record [ attr ] ] : val_freq [ record [ attr ] ] <unk> 1.0 else : val_freq [ record [ attr ] ] = 1.0 <eoq> you are testing if <unk> is in <unk> [ record [ attr ] ] ` without being sure if <unk> [ attr ] ` is in <unk> . what you might want to do is to check : if record [ attr ] in val_freq : also you should make sure if <unk> is actually in <unk> . i do n't know how exactly your data looks like , but that could be another error waiting to happen . <eoa> 
machine learning for <unk> scans i have a set of <unk> scans that i 'd like to use for <unk> classification and <unk> classification ( likely a regression problem ) . is it possible to use networks like cnns given that the data is 3 <unk> ( a set of horizontal slices which put together form a <unk> array ) ? and if not what is the best <unk> to go about this ? <eoq> there 's no inherent reason why cnns ca n't work on <unk> data - see this link for some examples . https : <unk> a big problem with <unk> data though is that you may not have enough per class ( or output value in your regression case ) to train the cnn effectively . cnns are <unk> for <unk> a very large amount of training data . they learn to extract the most important image features per target class , but because images have a large amount of <unk> in image data , this means <unk> several <unk> ( at the very least ) images per class to avoid the network from overfitting to <unk> image details . you could perhaps get away with <unk> images if you can do some pre-processing ( e.g . if your <unk> scans are of the brain , perhaps map them <unk> a <unk> brain template ) to reduce the <unk> . without knowing more about your problem , i think your best bet is to go down the traditional <unk> of <unk> out a set of features that you can compute from the images , and then using them to train a normal ( i.e . <unk> ) classifier . but feel free to <unk> me if you 'd like to <unk> some ideas around . <eoa> 
how do you guys download massive datasets ? how are you guys getting massive datasets ? anything over <unk> which is the <unk> by popular <unk> 's like comcast ? <eoq> if you work at a university or company , ask your it <unk> . otherwise i do n't really know . from what i hear about comcast , the <unk> will probably be <unk> , but maybe you can ask them to make an exception because you 're using the data for ( <unk> ) research or something . <eoa> 
how do you guys download massive datasets ? how are you guys getting massive datasets ? anything over <unk> which is the <unk> by popular <unk> 's like comcast ? <eoq> i 'm not sure what datasets you 're looking at , but you could always ask the <unk> to split it up into separate files , and then put them back together yourself . <eoa> 
trying to match transactions with receipts - where to start i 'm trying to match receipts ( ocr <unk> ) with their <unk> entry from my credit card <unk> . i have training data for receipts i already <unk> . but where to start ? and can i use an existing <unk> api ? <eoq> are you trying to automatically <unk> your <unk> with your receipts ? <eoa> 
machine learning , deep and wide data , how to start ? this is a very general overview question that i have not seen <unk> <unk> , so any pointers to literature is appreciated . i have been using scikit-learn with some success on a variety of ml problems . generally a main challenge is just to get the data <unk> and <unk> for <unk> to take it in , ie categorical variables etc . now have a much larger and <unk> dataset of <unk> related data to process for <unk> and there are multiple data <unk> per patient which are very wide and related . eg a subject ( patient ) will exists in many <unk> . each table is quite wide with many possible <unk> and once i <unk> the variables for categories it will really <unk> out . but that is not even the worst part , we also have encounter data with <unk> <unk> and <unk> , so many records for a patient , this table is also very wide and may have dozens or more records per patient . date order may be <unk> sequential <unk> . any suggestions for how to think about <unk> the data for <unk> ? is there a library for processing both deep and wide data ? or how should i be thinking about this ? i 'm currently working with a very small subset 500 persons , but the result will be applied to very large <unk> , <unk> 's if not <unk> . <eoq> a bit more research is <unk> me to hmm and rnn approaches similar to those used for natural language processing , at least for the `` <unk> , sequential '' <unk> of the <unk> <eoa> 
necessary math background ? considering taking a course in ml and trying to evaluate what specific math skills would be requisite . <eoq> calculus , linear algebra , probability , and statistics <eoa> 
necessary math background ? considering taking a course in ml and trying to evaluate what specific math skills would be requisite . <eoq> if this is in a university , try writing the <unk> and asking for a <unk> or at least what text they use . machine learning is a growing field and any one class will have different requirements than another . a lot will depend on whether it 's more theoretical or applied . <eoa> 
necessary math background ? considering taking a course in ml and trying to evaluate what specific math skills would be requisite . <eoq> i took an upper level undergrad course in ml and did well enough even though my grasp of math is n't great ( i was pretty much <unk> the linear algebra as i <unk> ) . the most <unk> linear <unk> i ended up doing is <unk> the <unk> of a matrix , which is super easy . also you need calculus , but only derivatives ( in fact , as i recall , mostly only partial derivatives ) and they 're always super simple . in this case i think the most complex thing you 'll need to understand is the chain rule . be able to do it ( it 's easy once you understand it ) . for probability you need to know bayes <unk> ( which every human being should know <unk> ) , and understanding the <unk> of a probability distribution is good . but the best thing you can do is talk to your <unk> , tell him your math background , and ask for his opinion . <eoa> 
kmeans clustering library ( x-post from <unk> ) does anybody know of a library in python that allows the user to change the distance ( similarity ) function for kmeans clustering to a user defined function ? i 've been using <unk> but they do not allow the user to do that . thanks in advance ! <eoq> ideally the answer to this is to not use k-means ; there are potentially much better clustering algorithms ( depending on what you want to do exactly of course ) . any algorithm that <unk> a distance matrix as input ( with <unk> ' in sklearn ) allows arbitrary distance functions ; as long as you have a small enough dataset ( or enough memory ) that computing the full distance matrix is <unk> then this is the easiest solution . in practice if you have enough data that this is n't a <unk> solution then a user defined function for distance <unk> is going to likely result in <unk> <unk> : the reason that many algorithms <unk> a limited set of distance functions is that those are the functions that have been <unk> <unk> ( usually via <unk> , or c libraries ) ; the <unk> of going out to a python <unk> function for every distance call is going to be huge in the long run . could you <unk> a little more about your problem and what you 're trying to do ( and why you chose k-means ) ? i might be able to recommend some better alternative algorithms . <eoa> 
how to <unk> ml <unk> ? for text log classification . so i 've spent time this week on <unk> filters and field <unk> for <unk> to read my log files and <unk> the logs and extracted fields into <unk> . my application is very <unk> and i was <unk> out the `` normal '' errors to better identify actual issues , so i 've <unk> been identifying patterns of the most common <unk> log entries to end up with the more rare ones . i <unk> the progress to <unk> , and one asked if machine learning could do the classification for me and free me up to better interpret the meaning . <unk> ... . so a few <unk> internet pages later ... i 'm wanting to <unk> mahout or <unk> to <unk> the <unk> , feed it some logs and see if i can figure out what to ask next . but much of the help material on <unk> on a cluster . i just want to set something up on a single machine and feed it up to a <unk> of log files and see what it i can do with it . so am i on the right track ? can mahout or <unk> run on a single machine , or should i be looking at something else ? <eoq> <unk> over the <unk> so far . but i got apache spark <unk> and doing a couple of simple things on a single machine , and the actual steps are n't difficult at all : - have java with <unk> set <unk> - [ download spark ] ( http : <unk> ) <unk> with <unk> client - <unk> the spark <unk> into a <unk> - run things in the bin <unk> on <unk> there were some errors even though there are <unk> versions of the commands in bin . i think i need extra libraries . but on a <unk> ubuntu <unk> <unk> <unk> java 8 and spark it 's running with no extra steps so far . [ this page ] ( http : <unk> ) has some example commands . [ this section showing language classification of tweets ] ( https : <unk> ) ( youtube <unk> included ) is where i 'm going to start my <unk> . it <unk> <unk> and classifying tweets into clusters that end up being more or less language <unk> , but i think this can do what i 've been trying to manually do : classify log entry types into clusters , and then i can focus on the small clusters as rare log entry types . i think my steps are going to be : - use my existing <unk> field <unk> against a couple of <unk> days ' logs - store that in some intermediate data store ... the tweet exercise uses sql ; with my relatively small data set i 'll see if i can use text <unk> or just pull it back out of <unk> . if those <unk> , mongodb ? - <unk> the log text . i may <unk> the <unk> and thread <unk> info ; or try both with and without . i 'll probably start with the [ <unk> ] ( http : <unk> # tf-idf ) method as the example uses , but the [ word2vec ] ( http : <unk> # word2vec ) method looks worth trying out for this purpose to my <unk> <unk> . - do [ k-means clustering ] ( http : <unk> # k-means ) against the <unk> log data - ... - <unk> - well , actually then i 'll see what size each cluster is and look for <unk> or <unk> cluster ( if that 's a thing ) against the rest of the log data . <eoa> 
<unk> image data to two channels for grasp detection i 'm <unk> transfer learning on an imagenet trained network to train a cnn for grasp location ( via regression ) . i 'm using image and depth data from a <unk> . to use an imagenet trained model , the network needs to be 3 channel , and i 've done this <unk> by <unk> <unk> away the blue channel to go from <unk> to <unk> . i have a working network from this , but i think it was a <unk> solution . i 've been thinking how for <unk> <unk> , <unk> is <unk> and in fact could lead to biases as my dataset is relatively small . in light of this , i was thinking i could convert rgb to <unk> and throw away the h channel . this <unk> away <unk> information but <unk> <unk> , which i think it more important for learning grasp features . is this thinking sound ? <eoq> if you have to choose 3 channels from <unk> , then it indeed sounds like <unk> is probably the least important for you . however , i 'm a little concerned about how well transfer learning is going to work if you change the features . so i could imagine <unk> might work better than <unk> , because those only `` <unk> up '' one channel . if you 're training the imagenet network yourself , then i suppose you could use <unk> from the start , and avoid this a bit . it might also be possible to just <unk> the original network with a <unk> channel . you can just add the connections for that if you wanted . if you think these could also benefit from transfer learning , you could initialize the weights based on those of the other channels . for instance , you could use their average . and then maybe multiply everything by <unk> to keep the size of the inputs to the next layer roughly equal . basically i do n't know what will happen , so you may just have to try different options . that 's usually the answer in these cases , but i hope it wo n't be too much <unk> / training time here . <eoa> 
single feature learning useful ? hi , i 'm currently working on a machine learning project in university . my <unk> <unk> me to split out features into <unk> and single features to compare their performance . while i understand <unk> , i ca n't see the point in making all <unk> series with <unk> of single attributes . is this common ? do you think it 's <unk> necessary ? <eoq> i think it 's pretty common to make <unk> for independent and dependent variables to <unk> how they appear to <unk> . i 'm not sure about <unk> single feature learning . what i 've seen more often are <unk> studies where you start with all of your features and then see how performance <unk> when you <unk> each feature . <eoa> 
i do n't know what you 're doing exactly or why you 're using neural network specifically , but have you not heard of scikit-learn , specifically their [ regression page ] ( http : <unk> # supervised-learning ) ? <eoa> 
i teach a bunch of courses on regression and classification using various linear and deep nets , and i find that the actual math you need to understand what 's going on is usually too much math for those who want to focus on the practical <unk> . ( at least from what i remember , you should have learned about <unk> , gradients , and the chain rule in undergrad ) . that said , why do n't you just try a more modern library like theano or tensorflow ? you would n't need to calculate gradients yourself , and it contains <unk> for more recently developed techniques . so using them <unk> is just a matter of reading and understanding the documentation . <eoa> 
inference stage in batch <unk> network i came across this paper http : //arxiv.org/abs/1502.03167 and it state that <unk> ( per batch ) the layer would <unk> in faster learning step . batch <unk> make sense in training however it 's not in inference step . as the matter of fact , section <unk> suggest that it 's <unk> . however , i am confused as on how do we set the mean and variance during inference stage . the <unk> suggest to simply use the mean of variance / mean during training . how many variance / mean should we <unk> during the training stage ? obviously using all of them wo n't make sense in this case . <eoq> your options are : * <unk> the weights , then calculate the <unk> by running through all the training data again . use these values for inference . * track a moving <unk> of <unk> during training , then use these values during inference . the second option is the one most people go with . <eoa> 
classification with numerical labels ? i have a dataset in which each row has information about who , where , when and how much a customer has <unk> in certain products . i was wondering if i could make a predictive model in which given the `` who '' , `` where '' and `` when '' i could predict how much <unk> is this customer <unk> . <eoq> what is your question ? yes , you might be able to build a model for that . how much data do you have ? what is important to discover ? do you want to use a certain algorithm or just solve the problem ? <eoa> 
a confusion <unk> kernels i fairly understand kernels in machine learning , how algorithms are <unk> and i also understand how kernels in image processing work , as they do in <unk> and in filters . but i ca n't help but wonder if they are related . for some time i began to relate the <unk> kernel to be some function to transform the image vector into a new feature space and all , but i 'm unable to bring out any <unk> . could someone help me . <eoq> i have been asking myself what is a kernel for some time now . i thought i understood it when it came to image processing , but then i started doing ml and i too could n't see the <unk> . looking up the <unk> on google , maybe that will help us : kernel - `` the <unk> or most important part of something '' . <eoa> 
machine learning vs. data mining , and recommendations for someone with no background in this ? hi machine learning . i 'm a <unk> student in an <unk> field and have an idea for my dissertation . it will involve <unk> big data and automated analysis to make recommendations . my <unk> areas involve big data <unk> . i do n't have any background with machine learning , statistics , or data mining . i am <unk> and <unk> in my area of study , and am willing and <unk> to learn about machine learning or data mining . i 've been researching which subset of big data i should be using for my dissertation , but have ended up more confused than before . for example , [ this reddit post ] ( https : <unk> ) <unk> to explain it , but everyone 's <unk> is different . basically , i need to begin studying one of the big data <unk> to get my dissertation started , but i 'm not sure which one i need . essentially , i 'm looking to <unk> a big <unk> <unk> mining approach that : * takes information about a large number of data i have collected * <unk> patterns with the data * makes configuration recommendations to me based on the rules i set * <unk> : <unk> the implementation of the `` best '' recommended <unk> is this data mining , machine learning , or something else ? any other recommendations for someone with no background in mathematics , statistics , or big data ( but can program , perform the other <unk> pieces , and <unk> about any <unk> i need to ) ? <eoq> the <unk> between data mining and machine learning is pretty <unk> and there is a ton of <unk> so i would n't worry too much about it . your question is pretty light on details ( what kind of data do you have ? what sorts of patterns are you looking for ? what kind of rules will you create ? are you planning on using the <unk> patterns to make rules ? ) so i 'm not sure how helpful the <unk> of my answer will be . if you provide more detail about the specific problem you are trying to solve there 's a chance someone here could provide some more helpful advice . since you are n't <unk> about dm/ml trying to find the correct way of <unk> the problem so that it can be <unk> by dm/ml will be impossible . there are many kinds of problems that have been <unk> quite a bit that are n't <unk> in introductory <unk> so <unk> on the standard <unk> is n't necessarily a good idea . you may very well start learning about <unk> and spend <unk> of study only to find out it is n't <unk> or to realize that you 've been studying the wrong <unk> of the field . some of what you are doing ( identifying patterns in data ) sounds like it would benefit from <unk> techniques , while other things ( making recommendations based on user defined rules ) does not ( if you were trying to get a computer to learn potential rules from data you should look into <unk> rule mining ) . <unk> you should probably try to find previous work in the field solving similar problems and look at the sorts of techniques they employ and use that as a <unk> off point for further background reading . ** if you are trying to automate the configuration of <unk> then search something like `` automated database configuration '' in google <unk> . if you want to try to learn about dm/ml your best bet would be to [ <unk> some of the introductory links on /r/machinelearning ] ( https : <unk> ) and read the first couple <unk> of one of the <unk> listed there or watch the introductory <unk> from a mooc so that you have a basic idea of what sorts of dm/ml tools are out there . but <unk> are good that your problem wo n't fit quite right into the basic <unk> <unk> in introductory <unk> so it is important that you look at previous work on problems similar to the one you are trying to solve . <eoa> 
how to recognize fields in web pages ? hi , i have got into machine learning recently and i would like to ask advice on a couple of questions : 1. how do you recognize <unk> ' in <unk> web page data ? for example , i have 2,000 web pages about the same topic and i want to to recognize the top 5 fields contained on each page . lets say that my <unk> pages dataset is about cars ( taken from the popular car <unk> websites ) . then the output for the 5 fields would be : > > * car manufacturer : <unk> > * color : blue > * car type : <unk> > * engine : <unk> > * <unk> : 6 > but , for example the field <unk> of <unk> ' would not <unk> the <unk> 5 ' fields list because , lets say , only 100 pages are talking about it , so , <unk> , it is not included . what is the sequence of <unk> to achieve this and what open source package would you recommend me to use ? i have found tools for topic <unk> and classification , but they seem to be focusing on some specific fields , like name <unk> , or places , but what i want is to <unk> detect the <unk> 5 ' fields . 2 second question , if i may , of course : how do you take advantage of already <unk> knowledge <unk> contained in html <unk> and <unk> webpage structure ? for example , the car model , can be already <unk> <unk> the <unk> ' <unk> of the html page describing the car . or maybe you do n't need to extract anything because there is already an html table with a lot of fields . how do you extract this knowledge from html ? but , you could not <unk> on it completely , because every web page will have different html template , so , i suppose , you must first scan the website fully , identify its template and then , extract the data from <unk> . i am correct ? do you know any tools that already <unk> html to <unk> it as an input for machine learning algorithms ? thank you very much in advance <eoq> found this : https : //www.youtube.com/watch ? <unk> maybe i could feed the html directly to the rnn , having such a complex <unk> logic it may understand the patterns and extract the data i need without any html preprocessing ? <eoa> 
small project on evolutionary algorithms / machine learning i have a small university project <unk> evolutionary algorithms in which i will work on the [ <unk> dataset ] ( https : <unk> ) that 's part of the current kaggle competition . the idea is : 1. train some ( simple ? ) learning algorithms on the dataset / <unk> of the data 2. create an ensemble <unk> by <unk> the different trained models , so we get a prediction based on all the base models . 3. repeat a lot : use evolutionary methods to find good weights for step 2 . the focus of the project is to try different approaches in step 3 for selection , <unk> , <unk> and evaluate which one works best . of course this might be easier on another dataset , but taking part in kaggle competitions is super fun : ) what i am still unsure about is <unk> base <unk> i should <unk> and this is my question that i hope you can help me with . i know that a random forest approach works well in many <unk> , so simply training a big amount of trees might work . however , since the dataset contains numerical and <unk> data , as well as binary <unk> data , i think it should be beneficial to use different learning methods that are able to handle those <unk> types of data well and combine them . what do you think ? which models should i try to combine ? thanks for your help ! edit : the task is predicting the probability of which basketball team wins a given <unk> , so i 'm looking for regression models . <unk> : after looking into this some more , i <unk> that what i want to do is called <unk> . i am probably going to first try a big <unk> of regression trees ( which i guess would <unk> random forest ) . <unk> , i will try to combine different base models , so i will try to find good weights for a combination of a regression tree ( or multiple ones ? ) , an svm , maybe mars , maybe ann . if you have any more suggestions on this , i 'd be very happy to receive more suggestions : ) <eoq> if i understand correctly you have to predict some kind of probability , right ? that means you need to do regression ( as opposed to classification ) . regression trees should work fairly well . they should be able to handle both <unk> and <unk> data . maybe [ mars ] ( https : <unk> ) ... if you want to use something like a neural network , it deals fairly <unk> with numerical data ( although you may want to normalize ) . you can use one-hot encoding for <unk> data ( if you have one variable that can take on n classes , create n corresponding input nodes and turn all off <unk> for the one that corresponds to the variable 's current value ) . for ( <unk> ) <unk> data you can use a <unk> encoding , so if there are n possible values , create <unk> nodes , and if the current value is the <unk> , turn on the first two nodes . ( so if you have temperature which can be cold , warm , or hot , then make 2 nodes and turn them all off for cold , turn the first on for warm and turn both on for hot . ) these <unk> might also work for other algorithms . <eoa> 
quick question about my facebook dataset classification <unk> <unk> ... take classification data and result in a binary dependent variable . for example : monday : <unk> , warm and windy . i go and play tennis tuesday : <unk> , warm and windy . i do n't play tennis <unk> : <unk> , cold and not windy : i go and play tennis . from this i predict whether i will play tennis on a given day . the data i was provided with is also data in classes . it is a facebook data set ( with all data from different facebook <unk> ) . the dependent variable is whether a person will like a page or not . some of the variables in the dataset have <unk> classification values per user per variable . for example : user 3 <unk> english , <unk> and <unk> . so this gives me 3 values for the language variable ( 3 rows . ) how do i tackle this ? this is the case for a lot of variables in my dataset <eoq> instead of a single `` languages <unk> '' variable which can have multiple values , you turn this into multiple variables ( `` english '' , `` <unk> '' , `` <unk> '' and more if other people in your data speak other languages ) with binary values . <eoa> 
<unk> k-means using neural networks . by all <unk> , k-means is a numerical method for unsupervised clustering . i 'm looking at doing k-means using neural networks because i believe there would be a speedup in doing so . unfortunately , i 'm having trouble dealing with output representation . for this , self organizing maps were recommended . can someone <unk> through self organizing maps ? ( explain it like im 10 ) <eoq> first of all , you should know that k-means and self organizing maps ( soms ) are different things and soms are not just a faster way to calculate a k-means clustering or something like that . the results for small soms will be similar to k-means though . in a som you have a number of <unk> that are <unk> into a <unk> point <unk> ( almost always a 2d grid ) . if you have <unk> data ( i.e . each data point has m features / values ) , then each node in the som has m weights or parameters . in that sense , these nodes could be compared to the cluster means in k-means . when the network is trained , each new data point is simply <unk> with the most similar node ( just like in k-means ) . to train the network , you take a data point d and calculate the distance to each node <unk> usually this just means the <unk> distance . remember those m values that the nodes and data points have ? just <unk> the d 's values from n 's values , square each result , sum them together , and take the square <unk> . we will call the closest / most similar node c . now we want to move c , and the nodes that are close to c ( on the grid ) , even closer to <unk> we want to pull c and nodes that are very close to it very hard in the direction of n , and pull nodes that are further away a bit <unk> . to do this , we define a similarity function called the `` <unk> function '' . a similarity function is basically the opposite ( <unk> ) of a distance function . if two nodes are the same , it should be 1 , and if they could not be more different , it should be <unk> one way to do this is to calculate the euclidean distance ( on the grid , not of their feature vectors ) , <unk> it into a [ gaussian function ] ( https : <unk> ) and take the absolute value ( if it 's negative , multiply by -1 ) . you can start out with a really wide gaussian ( high value of <unk> on that wikipedia page ) and make it <unk> when you 've been training the som for a long time . now we 're going to pull <unk> node n towards the current data point <unk> the amount by which we change n 's parameters is determined by the difference with d 's features , multiplied by the <unk> function , multiplied by the current learn rate . the learn rate is a number between 0 and 1 that <unk> how fast we should pull each node towards the current data point . you want this value to be high when you start training and then decrease over time . you do this for all of the data points in your data set , and then keep <unk> that until you are <unk> with the result . what you end up with is not just a <unk> from data points to a cluster as in k-means , but also a <unk> to a <unk> space ( the <unk> point <unk> / grid ) . this means you could also use it for dimensionality reduction ( like e.g . <unk> component analysis ) . <eoa> 
trying to predict my manger 's arrival times . i 've collected data that records when my boss walks through the office e.g . 9am , 9:15am , 9:12am , 9:05am , 9:30am and i 'd like to predict what the next values may be to make sure i 'm at my desk as often as possible during his next walkthroughs . is this just a case of simple linear regression to detect if there is a trend and just extrapolate forward and make sure i 'm at my desk during the average of those times or at least within 1 standard deviation ? is there something more advanced in the machine learning area that can deduce something interesting about this data to help me ? <eoq> what are all factors you recorded ? <eoa> 
trying to predict my manger 's arrival times . i 've collected data that records when my boss walks through the office e.g . 9am , 9:15am , 9:12am , 9:05am , 9:30am and i 'd like to predict what the next values may be to make sure i 'm at my desk as often as possible during his next walkthroughs . is this just a case of simple linear regression to detect if there is a trend and just extrapolate forward and make sure i 'm at my desk during the average of those times or at least within 1 standard deviation ? is there something more advanced in the machine learning area that can deduce something interesting about this data to help me ? <eoq> check the <unk> correlation and see if there is a potential to use an <unk> model ( using the minutes after 9 for each day as a time series ) . to get more fancy try random forest regression and add some categorical variables for <unk> , long <unk> , sports events etc the <unk> before . <eoa> 
trying to predict my manger 's arrival times . i 've collected data that records when my boss walks through the office e.g . 9am , 9:15am , 9:12am , 9:05am , 9:30am and i 'd like to predict what the next values may be to make sure i 'm at my desk as often as possible during his next walkthroughs . is this just a case of simple linear regression to detect if there is a trend and just extrapolate forward and make sure i 'm at my desk during the average of those times or at least within 1 standard deviation ? is there something more advanced in the machine learning area that can deduce something interesting about this data to help me ? <eoq> trying to not get too far into <unk> you can use this as an added component . http : <unk> ? <unk> i second adding <unk> info ( <unk> , day of week , etc ) you can <unk> <unk> <unk> data if you think they might be using <unk> instead . you 'll then get discrete <unk> when the <unk> general <unk> <unk> with their <unk> into your <unk> . however , i think you already <unk> their <unk> down to a <unk> <unk> , is n't that good enough ? <unk> <eoa> 
using ml to determine whether a webpage is an article or not ? does anyone know if this sort of work has been done or not and if there are any good labeled data sets to work with ? any <unk> on where i can look to start <unk> this problem would be great . <eoq> if there 's nothing existing , what scraping a <unk> site 's <unk> , like <unk> , to get a <unk> ton of example articles ? then maybe a site like reddit for <unk> examples . i bet this takes less than a few hours depending on how much web scraping you 've done , since <unk> already existing reddit scraping <unk> <eoa> 
svm kernels are like <unk> ? in the machine learning coursera course , andrew says that different kernels are <unk> by different similarity functions . he then gives an example of the gaussian kernel which is just a similarity function . so basically different kernels are different similarity <unk> ? <eoq> yes , in a sense . they are <unk> products that make <unk> easier and they do look for <unk> for all <unk> and purposes . kernel functions are actually ( or should be ) based on the domain knowledge about how the data should appear . is it linear ? polynomial ? <unk> ? that 's basically the best option for choosing kernel functions , although there are some automated functions out there now . <unk> 3 links i enjoy on kernel functions : [ <unk> ] ( https : <unk> ) [ great visual representation ] ( http : <unk> ) [ basically all kernel functions ] ( http : <unk> ) <eoa> 
how to learn user behavior ? hi , i 'm a absolute beginner in this field and i do n't have a plan to solve my problem ( it is n't a real problem , i 'm only interested in this ) . okay i have a android app with 3 buttons `` a '' , `` b '' and `` c '' . every weekday between 9am and <unk> the user clicks on button a . but every weekday between <unk> and <unk> the user clicks on button b . at <unk> the user clicks on button c . but on <unk> between <unk> and <unk> the user clicks on a . what i want is that my android app learns this <unk> behavior and <unk> the buttons depending on the time . i played a <unk> bit with apache mahout example [ 1 ] but i 'm not sure if it is the best solution the recommend the right button . [ 1 ] ... https : <unk> <eoq> it does n't sound like you need machine learning for this , why not just keep track of what times which buttons are most frequently <unk> , and use that ? <eoa> 
question <unk> system where to start ? assuming i have a raw dataset with thousands of questions and answers , what would be the best way to tackle a system that is <unk> enough suggest possible answers for similar questions ? are there any libraries or systems that can already do this and that i can build on top ? cheers ! <eoq> you might take a look at the papers we covered in an <unk> course last term at <unk> ( <unk> <unk> 's class ) . we started by <unk> recurrent neural networks and <unk> . the papers starting <unk> <unk> specifically covered <unk> systems . http : <unk> <eoa> 
can someone explain to me or provide me with a practical application of the use of a perceptron learning algorithm ? <eoq> the <unk> is useful in that it <unk> the basic structure of most learning algorithms ... predict , feedback , update ... <eoa> 
<unk> dimension inputs for nn how can i structure inputs to an nn so that trends within each day are not lost . my inputs are a 2d array for each weekday . y is a single real for each day that is ultimately the bottom right value from that days <unk> i want to get an updated y as each vector of x <unk> by . just <unk> in the <unk> one after another , my nn is not learning the <unk> within each day . i 've been experimenting with <unk> features , lambda and the number of hidden units without success . seems i have a bias problem . i <unk> i need an rnn or lstm to factor in the memory but i still <unk> know how to flag each day as a self contained training example ? <eoq> you could do a <unk> rnn , then every hidden state of the rnn represents a feature in the both the forward and <unk> <unk> , then use these as features for upper level stuff . think you 'd want to do this over your week dimension . this might be <unk> though . <eoa> 
should i get into ml ? i 'm not <unk> sure how i should proceed and looking for some advice . i 'm an <unk> who is looking to create a neural network and train it to my taste ( taste , from what i understand , is the way our neurons are <unk> which are <unk> based on experience ) and have it search for art for me . i 'd basically train it to art pieces i like and ones i do n't like . i realize this is not an accurate method for <unk> down my taste , it 'd be better if it could scan my <unk> , or even be a neural network <unk> on my neural <unk> , but these days it seems that 's not possible . art <unk> is something i try and do every single day , looking for fine art photography , music , <unk> , <unk> design , <unk> , architecture , <unk> , <unk> , etc . i do this because , in a <unk> `` a <unk> never <unk> his own alphabet '' - <unk> <unk> and `` you ca n't exceed your environment . if you give a <unk> a watch , he does n't look at it and say <unk> <unk> are not <unk> , they are ten <unk> of an <unk> off ' he does n't say that . it 's impossible . that 's what i mean by you ca n't exceed your environment . you ca n't exceed what you 've been <unk> to . '' - <unk> <unk> so i 'm basically trying to expand my environment by <unk> myself to as many different systems as possible . i 've been doing this for 3 years and my work has <unk> <unk> i never thought it could as i <unk> experienced the <unk> effect . now i 'm wondering if it 's possible i could create a <unk> neural network , feed it my favorite <unk> , music , books , etc . with a rating , and my least favorite with a rating . i 'd be basically training it <unk> as i discover more art . after that i want to program it to search for art for me , with the <unk> of finding <unk> <unk> on the internet ( this is <unk> , because it is the whole point ) , that <unk> with my taste . art that maybe combines all areas of my taste . if i 'm being <unk> , the subject of machine learning does not give me the same <unk> i experience when making a <unk> . which is why i 've come here to ask for advice and guidance . i 'm not sure if it would be better to <unk> a developer ( after i 've saved up enough <unk> ) to write these for me - or would it be worth the time <unk> to get into ml on my own and build neural networks from scratch on python ? for the kind of program i want to build , how long do you think it would take ? i <unk> for <unk> that build neural networks and only found one so far called <unk> , but it does n't seem ( although i 'm probably wrong ) , it can do what i want . it seems it can only <unk> <unk> . would love some help : / <eoq> <unk> <unk> it 's possible , that 's what the <unk> for you ' sections of amazon and <unk> are doing ! you should totally teach yourself and build something on your own . who knows , maybe the <unk> will come later . especially after you 've made something that works ! look into <unk> systems , that 's what they 're called . also there 's an online course called <unk> on convolutional neural networks that 's used in image recognition . sounds like a fun project . <eoa> 
r or python , which is best for an ml beginner ? i would like to know which language i should use for starting machine learning . from what i can tell most people in the field use r , but i 've also heard that python is being used more and more . i have some experience with python , should i stick with what i know or should i learn r ? <eoq> if you already know python , continue using python . for all <unk> and purposes , they 're <unk> . <eoa> 
r or python , which is best for an ml beginner ? i would like to know which language i should use for starting machine learning . from what i can tell most people in the field use r , but i 've also heard that python is being used more and more . i have some experience with python , should i stick with what i know or should i learn r ? <eoq> > from what i can tell most people in the field use r <eoa> 
r or python , which is best for an ml beginner ? i would like to know which language i should use for starting machine learning . from what i can tell most people in the field use r , but i 've also heard that python is being used more and more . i have some experience with python , should i stick with what i know or should i learn r ? <eoq> python , because it <unk> better to programming systems beyond where r <unk> . <eoa> 
r or python , which is best for an ml beginner ? i would like to know which language i should use for starting machine learning . from what i can tell most people in the field use r , but i 've also heard that python is being used more and more . i have some experience with python , should i stick with what i know or should i learn r ? <eoq> focus on the statistical techniques . use what 's easier . <eoa> 
best algorithms for learning on sparse data ? i have a dataset that i have collected with <unk> <unk> binary features and less than 1 % of them are 1 's . i did a quick search to see if there are algorithms that are particularly good at working with sparse data and only found stochastic gradient descent . are there any others that are particularly good at working with sparse data ? <eoq> not sure of your <unk> but recommendation <unk> systems are built on this <unk> . <eoa> 
best algorithms for learning on sparse data ? i have a dataset that i have collected with <unk> <unk> binary features and less than 1 % of them are 1 's . i did a quick search to see if there are algorithms that are particularly good at working with sparse data and only found stochastic gradient descent . are there any others that are particularly good at working with sparse data ? <eoq> what are you trying to do ? supervised classification ? if so , then linear models with <unk> <unk> ( called <unk> ' ) have been developed with this problem in mind . comes up frequently in <unk> expression studies . <eoa> 
<unk> <unk> machines how are <unk> <unk> machines trained ? what are they used for ? are they used at all ? <eoq> the maximum likelihood gradient of any boltzmann machine learning <unk> down into two terms , the `` positive phase '' and `` negative phase '' . part of the reason that rbms are <unk> tractable is that an <unk> estimate of the positive phase statistics can be had in <unk> form . the negative phase statistics need to be estimated via sampling ( it is an expectation under the distribution p ( v ) ) , and because you ca n't sample p ( v ) directly , you need to run a markov chain , the most popular method being to do block gibbs sampling of p ( <unk> ) and p ( <unk> ) <unk> . [ deep boltzmann machines ] ( http : <unk> ) are another form of restricted <unk> where things break down into layers . they can be trained , by the method <unk> in that paper ( <unk> rbm <unk> ) and also <unk> all at once in a few different ways . <unk> the positive phase statistics <unk> the negative phase statistics are tractable but people have successfully used mean field <unk> for the positive phase and monte carlo ( you can do block gibbs sampling by sampling the odd layers given the even layers and <unk> <unk> ) . training general , <unk> boltzmann machines is hard because there 's no block structure you can <unk> for efficient sampling . [ this <unk> report ] ( http : <unk> ) <unk> a procedure that can apparently train at least some <unk> general boltzmann machines . boltzmann machines ( <unk> rbms ) are n't really used for much anymore ; rbms were used for <unk> <unk> deep neural networks but it 's become <unk> clear that they are <unk> most of the time ( there are a small number of tasks where <unk> networks still <unk> <unk> for the time being , apparently , <unk> some <unk> speech recognition tasks i think ) . the exception might be collaborative filtering , i know [ this <unk> ] ( http : <unk> ) is <unk> one of the models in production at <unk> . <unk> were an active area of research but interest in <unk> graphical models for deep learning seems to have <unk> up . <eoa> 
help understanding <unk> markov models i 'm a bit confused about inputs and outputs when it comes to hidden markov models . say there is a general problem of some sequence x with some labels y. for example , x can be a sequence of words , and y can be their parts of speech <unk> . let 's assume that the alphabet ( state space ) for x is size d and the alphabet of y is size l . say i want to create a hidden markov model ( hmm ) to <unk> y from x ( <unk> , guess parts of speech from words ) by training a transition matrix a and an observation matrix <unk> is the observation matrix the probability of seeing an x given a y or the other way around ? is the transition matrix a the normalized transition between the x states or is it between the y states ? do we normalize it for all previous states , or for all current states ? in other words , is the transition matrix a <unk> matrix or a <unk> matrix ? <eoq> you are talking about learning an hmm with fully observed data . can i <unk> you to a few pages of <unk> <unk> 's book ? it 's a simple read through . and the transition matrix will be <unk> , the other matrix would be an <unk> matrix which would be either <unk> or <unk> . <eoa> 
<unk> persons in picture of a <unk> , what method ? hello ! i am looking for a method to <unk> count people in a <unk> of a <unk> . example picture , but with <unk> included . we could change the <unk> as we wish : http : <unk> there are a lot of available methods but we are unsure of which have the potential to work <unk> well and are quite <unk> to implement since we are quite new to the area . we are mainly working in c++ or python . we have been looking on some open source libraries like <unk> etc . we got a tip that machine learning , random forest , neural networks might work in some <unk> . there are also a bunch of object detection algorithms like <unk> classifiers <unk> ( <unk> , <unk> or <unk> ) . and at last there are a few <unk> detection algorithms ( <unk> , <unk> or <unk> ) . we <unk> have the time to test all of the methods and would like to know what you would choose or not choose for the task . <eoq> nice try <unk> . deep learning is best for image recognition . look at software ( they have <unk> ) such as torch , theano , and <unk> . <eoa> 
automatically changing regularization level during training ? so i 've been playing with neural nets in lasagne , which outputs the train <unk> error ratio during training . it seems like a very useful way to tell if your nn is <unk> from high bias or <unk> from overfitting . anyway , i <unk> i would often see that the error ratio was too low , then <unk> the net with a higher dropout p or weight decay term and start training over . this seems like it could be very <unk> to automate . why not just average the <unk> error ratio of the past 10 epochs and change the dropout <unk> decay term size accordingly ? ( eg if the <unk> error is too much higher than the training error , increase dropout p and l2 regularization size . if the <unk> error is too much lower then training error , decrease dropout p . ) it seems like doing this would help by <unk> the regularization level so the net will never <unk> too high bias or too high variance . the main thing i 'm <unk> about is it will somehow lead to the net sort of over fitting the validation set . i also do n't have nearly enough programming skills to actually implement and test this myself , so can you <unk> offer any insight ? <unk> this a good idea , or will it lead to poor results on the test set ? <unk> it will end up <unk> <unk> , <unk> ? <unk> something like this already been done ? <eoq> i do n't it 's a bad idea , it might work out . this is somewhat similar to a learning rate <unk> , e.g . <unk> the learning rate when validation loss stops <unk> . <eoa> 
<unk> random forests ? is it possible to use <unk> to build random forests on a spark cluster ? does anybody have an example of this ? <eoq> sounds like this group <unk> <unk> ( unfortunately no syntax ) . https : <unk> <eoa> 
can someone explain thresholding an image for me ? im doing an image <unk> task and i think thresholding would work . i need to make a program that detects <unk> <unk> on an image and <unk> them ( active neurons in a <unk> <unk> image ) . im pretty new to these things . thanks . <eoq> at its most basic , if the <unk> value of a pixel is above the defined threshold , then set it to white , otherwise set it to <unk> . <eoa> 
can someone explain thresholding an image for me ? im doing an image <unk> task and i think thresholding would work . i need to make a program that detects <unk> <unk> on an image and <unk> them ( active neurons in a <unk> <unk> image ) . im pretty new to these things . thanks . <eoq> typically in <unk> , people want to keep <unk> <unk> above a value or zero out some values . you might also use thresholding to make a <unk> from an image . if you had a brain or other <unk> that is clearly defined , you can turn it into all 1 's with 0 <unk> else . then any other image that is in <unk> ( <unk> <unk> ) can be <unk> by multiplication . if this is <unk> or pet , you can use <unk> tool 's <unk> to do the thresholding . <unk> <unk> has all the options . <eoa> 
what is the difference between a ( dynamic ) bayes network and a hmm ? <eoq> hmm has the markov <unk> of the first order . <eoa> 
format question using scikit <unk> to put [ this ] ( https : <unk> ) data into scikit & work with it . i am having a bit of an issue seeing how the data <unk> to the features <unk> . can anyone help me ? [ here ] ( https : <unk> ) is a link about the data <eoq> i 'm not sure if i understand the question . do you have problems to see what column corresponds to which feature ? in this case the [ pandas ] ( http : <unk> ) library might be helpful : import pandas as <unk> features = [ `` <unk> '' , `` <unk> '' , `` <unk> '' , `` <unk> '' , `` <unk> '' , `` <unk> '' , `` <unk> '' , `` <unk> '' , `` <unk> '' , `` <unk> '' , `` class '' ] <unk> = <unk> ( `` <unk> '' , <unk> , <unk> ) <unk> ( ) this will output the first few <unk> of your data set with the header names as defined in the features list . i also recommend using [ jupyter notebook ] ( http : <unk> ) for just trying around with datasets . the pandas library outputs <unk> in a <unk> little table , which makes it a bit easier to see what you are dealing with . if i <unk> the question , please <unk> : ) <eoa> 
x-post from /r/machinelearning : question about <unk> hi guys , i 'm currently learning tensorflow . i 'm working on the <unk> tutorial . i 'm a bit confused and wanted to check whether i 'm understanding correctly how i would <unk> data myself to train the model . here 's what i think i should do , i 'd appreciate any feedback : first , convert the images to a numpy array of shape <unk> x length x channels ( 3 for rgb image ) . i 'm unsure what i 'd do with the labels , actually ... let 's say i call the final file foobar then , i could use <unk> ( foobar ) to read in the image in my code . in the code , the individual images are then arranged in <unk> the way tensorflow <unk> all to receive them . is this correct ? if i want to read in more than 1 image , what will i write instead of foobar , let 's say the files are called <unk> and 2 ? <eoq> for each batch , you 'll want to run <unk> ( <unk> ) <unk> ( <unk> ) the cost function google uses in their examples is cross entropy , in the form of <unk> ( <unk> * <unk> ( <unk> ) ) <eoa> 
machine learning uses the same few equations over and over the more papers and ml books that i read , the more i see the same equations repeated over and over , only with slightly different names . i 'm wondering if the machine learning literature is <unk> complicated by what is essentially <unk> with <unk> <unk> keep out those who are afraid to <unk> through the <unk> edit : maybe i was little <unk> when i wrote that . : ) <eoq> <unk> a few examples ? <eoa> 
machine learning uses the same few equations over and over the more papers and ml books that i read , the more i see the same equations repeated over and over , only with slightly different names . i 'm wondering if the machine learning literature is <unk> complicated by what is essentially <unk> with <unk> <unk> keep out those who are afraid to <unk> through the <unk> edit : maybe i was little <unk> when i wrote that . : ) <eoq> i would also like to see some examples . i 'm <unk> that you 're describing something like the fact that the linear regression equations appear over and over again - they do ! the basic <unk> of a linear model is the <unk> <unk> <unk> for just about every supervised learning model . the fact that this is a pattern we can <unk> is <unk> and for some people ( myself included ) very <unk> . we use these over and over again because they are <unk> models . i find it difficult to believe any <unk> that it 's simply a matter of <unk> <unk> on the part of authors , though . there are many alternative <unk> that fit the data here , such as : 1 ) the same equations appear over and over again , but the variations between each usage are hard enough to <unk> that we have yet to find an <unk> system which totally <unk> all of them . 2 ) most papers are written by experts in the field , who have seen these equations and patterns before . when they write their papers , they <unk> <unk> ideas and equations , both because it gives them a theoretical base to build on and a <unk> that their <unk> will already be familiar with . note that in this case , it 's the opposite of your <unk> - the various authors actually go out of their way to present the material in a familiar way , knowing that it would <unk> understanding to do so . <unk> , why on <unk> would someone spend months or years of their life researching something , only to <unk> a paper or a book which is <unk> <unk> ? <eoa> 
if glm performs better than gbm or rf , what does that mean ? hi , i was asked this question in interview and i <unk> i 'm not sure about my answer . what would be <unk> ? > if glm performs better than gbm or rf , what does that mean ? i think the answer expected was something about the data , and maybe a `` solution '' . cheers note : - glm : logistic regression - gbm : gradient <unk> trees - rf : random forest ( of trees ) <eoq> google is your friend my <unk> ... https : <unk> but the short version is it means there is a clear linear <unk> in the logistic regression via <unk> , whereas the data is not <unk> to a clear split for decision trees which ( for <unk> of <unk> ) are <unk> to each <unk> . <eoa> 
[ ufldl ] [ tensorflow ] can i consider my sparse <unk> implementation a success ? trying to follow the ufldl tutorial with tensorflow as my tool . i 'm on lesson 1 . [ here is my <unk> looking output . ] ( https : <unk> ) i am concerned that i found <unk> rather than edges . should i be ? or is the sample image of the tutorial an <unk> expectation for how these filters go ? unfortunately , i had to do two things <unk> than andrew ng 's sample code to achieve these results . <unk> used <unk> sample patches instead of <unk> . ** trained with gradient descent instead of <unk> , so that could explain why i need more . by contrast , [ here is a <unk> sample . ] ( https : <unk> ) it has several filters that look like random noise . <unk> learned filters are <unk> high contrast ! ** the above samples were first put through my <unk> function ( which is exactly the same function used in <unk> in the exercise ) before being <unk> to the <unk> . [ here is an <unk> image ! ] ( https : <unk> ) <unk> ! my first <unk> <unk> me this is an issue with my regularization of the cost function . the exercise has a lambda of <unk> increasing this <unk> <unk> to <unk> [ produces something a <unk> better ] ( https : <unk> ) . perhaps this means i am using the <unk> function of tensorflow <unk> ? here is that line of code : <unk> = <unk> * ( <unk> ( weights [ <unk> ' ] ) + <unk> ( weights [ <unk> ' ] ) ) finally , for tensorflow , <unk> did absolutely nothing for me , <unk> me with completely random filters in the end . is there a reason <unk> <unk> ( <unk> ) with <unk> ( ) should n't work for me ? [ here is the code on github ] ( https : <unk> ) <unk> one issue i 've found so far : i was not <unk> my output with sigmoid . now i do n't need to normalize my data ! but i 'm also no longer learning anything that looks like structure ... back to square one ... <eoq> i 've got a working solution based on your code . hope it helps . https : <unk> <eoa> 
which is your favorite tool for machine learning and predictive modelling when working with <unk> ? some examples would be : r - <unk> , <unk> , etc . python - scikit-learn , <unk> , etc . <eoq> numpy <eoa> 
interpreting the results of <unk> recurrent networks does any of you know , if any research has been done interpreting the results of <unk> recurrent networks and <unk> them back to the features that were the input to the model ? <eoq> <unk> and understanding recurrent networks , http : <unk> <eoa> 
best starting place for beginners ? i 'm 15 and starting to learn about machine learning , i am currently read artificial <unk> a modern approach but after that what should i do ? <eoq> i 'm by no means an expert , but learning python , or another similar language , so that you can actually implement the stuff they talk about in the books is probably a <unk> bet . <eoa> 
best starting place for beginners ? i 'm 15 and starting to learn about machine learning , i am currently read artificial <unk> a modern approach but after that what should i do ? <eoq> you 're 15 and already understand stochastic calculus , linear algebra , and optimization theory ? <unk> . <eoa> 
are n't features more important than any particular algorithm or ml method ? hi all , i just finished my first neural network ( ng 's coursera assignment , so as basic as possible , but still quite cool to me ) , and i was curious about how these can be <unk> when it comes to object recognition , etc . this assignment is <unk> recognition on a <unk> image , a <unk> <unk> input array . but <unk> images are <unk> bigger than this . so my question is , if individual pixels are n't used as inputs , how do features get extracted from images for use in neural networks ? it seems to me that learning how to develop quality features is a better <unk> of my time while trying to learn ml than <unk> deep into the particular algorithms . is there any <unk> to that ? for example , i learned of <unk> <unk> 's name from the ml : a probabilistic perspective , and on his front page there 's an intro of how <unk> learn to <unk> a horse from just a <unk> of examples ( which seems so common sense to me , but in the perspective of machine learning is quite <unk> ) . but is the problem that there 's no algorithm that can <unk> <unk> after such few training points , or is it that the features being fed to these algorithms do n't <unk> enough information to allow such few training points ? i really appreciate your time and any insight you can give me . cheers <eoq> for image work and neural networks , they often use convolutional neural networks ( cnns ) to find features that are combinations of individual pixels . as the cnns are trained to find `` useful '' features , you get things like `` <unk> edge '' or `` <unk> edge '' . these then become the features used by <unk> neural network layers . i 'm not an expert - i 've just read about this in one of my classes . <eoa> 
what are some good resources for someone looking to <unk> deeper into neural networks after having taken an introductory undergraduate course in ml . i am familiar with feed forward networks and how they work and am even taking a graduate course on neural nets this coming spring <unk> but would like to do some reading in the mean time . the thing is , a <unk> <unk> of the <unk> i read over on /r/machinelearning about the topic are way over my head . does anyone have any suggestions ? thanks ! <eoq> if you 're explicitly interested in neural networks then i 'd recommend <unk> hinton 's ( one of the <unk> in deep learning ) course on coursera which goes through a lot of the important <unk> <unk> to nns . <eoa> 
what are some good resources for someone looking to <unk> deeper into neural networks after having taken an introductory undergraduate course in ml . i am familiar with feed forward networks and how they work and am even taking a graduate course on neural nets this coming spring <unk> but would like to do some reading in the mean time . the thing is , a <unk> <unk> of the <unk> i read over on /r/machinelearning about the topic are way over my head . does anyone have any suggestions ? thanks ! <eoq> https : <unk> <eoa> 
looking for free offline resource to learn machine learning and prerequisite knowledge can anyone point me to any ? i 'm new to ml , i have a basic understanding of stats or probability , and i wanted to be able to download it for offline reading . i 'd prefer something in the epub or mobi format , but pdf works as well . <eoq> scikit learn and <unk> <unk> <eoa> 
looking for free offline resource to learn machine learning and prerequisite knowledge can anyone point me to any ? i 'm new to ml , i have a basic understanding of stats or probability , and i wanted to be able to download it for offline reading . i 'd prefer something in the epub or mobi format , but pdf works as well . <eoq> i already <unk> [ this ] ( https : <unk> ) reddit post to read at my <unk> . great place to start ( i 'm new to ml as well ) , but i would also recommend <unk> ml courses to any beginner . <eoa> 
looking for free offline resource to learn machine learning and prerequisite knowledge can anyone point me to any ? i 'm new to ml , i have a basic understanding of stats or probability , and i wanted to be able to download it for offline reading . i 'd prefer something in the epub or mobi format , but pdf works as well . <eoq> elements of statistical learning is a good book . <eoa> 
rnns as generative models i was going through [ <unk> <unk> 's thesis ] ( http : <unk> ) and in section <unk> he states `` an rnn defines a generative model over sequences <unk> the loss function <unk> l ( <unk> ; y^^t ) = <unk> ( p ( y^^t ; <unk> ) ) for some <unk> <unk> of distributions p ( <unk> ; z ) and y^^t = <unk> ( <unk> ) `` ( <unk> <unk> ) does this mean that the rnn will not act as a generative model if we use , let 's say , the squared loss function ? <eoq> <unk> ( p ( y^t ; z^t ) ) is the `` negative log likelihood '' . it does not <unk> a specific loss function ( maybe you <unk> and thought that it is cross-entropy ? ) it is also a generative model if the loss function <unk> l ( z^t ; y^t ) = - p ( y^t ; z^t ) for ... ( adding the log does n't change anything , as log is <unk> and increasing function ) . maybe related to your question : we write the mean squared error without the log because it does n't help . cross entropy has <unk> terms so there are computational issues because of limited <unk> of <unk> , right ? but squared loss is a sum so there is n't such a problem . please , correct me if i 'm wrong . <eoa> 
how do you keep track of the progress of a machine learning project ? i am talking about what features and algorithms you use , their performance etc . <eoq> best way is often to use r <unk> language ( or whatever software you use ) . you have the syntax and output but you can also write a <unk> <unk> and closer in each section that will describe what you did and why and the final results . <eoa> 
do i need a phd ? is a phd required to do work in machine learning ? <eoq> no . <eoa> 
do i need a phd ? is a phd required to do work in machine learning ? <eoq> absolutely not , if you do n't have an upper level degree <unk> you learned the <unk> and have a project to show for it ( thesis , dissertation ) , etc . your next best option is to create projects with <unk> results to show your <unk> and understanding . i am part of a machine learning <unk> type group . about every <unk> or two there are industry <unk> looking for people to bring into their <unk> to solve a machine learning problem . they essentially ask who in the group had done a similar problem then <unk> into that project . if they feel the individual or <unk> can tackle it they get <unk> for the project or the job . a good <unk> of the financial guys are either still in undergrad , have <unk> in a completely different field , or have nothing at all in paper <unk> , but have a huge amount of high results in online ml financial challenges . unless your looking for <unk> or something that essentially has a <unk> <unk> the <unk> needs to have a phd your good to go on your <unk> of previous results . i 've seen this same thing happen in python groups . someone will ask what you 're working on or present an open <unk> available and as long as people know what you 're working you 'll be <unk> towards each other . i personally feel this is true in all of computer science . simply because a degree is n't always <unk> of being a problem <unk> of <unk> questions . ( sure a degree helps , but this industry knows that it 's not the only way , <unk> say practicing <unk> on a human . ) <unk> for hard questions simply want results not a person in a <unk> . <eoa> 
trying to predict user behavior . achieved some results , stuck on choosing a better model / input . total newbie here . using sklearn . so i have an web app with a <unk> <unk> . this means i have data about the user approximately in this form , i 'm using reddit urls just to give you an idea : user 123 requested /r/machinelearning at <unk> user 123 requested <unk> at <unk> user 123 requested /r/mlquestions at <unk> user 123 requested <unk> ? <unk> at <unk> ... etc . basically , i have the user 's <unk> log of the web app . what i 'm trying to achieve is classify whether the user <unk> the app or not . there 's a 30 day trial <unk> where you can use the app for free and at the end of this <unk> you 'll have to start <unk> for the app in order to continue using it . on my first try i <unk> the urls completely and just fed in the number of <unk> per a day . so i might feed a vector like this to the ml algorithm : [ <unk> , 10 , 0 , 15 ] so <unk> actions taken on the first day , 10 actions on the second day , etc . with <unk> days , i achieved very close to 90 % accuracy . this was with a balanced set , so the roc auc score was also very close to <unk> i just tried a bunch of classifiers from sklearn and chose the best one . random forest and a simple linear logistic regression performed the best . i 'm now trying to do the same thing again , but only looking at the first hour of usage . i 'm also trying not to <unk> the different categories <unk> actions . however , this has made the input matrix really huge and very sparse , etc for user 123 you 'll have something like this : [ [ 1 , 2 , 2 , 1 , 0 , 0 , ... , 0 ] , # actions performed within the 1st minute [ 0 , 0 , 0 , 0 , 0 , 2 , ... , 0 ] , # actions performed within the 2nd minute ... [ 0 , 0 , 0 , 0 , 0 , 0 , ... , 0 ] # actions <unk> within the <unk> minute ] i thought i might feed each of the minutes to an unsupervised clustering algorithm for dimensionality <unk> , to end up with a vector like this : [ 1 , 2 , 0 , 0 , 0 , 5 , ... , 0 ] where the numbers , hopefully , represent types of user behavior the clustering algorithm found . i just tried this using k-means and <unk> . <unk> did find lots of clusters , probably around 1500 different clusters . it 's a huge number , so maybe i 'm doing something wrong . i do n't thing new users can do 1500 different general things within the first hour of usage ... i could also try to reduce the number of different <unk> by just taking the first category , etc <unk> and <unk> would both be just <unk> just so that i could reduce the dimensions of 1 minute . okay so my current plan is to feed these clusters to a classifier . the thing is , the whole thing is a time series , so would another algorithm be more suitable for this ? i 've heard lstm is good for time series . am i <unk> in the right direction or am i doing something stupid ? <eoq> just saw this paper `` <unk> recommendations with recurrent neural networks '' : http : <unk> . i have n't read it yet . maybe it helps to solve your problem . <eoa> 
hardware ? i 'm new to the ml game . starting on kaggle competitions . thinking about getting a new computer . what hardware will make the biggest difference ? not looking to break the bank , just some guidelines i.e . minimum <unk> requirements , <unk> up the speed , <unk> for my <unk> kinda stuff , etc . <eoq> the biggest speedup will come from a video card that supports <unk> ( i.e . <unk> ) . i recently <unk> my <unk> <unk> for a <unk> to <unk> theano 's <unk> support , and the speedup is <unk> . <eoa> 
hardware ? i 'm new to the ml game . starting on kaggle competitions . thinking about getting a new computer . what hardware will make the biggest difference ? not looking to break the bank , just some guidelines i.e . minimum <unk> requirements , <unk> up the speed , <unk> for my <unk> kinda stuff , etc . <eoq> all major open source ml libraries support <unk> hardware <unk> . so something that supports an <unk> card . correct implementation took some of my compute times down from 2 hours to 5 minutes . <eoa> 
reshaping in tensorflow mnist tutorial ? <unk> , i 'm trying to follow [ tensorflow 's <unk> tutorial ] ( https : <unk> ) , and i can not understand the <unk> of reshaping . can someone <unk> explain why and how is it done ? ( i 'm also a little confused by the negative value in the first place , what does that mean ? ) . the code in question is ... <unk> = <unk> ( x , [ <unk> ] ) <eoq> a matrix is a vector of vector of values . reshaping turns it into a single large vector of values . then turns it into a matrix of vectors again of whatever dimensions you want it in . simplest application for this is <unk> a matrix to calculate a <unk> or a <unk> <eoa> 
will be possible to develop a learning maching to try to predict tennis matches with rapidminer ? hi , i 've got tons of data with a lot of info about tennis matches of the last 5 years . i would like to create a machine learning and train it with the data from 2011-2014 and then test it with 2015 data . is this possible ? the info in the datasets is a little confusing . is this possible with rapidminer ? thanks . <eoq> yes and yes . split the data into training and test sets ( splitting by years may affect your tests <unk> ) . run neural nets , random forests , etc . <unk> . <eoa> 
will be possible to develop a learning maching to try to predict tennis matches with rapidminer ? hi , i 've got tons of data with a lot of info about tennis matches of the last 5 years . i would like to create a machine learning and train it with the data from 2011-2014 and then test it with 2015 data . is this possible ? the info in the datasets is a little confusing . is this possible with rapidminer ? thanks . <eoq> you might look at `` analyzing <unk> data with r '' by max <unk> . different <unk> , but the <unk> are the same . <eoa> 
will be possible to develop a learning maching to try to predict tennis matches with rapidminer ? hi , i 've got tons of data with a lot of info about tennis matches of the last 5 years . i would like to create a machine learning and train it with the data from 2011-2014 and then test it with 2015 data . is this possible ? the info in the datasets is a little confusing . is this possible with rapidminer ? thanks . <eoq> at the most naive level , you might just look at each player and determine a mean and standard deviation for the number of points they score . then to predict the <unk> of a game , for each player , randomly sample from a normal distribution using the mean and <unk> you found for each player . the one with the most points wins . maybe you do that an odd number of times , and choose the <unk> from who <unk> the most <unk> games . this is essentially a <unk> <unk> . you could get more <unk> by using more of your variables in your data to predict a number of points . you could even try to get more <unk> if your data can help you model when they 'll score in the game and their probabilities of <unk> points given things like ... `` <unk> in the game , and behind '' , etc . the <unk> are <unk> . you might also try to find ways to classify players based on your data ... good on <unk> vs <unk> , <unk> , <unk> , good <unk> , etc . these would then be factors to help <unk> your prediction of their performance . but i 'd go with starting simple and adding <unk> as you go . i 'd probably look at some simple <unk> to see how well your variables predict points or outcomes . <eoa> 
advice <unk> : using ml for neuroscience research , do i need to <unk> matlab ... hi all ! i 'm a <unk> neuroscience <unk> in the field of <unk> , meaning most of my work <unk> <unk> and anything signals from one or more neurons in order to determine their function . one of the common problems in my field is <unk> the activity of neurons that are <unk> to be encoding information about something , without any <unk> idea of how they 're <unk> that information . the information can be <unk> in so many different ways ... by the <unk> of a spikes relative to an <unk> triggering <unk> , by the <unk> of the rate of <unk> , by a <unk> <unk> pattern of spikes , by the <unk> of spikes relative to the phase of a particular neural <unk> ... it is common in my field to use machine learning as a way to show that a particular type of neuron <unk> information in a particular way , by using the neural activity as inputs to a classifier ( and the different types of events / <unk> <unk> or following that activity as the target categories ) . the underlying <unk> assumption is that if a ml classifier can identify the <unk> using that information alone , then there 's a decent chance that the brain areas that receive that information are doing so as well . i started off learning matlab for data analysis <unk> . i had no real programming experience ( just writing scripts for data analysis software , or to <unk> <unk> <unk> systems ) , had no idea what convolution was , and had no experience working with matrices and vectors and <unk> . i 've become reasonably <unk> in matlab ( i think ) , and found the various ml tools in matlab to be helpful for data analysis . but i have run into a number of <unk> . my issues are : difficulty with <unk> data ( phase angles ) using feedforward nets and/or <unk> i have been just <unk> inputs as the <unk> and <unk> of the angles , but i wish i could just use complex inputs . difficulty training classifiers when the <unk> of the classes is not known ( e.g . there are ten classes , but it is quite possible that the inputs only contain enough information to <unk> the samples into <unk> , <unk> , and <unk> ) ; classifiers often get <unk> up trying to minimize errors instead of <unk> information . i have tried to get around this by using evolutionary algorithms to train <unk> , using <unk> information ( or normalized variation of information ) between network outputs and targets as the <unk> function . difficulty <unk> out how to get matlab 's training algorithms to use my <unk> of data into training and validation sets . uncertainty about how to determine the best type of classifier for my <unk> this is more of a <unk> problem ' than a matlab problem . uncertainty about how to expand my <unk> to a <unk> range of input types : i have so far usually just <unk> my data down to some number of phase angles and then used those as inputs , but i might wish to do <unk> in which the input is a continuous signal <unk> a discrete <unk> signal , with the goal being to identify <unk> in the signal during which certain events are <unk> . things i do n't need to do : image recognition <unk> <unk> models so , that 's my situation . i could use advice on whether it 's worth it for me to <unk> matlab in <unk> of something more flexible . i could also use general advice about how to solve any of the issues i 've described . any help is appreciated ! and if you want to know more , just ask . <eoq> what do your <unk> <unk> use ? do you need to share code ? <eoa> 
what is the difference between convolution and correlation . why do cnn 's use convolution ? <eoq> correlation <unk> a relationship between two variables . convolution is a product operation that combines two distributions ( or series ) to create a new distribution . in the context of a time series , the convolution operation combines the past information of two time series to create a new series . in the context of a cnn , at each time step the network contains the history of all past inputs and activations . in this way it is a generalized version of the convolution operation . <unk> : i just <unk> that i confused cnn and rnn . : ( must have been on <unk> <unk> . <eoa> 
guide to implementing rnn hi ! i want to build a simple rnn with matlab to learn reading text . i have tried to understand how to implement it in code , but i would love to see if there are good guides out there that gives exercises in implementing the code ! any tips would be great : ) <eoq> sorry if i 'm <unk> what you are saying , but , if you are n't implementing it to learn about nns but rather to do a specific task , i 'd really suggest not implementing your own nns . <eoa> 
guide to implementing rnn hi ! i want to build a simple rnn with matlab to learn reading text . i have tried to understand how to implement it in code , but i would love to see if there are good guides out there that gives exercises in implementing the code ! any tips would be great : ) <eoq> this is a very good post on rnn 's helped me understand them alot better and has some good code samples , though not in matlab . http : <unk> <eoa> 
want to focus on the financial application of machine learning , looking for good study material i 've already <unk> the coursera machine learning course <unk> by stanford so i believe i have a good understanding of the basics of machine learning . i want to now focus on the financial applications of machine learning , but i want to make sure i pick the right material to study . does anybody have any recommendations ? i am primarily interested in the prediction of stock prices . thank you . <eoq> it 's worth doing some <unk> related course to get more of an understanding of things from that angle . this coursera one is quite good https : <unk> <eoa> 
i want to make a program that will help me hunt for apartments . is ml the right tool and where should i start ? i have this project to write some kind of app ( i 'm more proficient in ruby and js ) that would eventually be able to sift through apartments for rent ads and send me the relevant ones . the way i see it is that i 'd feed the program ads and then tell it whether this particular listing is of interest to me or not and hope the machine will learn to eventually be able to do the sorting on its own . is this something that can be achieved with ml ? are there any frameworks that are better suited to this ? i am a developer but i do n't know much about statistics or maths and i 've never worked on anything related to ml . any pointers ? thanks ! <eoq> it just sounds like a job for regular <unk> . anything more is <unk> . <eoa> 
i want to make a program that will help me hunt for apartments . is ml the right tool and where should i start ? i have this project to write some kind of app ( i 'm more proficient in ruby and js ) that would eventually be able to sift through apartments for rent ads and send me the relevant ones . the way i see it is that i 'd feed the program ads and then tell it whether this particular listing is of interest to me or not and hope the machine will learn to eventually be able to do the sorting on its own . is this something that can be achieved with ml ? are there any frameworks that are better suited to this ? i am a developer but i do n't know much about statistics or maths and i 've never worked on anything related to ml . any pointers ? thanks ! <eoq> machine learning can help you figure out how good of a deal a particular listing is , and if you want to get even more advanced then go ahead and try to train an algorithm that could even figure out if a given posting is too good to be true and is probably a <unk> . <eoa> 
i want to make a program that will help me hunt for apartments . is ml the right tool and where should i start ? i have this project to write some kind of app ( i 'm more proficient in ruby and js ) that would eventually be able to sift through apartments for rent ads and send me the relevant ones . the way i see it is that i 'd feed the program ads and then tell it whether this particular listing is of interest to me or not and hope the machine will learn to eventually be able to do the sorting on its own . is this something that can be achieved with ml ? are there any frameworks that are better suited to this ? i am a developer but i do n't know much about statistics or maths and i 've never worked on anything related to ml . any pointers ? thanks ! <eoq> i am planning to do the same thing for job <unk> let 's do it together ? i have ml and nlp knowledge , wan na work together ? <eoa> 
deep learning simplified : <unk> 5 - an old problem while deep neural nets are the state of the art in machine learning , the <unk> is that they are really hard to train . up until <unk> , there was no way to train them <unk> . here is a <unk> that <unk> further . https : //www.youtube.com/watch ? <unk> <eoq> this one is on the vanishing gradient . enjoy : - ) <eoa> 
trouble of using <unk> for <unk> i 'm trying to implement the net in [ accurate image <unk> using very deep convolutional networks ] ( http : <unk> ) . it 's <unk> by <unk> : the only difference is that there are no <unk> layers , so all intermediate weight layers have <unk> channels . i <unk> the details of the paper 1. gradient <unk> 2. learning rate = <unk> , decreases by 10 every <unk> epochs 3. <unk> weight initialization : <unk> ( <unk> ( <unk> ) ) <unk> , where filter <unk> = 3 , number of input channels = <unk> <unk> <unk> = <unk> , l2 regularization = <unk> in <unk> , i 'm keeping track of the <unk> <unk> , and it seems after a certain time , the weights stop changing . this may mean the gradient <unk> , or we 've <unk> on a bad local minimum . however , i am getting performance <unk> off than <unk> <unk> . in the paper , they get better performance even in the 1st epoch . i was wondering why is this <unk> ? <eoq> initialize the weight matrix of the last convolution , the one that results in the <unk> , with <unk> . it also helps to use <unk> instead of rgb . <eoa> 
deep learning simplified : <unk> 4 - how to choose deep learning as a field has developed quite a bit in the last <unk> , and we now have a variety of models to pick from with new models and improvements <unk> frequently . the <unk> side to this is , the <unk> of choice now falls on you to figure out which model to pick for what application . here is a <unk> that gives you some guidelines to help you decide . https : //www.youtube.com/watch ? <unk> <eoq> some simple rules of thumb on how to pick a deep net . enjoy : - ) <eoa> 
looking for em derivations exercises with solutions hi , i am looking for a good source of exercises of derivations of the em algorithm ( possibly also other inference methods , such as variational bayes ) with solutions . ideally this would contain an <unk> of different graphical models on which to do the derivations of the <unk> , the e and m steps . any pointers ? <eoq> * [ em ] ( http : <unk> ) * [ variational inference in 5 minutes ] ( http : <unk> ) * [ general purpose <unk> inference ] ( http : <unk> ) * [ <unk> variational bayes ] ( http : <unk> ) <eoa> 
deep learning simplified : youtube series hi everyone ! i am new to this sub-reddit and wanted to introduce myself . i have been working on a youtube series for deep learning that you may like . if you are ever need to explain deep learning to a newbie ( or are new to deep learning yourselves ) , you may like this series . content you 'll typically find online on the topic is highly mathematical/technical , which is great ! but if you 're like me , you probably want to just understand the models and the intuition . thats what this series is about ! here is the link to the series intro . please take a look and let me know what you think ! https : //www.youtube.com/watch ? v=b99uvkwzytq <eoq> this is the series intro - 6 total <unk> so far and many more to come . enjoy : - ) ! <eoa> 
deep learning simplified : youtube series hi everyone ! i am new to this sub-reddit and wanted to introduce myself . i have been working on a youtube series for deep learning that you may like . if you are ever need to explain deep learning to a newbie ( or are new to deep learning yourselves ) , you may like this series . content you 'll typically find online on the topic is highly mathematical/technical , which is great ! but if you 're like me , you probably want to just understand the models and the intuition . thats what this series is about ! here is the link to the series intro . please take a look and let me know what you think ! https : //www.youtube.com/watch ? v=b99uvkwzytq <eoq> very nice ! i <unk> the intro video . the production quality is great and i like the <unk> 's <unk> . i look forward to watching the rest . thanks for doing this ! <eoa> 
deep learning simplified : youtube series hi everyone ! i am new to this sub-reddit and wanted to introduce myself . i have been working on a youtube series for deep learning that you may like . if you are ever need to explain deep learning to a newbie ( or are new to deep learning yourselves ) , you may like this series . content you 'll typically find online on the topic is highly mathematical/technical , which is great ! but if you 're like me , you probably want to just understand the models and the intuition . thats what this series is about ! here is the link to the series intro . please take a look and let me know what you think ! https : //www.youtube.com/watch ? v=b99uvkwzytq <eoq> i found that really useful ! thanks very much . <eoa> 
correct cost function to use with a softmax output layer with a continuous target distribution ? lets say i am training a neural network to play rock , paper , <unk> . the network outputs a probability distribution over the three actions . i am using a softmax as the final layer of the network to <unk> that <unk> + <unk> + <unk> = 1.0 . i understand ( i <unk> ) that if my networks target output was discrete , ie ( <unk> ) , ( <unk> ) or ( <unk> ) , i should use cross-entropy as my cost function . my question is what cost function should i use to train the network if my required target output is continuous , for example ( 1./3 , 1./3 , 1./3 ) ? i have tried mean squared error , but it does not converge . i do n't know if i have a bug , or i am using the wrong cost function . is mean square error or cross-entropy appropriate in this case ? would anyone be kind enough to point me in the right direction ? thanks in advance ... <eoq> output of your network is still discrete , it is classification problem , but you will never get full one and <unk> , because softmax is giving you probabilities that it should be that output . if you want result you need to sample from that distribution . so you should use cross-entropy . <eoa> 
<unk> error <unk> when <unk> svm with polynomial kernel : `` <unk> : <unk> max number of iterations '' it is my first time working with support vector machines . i am trying to solve this homework , but am <unk> the above mentioned error ... here is my code : library ( <unk> ) <unk> = # <unk> test data here . <unk> <unk> ( <unk> ' , <unk> = ' , ' , header = true ) y = training_data $ y chosen_svm = function ( y , training_data , kernel_name ) { obj < - <unk> ( <unk> , data = training_data , gamma = <unk> ( <unk> ) , cost = <unk> ( <unk> ) , kernel = kernel_name ) gamma = obj $ <unk> $ gamma cost = obj $ <unk> $ cost model = svm ( <unk> , data = training_data , gamma = gamma , cost = cost , kernel = kernel_name ) return ( model ) } <unk> = chosen_svm ( y , training_data , <unk> ' ) <unk> = chosen_svm ( y , training_data , <unk> ' ) <unk> = chosen_svm ( y , training_data , <unk> ' ) any idea why this is <unk> ? <eoq> i am a bit <unk> to check but i would guess that svm requires convergence and it was <unk> to converge . <eoa> 
what effect does loss function have on training ? i understand it is only used to <unk> progress , ie . it does not really have any <unk> in how weights are updated ? if so , why is the choice of loss function important ? the network will converge to what it will converge anyway ... a slightly related question about training <unk> , ie . error for the output layer , from what i 've seen sometimes its just target - output sometimes its ( target - output ) <unk> ( output ) is it chosen independently of loss function ? <eoq> false <unk> , figured it out <eoa> 
given a list of centroids , how to find optimal set of length k ? think this is an easy one i 'm having a brain <unk> about . given some dataset where each observation x has a <unk> distance to n centroids . if i know i want to separate the entire set into k centroids , how can i pick which k ? or more <unk> , lets say i have 1000 rocks and each rock has a set of 1000 <unk> to some centroid representing a feature ( maybe color , likelihood to be found in a <unk> , etc ) and i know i want to <unk> the whole set of rocks into 5 centroids ( either a rock is closest to the <unk> centroid or to the color centroid ) . how do i find the 5 centroids which will minimize my distance to the whole set of rocks ? i feel like there 's some kind of <unk> i 'm just not thinking of . <eoq> [ k-means clustering ] ( https : <unk> ) . <eoa> 
need help with understanding how to compute the weight gradient in a convolutional layer . hi , so i have a have a hard time understanding how to compute the gradient for the convolutional layer . to test my understanding i have <unk> a simple convolutional network <unk> one convolutional layer , one <unk> layer and a fully connected output layer . the whole network looks like this : http : <unk> so when applying <unk> computing the output <unk> is simple , but i have some <unk> when computing the other <unk> . for the <unk> layer most <unk> will say to simply `` repeat '' the errors from the previous layer since this layer does not do any learning . so for delta^3 i compute a matrix with the error values from <unk> for the <unk> <unk> and 0 <unk> else . so for the <unk> for the convolutional layer ( delta^2 ) i guess one only <unk> the <unk> product between <unk> and delta^3 ? . when computing the gradient for the weights i <unk> compute the convolution between the input ( i ) and delta^2 . using this gradient does not <unk> <unk> results ( i.e reduce the error ) when updating the weights , so i am obviously doing something wrong . how i compute the gradient is <unk> in this picture : http : <unk> basically my question is what is the correct way of <unk> the weight gradient in this example . <eoq> figured it out , to compute delta^2 you <unk> compute it <unk> = <unk> ( <unk> delta^3 ) <unk> ( <unk> ' ( z ) ) . where delta^3 = <unk> . <eoa> 
[ help ] supervised classification implementation this is my <unk> ml project , so i am trying to build a really simple web analytics tool . i want to use apache web logs to determine whether or not to issue a <unk> to a customer . the web logs contain <unk> <unk> and <unk> history , so i can determine what pages a customer <unk> to and whether or not they bought the product . using this data as a training set , i hope to build a model that can report to me who i should issue new <unk> to , in order to improve sales . my question is : how can i implement this ? any details i can get will be really helpful . i already have a java program that can take the web logs and split them into individual pieces of data . how do i get this data into a form that is <unk> , and how do i make the program ? its hard for me to find tutorials that are <unk> and implementation <unk> . thanks ! <eoq> let 's get the data extraction <unk> out . what you want is the pattern of a customer 's <unk> before <unk> makes a <unk> n't ever make a buy ( no buy for next 4 <unk> ) . so your data is only limited to <unk> making a buy . can you see how to extract this data ? now you have a time series data ( do n't be <unk> , fancy way of saying simple stuff ) of buying patterns . split into training and test . i guess a hidden markov network would do good here . the observed data is the page being <unk> , and hidden state is probability to buy . learning this model is <unk> ( tractable ) . read <unk> <unk> 's section on learning a partially observed hmm . you can <unk> the length of the hmm to a value based on the amount of data and performance on test set . smaller <unk> may give less test performance , longer hmm might not have enough data to train on . feel free to ask <unk> : ) <eoa> 
the typical 25 horse problem with a <unk> the typical interview question : you have 25 horses and want to identify the 3 fastest horses . you have a track that can hold 5 horses at a time . what is the minimum number of races it would take for you to identify the 3 fastest horses . <unk> : same problem , <unk> now you want to identify the 6 fastest horses . edit : i forgot to mention , the times are not recorded so you can not just find the speed of each horse individually and compare . ex : for the top 3 horses , the answer is 7 races to <unk> this , i will show label the horses with a <unk> and a number . the <unk> refers to their first race set and the number refers to their order for the first race . race <unk> <unk> <unk> <unk> 5 : -- | : -- | : -- | : -- | : -- <unk> <unk> <unk> <unk> <unk> we can then find the fastest horse by comparing the best in each of the first 5 : race <unk> : -- | : -- <unk> <unk> <unk> <unk> <unk> lets say that horse <unk> , b1 , and <unk> are the fastest , showing up in their <unk> <unk> ( i.e . <unk> is the fastest ) . then to identify the 2nd and <unk> fastest , we race the set of <unk> : race <unk> : -- | : -- <unk> <unk> <unk> <unk> <unk> this will give us the 3 fastest horses . <eoq> 5 for both assuming the horses are in random order and you do not have any information on them . since 5 horses x 5 horses per track is 25 and you want to <unk> each horse exactly once because you do not have any prior information on them . then you sort the results by speed . any <unk> races can bias the results because horses used multiple times can be <unk> . <eoa> 
machine learning on <unk> ? hello everyone , i 'm currently a student working on a <unk> with a [ database ] ( https : <unk> ) from <unk> > ( from their website ) > > you are given two wikipedia articles starting from the first article , your goal is to reach the second one , exclusively by following links in the articles you encounter . i 'd like to train an algorithm to guess the shortest path to go from one article to another , knowing only the categories ( e.g historical figure , <unk> , <unk> <unk> ... ) the idea is that computing every possible path from one point to another is simply impossible on such a large graph in a reasonable amount of time , so i want to use machine learning to have a <unk> solution in a relatively short amount of time . i 'd like to say that i 'm a <unk> <unk> machine learning , although i already <unk> a few projects , but i never worked on graphs before : do you have an idea where i could find some ideas on what kind of algorithms to use for my project ? ( if it can help , i code in python and used scikit-learn for my previous projects ) thank for your help <eoq> this seems similar to ( not exactly ) how google maps would give you a <unk> from one end of us to <unk> ( edit ) the difference being that in maps you have a measure of <unk> to the destination ( euclidean distance ) , in this problem it 's not that obvious . my take at it ( multiple ideas ) extract a graph from wikipedia , the articles are the nodes and the links are <unk> edges . then , this article is a good start : https : <unk> 1 ) <unk> search from source to destination . you 'll have to figure out the <unk> to use . we can <unk> further on this . 2 ) by <unk> down the problem into <unk> ( <unk> ) based on major topic in wikipedia ( or other clustering <unk> ) . you can run single <unk> <unk> shortest path on all <unk> and then <unk> the outputs ( details need to be figured out ) . this would be useful if you want shortest <unk> from all articles to all other . the graph you construct would be sparse , that <unk> could also be used . at the least <unk> should be done by an array of <unk> . computational <unk> might also be needed to look into . <eoa> 
help with a deep convolution network i 'm building a deep convolution neural network on my own with c++ and am getting stuck . how does the convolution layers update its weights ? i understand how the hidden layer and the output layer 's weights are updated , but i ca n't think of how to update the convolution filters . can anyone explain how to update the convolution layer , provide an algorithm , or resources where i can learn more about this ? <unk> , for some context , the goal of my network is to learn to play <unk> . with some hand <unk> filters i was able to get it to <unk> the <unk> 2-3 times , but not that reliably . <eoq> it 's same as in the dnn . [ <unk> ] ( http : <unk> ) would be a good start to check it out , or see how people have implemented it in frameworks e.g . <unk> . <eoa> 
<unk> a random forest from scratch , with no ml libraries <unk> . hi all , not a programmer but i 'm taking a machine learning module i 've been asked to design a rf from scratch . i am lost . i have basic skills in python - very basic . can anyone help ? so far i 've <unk> a decision tree from the internet but i 'm a bit lost on it as its <unk> me my features in the function argument are n't defined . my attempt at the code below . import numpy import csv with open ( <unk> ' , <unk> ' , <unk> ' ) as csvfile : # type name of <unk> with <unk> in first set of <unk> . data = <unk> ( csvfile , <unk> ' ' , <unk> ' ) # for row in data : # print ( ' , <unk> ( row ) ) # <unk> out all data in file def newdt ( data , features , targetclass , fitness_func ) : # define a new decision tree <unk> [ : ] <unk> [ record [ targetclass ] for record in data ] <unk> ( data , targetclass ) <unk> ( data [ <unk> ] ) if not data or ( len ( features ) -1 ) < =0 : return empty <unk> <unk> ( values [ 0 ] ) <unk> ( values ) : return values ( 0 ) else : <unk> ( data , features , targetclass , fitness_func ) tree= { best : { } } for value in <unk> ( data , best ) : <unk> ( <unk> ( data , best , value ) , [ attr for attr in features if attr ! = best ] , targetclass , fitness_func ) tree= [ best ] [ value ] <unk> return tree newdt ( data , features , targetclass , fitness_func ) <eoq> i 'm a bot , <unk> , <unk> . someone has linked to this thread from another place on reddit : <eoa> 
<unk> a random forest from scratch , with no ml libraries <unk> . hi all , not a programmer but i 'm taking a machine learning module i 've been asked to design a rf from scratch . i am lost . i have basic skills in python - very basic . can anyone help ? so far i 've <unk> a decision tree from the internet but i 'm a bit lost on it as its <unk> me my features in the function argument are n't defined . my attempt at the code below . import numpy import csv with open ( <unk> ' , <unk> ' , <unk> ' ) as csvfile : # type name of <unk> with <unk> in first set of <unk> . data = <unk> ( csvfile , <unk> ' ' , <unk> ' ) # for row in data : # print ( ' , <unk> ( row ) ) # <unk> out all data in file def newdt ( data , features , targetclass , fitness_func ) : # define a new decision tree <unk> [ : ] <unk> [ record [ targetclass ] for record in data ] <unk> ( data , targetclass ) <unk> ( data [ <unk> ] ) if not data or ( len ( features ) -1 ) < =0 : return empty <unk> <unk> ( values [ 0 ] ) <unk> ( values ) : return values ( 0 ) else : <unk> ( data , features , targetclass , fitness_func ) tree= { best : { } } for value in <unk> ( data , best ) : <unk> ( <unk> ( data , best , value ) , [ attr for attr in features if attr ! = best ] , targetclass , fitness_func ) tree= [ best ] [ value ] <unk> return tree newdt ( data , features , targetclass , fitness_func ) <eoq> hi ! once you implement a decision tree you are almost there , just build n of them , predict your validation set through all of them and for each input vector take the mean ( regression ) or the <unk> ( classification ) . if you can share your <unk> file i will take a look at it <unk> . <unk> a link to where you found the <unk> . <eoa> 
how do i describe a perceptron , comparing two inputs <unk> triggering when x > y . i tried to setup a perceptron <unk> outputs an 1 when output x is bigger than y. how can i <unk> this ? like <unk> ? <eoq> if i recall perceptron well , this python code should describe such perceptron : <eoa> 
how do i describe a perceptron , comparing two inputs <unk> triggering when x > y . i tried to setup a perceptron <unk> outputs an 1 when output x is bigger than y. how can i <unk> this ? like <unk> ? <eoq> y is an input ? are there only 2 inputs ? are you going to use back propagation for training ? <eoa> 
does changing contrast , <unk> , etc in data set increase the accuracy of the trained net ? i have a data set of around <unk> images , if i <unk> these images by changing their <unk> such as contrast , <unk> , <unk> <unk> to create a larger data set will the accuracy of my net be greater ? very new to this ! <eoq> depending on what your images are , it can . i remember someone who <unk> a kaggle competition on identifying <unk> ( ? ) did a <unk> on his processes , and part of it was <unk> the images in different ways to create a more generalized dataset edit : http : <unk> <eoa> 
simple projects to begin with ? hello ! i 'm in a research program at my school and choose to learn about ml . currently i 'm learning algebra <unk> , so my math education is not nearly enough to easily understand ml . i 've been working at a decent <unk> and i currently understand gradient descent thanks to coursera and andrew ng , however suddenly my <unk> requested 10 pages of original research done in a very short amount of time ( monday , <unk> ) . are there any basic data sets and techniques i can use to at least get credit for my research thus far ? ml seems to be a very deep topic you learn about over time , and i currently do n't have time . i was thinking about trying to use some modified gradient descent algorithms on data sets , but my issues currently are : i do n't know what kind of data i <unk> to get it , i do n't know how to make a gradient descent algorithm work for more than 2 parameters ( linear regression ) . any advice would be appreciated . i 'm sorry for asking this question , i know it 's very `` how do i learn ml quick '' style , but really i just need a simple project to pass and continue learning at a normal rate . <eoq> andrew ng 's intro to ml is really good and i 'm not sure how you would go about learning ml much faster than that . i think i would recommend <unk> a bit on his lectures so that you get a bit beyond linear regression with 2 parameters . if i recall correctly , there are assignments in this course , right ? maybe you can use that in your report ( maybe <unk> the exercises a bit ) . i do n't really know anything interesting to do with 2 parameter linear regression . you can fit a line through a bunch of data points . if you learn to use more parameters , you can make things slightly more interesting by comparing the lines you get with various numbers of parameters . if you learn about logistic regression , you can start doing <unk> . you could make `` networks '' that simulate and and or logic <unk> . for xor you need to use a <unk> technique since it 's not <unk> separable . for instance , a neural network with a hidden layer ( i.e . a <unk> perceptron ) . one fairly small but interesting problem is classifying the [ mnist ] ( http : <unk> ) dataset of handwritten digits . however , this is quite a bit beyond linear regression with 2 parameters . you could also take a look at some of the challenges on [ <unk> ] ( https : <unk> ) . good luck ! <unk> , i hope you meant monday the <unk> and not <unk> the <unk> ( <unk> depending on your time <unk> ) . ( monday the <unk> does n't exist . ) <eoa> 
can two machine-learning computers be identital ? hi , i have a question that 's been eating me for some time and do n't know where else to post it . so here it is : assuming that two computers ( m1 & m2 ) are fed the exact-same training data , would they behave identically , to the point where we can accurately assess/predict the behaviour of m2 based on m1 . i guess i 'm trying to find out whether it is impossible , due to inherent differences in the hardware ( not the exact same chip , even if from same manufacturer ) akin to a genetical difference . it 's a bit like the rhetorical `` if two humans grew up in the *exact* same environmnent , would they behave in the same way '' , though i would assume it 's technically impossible to do such a test . if anyone could shed some answer i would appreciate it , thank you : ) peace <eoq> i 'm not sure if i understood the question correctly . <eoa> 
can two machine-learning computers be identital ? hi , i have a question that 's been eating me for some time and do n't know where else to post it . so here it is : assuming that two computers ( m1 & m2 ) are fed the exact-same training data , would they behave identically , to the point where we can accurately assess/predict the behaviour of m2 based on m1 . i guess i 'm trying to find out whether it is impossible , due to inherent differences in the hardware ( not the exact same chip , even if from same manufacturer ) akin to a genetical difference . it 's a bit like the rhetorical `` if two humans grew up in the *exact* same environmnent , would they behave in the same way '' , though i would assume it 's technically impossible to do such a test . if anyone could shed some answer i would appreciate it , thank you : ) peace <eoq> you mean would their <unk> error be the same for the same algorithms ? <eoa> 
can two machine-learning computers be identital ? hi , i have a question that 's been eating me for some time and do n't know where else to post it . so here it is : assuming that two computers ( m1 & m2 ) are fed the exact-same training data , would they behave identically , to the point where we can accurately assess/predict the behaviour of m2 based on m1 . i guess i 'm trying to find out whether it is impossible , due to inherent differences in the hardware ( not the exact same chip , even if from same manufacturer ) akin to a genetical difference . it 's a bit like the rhetorical `` if two humans grew up in the *exact* same environmnent , would they behave in the same way '' , though i would assume it 's technically impossible to do such a test . if anyone could shed some answer i would appreciate it , thank you : ) peace <eoq> the hardware differences will not affect your algorithm in any way ( other than how quickly it <unk> of course ) . <eoa> 
can two machine-learning computers be identital ? hi , i have a question that 's been eating me for some time and do n't know where else to post it . so here it is : assuming that two computers ( m1 & m2 ) are fed the exact-same training data , would they behave identically , to the point where we can accurately assess/predict the behaviour of m2 based on m1 . i guess i 'm trying to find out whether it is impossible , due to inherent differences in the hardware ( not the exact same chip , even if from same manufacturer ) akin to a genetical difference . it 's a bit like the rhetorical `` if two humans grew up in the *exact* same environmnent , would they behave in the same way '' , though i would assume it 's technically impossible to do such a test . if anyone could shed some answer i would appreciate it , thank you : ) peace <eoq> i think it depends if the algorithm used is deterministic or not . if it uses <unk> in some way , then there 's no way to <unk> that the results will be exactly same . <eoa> 
<unk> as individual <unk> what happens if you feed in your whole <unk> of facebook to the <unk> as sequence to sequence model whereas the <unk> it the text i wrote and the input the text my <unk> wrote . will the result of the training be a <unk> which is using language like me ? <eoq> i doubt it . the biggest challenge in nlp is actually understanding the context , not just following the <unk> rules of humans . also , i imagine the training data is not going to be large enough since i assume most of the <unk> are unique and <unk> depends on the situation . of course it will output in the language of <unk> , but it wo n't effectively answer the question . <eoa> 
question about types of neural nets hey all ! so i have one main question about the different types of neural nets that exist . it seems like a lot of different types of nns <unk> under the <unk> term of neural net and i was wondering what kinds there are . i 'm familiar with a cnn , an rnn and so forth . do those employ <unk> ? the only one i 've had experience with has been the one as done in the andrew ng coursera lectures ( feedforward with <unk> not sure if this is its own type of neural net or if this is just a general term ) i 'm also aware that many different types of cost functions exist ? does this actually change the type of net or is this really just a different way to <unk> `` cost '' ? <eoq> those are the 3 main types of nets that you will usually see . also , there are variations of these ( lstm , <unk> , etc are types of rnns ) . <eoa> 
question about types of neural nets hey all ! so i have one main question about the different types of neural nets that exist . it seems like a lot of different types of nns <unk> under the <unk> term of neural net and i was wondering what kinds there are . i 'm familiar with a cnn , an rnn and so forth . do those employ <unk> ? the only one i 've had experience with has been the one as done in the andrew ng coursera lectures ( feedforward with <unk> not sure if this is its own type of neural net or if this is just a general term ) i 'm also aware that many different types of cost functions exist ? does this actually change the type of net or is this really just a different way to <unk> `` cost '' ? <eoq> there are <unk> many types of neural networks . check out the wikipedia page on [ recurrent neural networks ] ( https : <unk> ) . the <unk> of rnns are [ feedforward nns ] ( https : <unk> ) . this is one of the more <unk> <unk> since <unk> are <unk> <unk> graphs and rnns contain at least one <unk> . however , i would say that a recurrent mlp is more similar to a feedforward <unk> perceptron than it is to a <unk> network ( which is also recurrent ) . basically you can come up with all kinds of variations by adding , <unk> , <unk> and <unk> connections or changing some activation functions . it 's not always clear when a change <unk> a new `` type '' of network . what most of these networks have in common is the the node 's activation is given by an activation function applied to the weighted sum of other nodes ' activations , although some nodes in <unk> compute the product . this is different in [ <unk> neural networks ] ( https : <unk> ) , but they are <unk> used since they are hard to train . i would say [ boltzmann machines ] ( https : <unk> ) are also quite different . most of the time some form of backpropagation is used . i think backpropagation technically refers to a fairly specific algorithm , and people often use <unk> like <unk> or <unk> instead , but it 's still all about <unk> an error back <unk> the network . things like <unk> and dropout and other <unk> <unk> can be added . it 's also possible to use other optimization techniques ( like genetic algorithms ) . <unk> learning ( increase weight between nodes that are <unk> active ) is not used much anymore i think . ( restricted ) boltzmann machines have their own training procedure that uses gibbs sampling and gradient descent . i would say that the used cost function is probably more part of the training procedure than it is of the network . finally , i should mention that there are also computational neuroscience projects that basically <unk> to actually simulate parts of the brain ( like <unk> 's blue brain project ) . these are technically also neural networks , but they are also pretty different . <eoa> 
sklearn pca not making sense to me . i 'm under the <unk> that if you have a 2d data set and you perform pca on it , without any dimensionality reduction it will essentially <unk> your data to a new <unk> system . why then when i attempt this in sklearn does the data seem like its being <unk> in some way ? here is my code : import numpy as <unk> from <unk> import pca import <unk> as <unk> x = <unk> ( <unk> ) y = x + <unk> ( 1 , 101 ) y = y [ 0 ] data = <unk> ( <unk> ( x , y ) ) pca = pca ( <unk> ) new_data = <unk> ( data ) <unk> ( 0 ) <unk> ( data [ : <unk> ] , data [ : , 1 ] ) <unk> ( 1 ) <unk> ( new_data [ : <unk> ] , new_data [ : , 1 ] ) <unk> ( ) <eoq> i think it may just be <unk> that way because of the different <unk> . when i look at it the corresponding points seem to be in the correct <unk> . add <unk> = range ( 101 ) , and pass in <unk> to <unk> ( ) and you 'll see it more easily . <eoa> 
reinforcement learning help ! im learning reinforcement learning . can someone provide me with some example problems which i can try out and improve my understanding . thank you ! <eoq> [ <unk> ] ( http : <unk> ) is an open source <unk> for performing sequential decision making <unk> in python . it has a bunch of [ example <unk> ] ( https : <unk> ) . <eoa> 
reinforcement learning help ! im learning reinforcement learning . can someone provide me with some example problems which i can try out and improve my understanding . thank you ! <eoq> in [ the book by <unk> & <unk> ] ( https : <unk> ) there are sometimes scenarios used in the <unk> , as well as some case studies in the end , which you can use to create some own problems . did you mean something like this ? <eoa> 
ml newbie : how to approach this problem , given a screenshot , find areas of interest or <unk> . suggestions ? **the problem ( in detail ) : ** given a screenshot , i want to be able to detect areas where a user would typically <unk> - whether that is through <unk> or <unk> . i would want my output to be somewhat similar to an ocr <unk> . <unk> 1 - ocr like document analysis : ** use some kind of modified version of [ document <unk> analysis ] ( https : <unk> ) more suited to this problem . **the problem : ** i really have no idea how to even begin here . <unk> 2 - attempt to learn areas of interest : ** my idea would be to download a bunch of web pages , screenshot them , and use that as training data because areas of <unk> can be <unk> fairly easily in a webpage ( as either an input , <unk> , or button <unk> ... it does n't get everything , but may be good enough ) . **the problem : ** my hypothesis is that something like a neural net would not be able to <unk> interactive and <unk> elements with a high enough accuracy . another problem , even if i think it could be accurate , how could i <unk> this as a classification problem to be useful to me ? i.e . my output would just tell me whether or not that screenshot contains an interactive element ? i 'm looking at this the wrong way but i ca n't see another approach . i 'm definitely lost , so any pointers would be greatly appreciated ! <eoq> you are like 90 % of the way to a complete solution . it looks like what you really need is a classifier that , given an image , produces <unk> <unk> around the interactive elements . <unk> algorithms to both detect ( decide if an <unk> exists in the image ) and <unk> ( <unk> <unk> <unk> around the <unk> ) are quite common in computer vision . its called `` object detection '' or `` object recognition '' , and its a common enough thing that you could find tutorials about online . see the <unk> <unk> challenge for more <unk> edge algorithms . i am not an expert in this field but if you <unk> around you should be able find implementations of such algorithms , <unk> ones that use deep learning and ones that do n't . as you say , you could then imagine <unk> <unk> , identifying the interactive elements by <unk> the html ( in practice this might be the hard part ) , and then using that as training data for your classifier . since you have potentially <unk> training data , and i would guess that this is a difficult but not extremely difficult classification problem , my guess is that that approach could achieve very good accuracy . note , of course , it is <unk> to only work for screen <unk> of <unk> , not screen <unk> in general , since that is where what your training data coming from . <eoa> 
unsupervised learning with <unk> i have a huge amount of respect for computational graph frameworks , and would love to start working with them more , but most of what i want to do is unsupervised or generative . i 've not been able to figure out what kind of loss function to use or how to <unk> them to <unk> what i want , so i 've always been <unk> to writing my own libraries . a google search yields a few pages from <unk> <unk> they define an rbm loss function , but that 's really about it . the rest are just links to frameworks . i realize the <unk> trend is to do unsupervised <unk> on the bottom layers of the network and <unk> a <unk> network on top , but i really want to do unsupervised all the way to the top . are there good resources or tutorials on using any of these frameworks ? any advice for writing loss functions there ? edit : or am i <unk> to only using <unk> with graph frameworks ? <eoq> [ <unk> ] <eoa> 
are <unk> needed if you add bias at each layer ? from what i understand the stacked weight matrices are no longer <unk> to one matrix if you have bias at each layer ... yes or no , what am i missing here ? <unk> , does the same apply if you have normalization at each layer during training and forward pass ? <eoq> let 's say you have a mlp with two inputs ( x1 and <unk> ) and two neurons : neuron 1 : <unk> + <unk> + b1 neuron 2 : <unk> + <unk> + <unk> output : <unk> + <unk> + b = <unk> ( <unk> + <unk> + b1 ) + <unk> ( <unk> + <unk> + <unk> ) + b = <unk> + <unk> + c ( the <unk> for a , b and c are left for the <unk> , but they do not depend on the inputs x1 and <unk> ) . clearly the output is linear with <unk> to the inputs , the only thing we 've <unk> is a <unk> way to determine the linear coefficients a , b and c , which would make the <unk> <unk> . so the answer is no , biases do not add any nonlinearity to the network , activation functions are definitely required . what do you mean by normalization at each layer ? batch normalization ? <eoa> 
[ <unk> question ? ] help with <unk> for an interview ? `` desired qualifications '' . <unk> with machine learning or pattern recognition algorithms <unk> experience with <unk> machine learning <unk> . this includes writing and <unk> machine learning algorithms <unk> programming skills in either python ( preferred ) , java or c++ <unk> in undergraduate or graduate studies in cs , <unk> or related <unk> <unk> with data analytics 1 ) any recommendations on <unk> sections i should know in and out ? 2 ) should i focus on those things or focus on typical `` software engineering '' questions ? like big-o of <unk> sort algorithms , writing data structures and all that . <eoq> > any recommendations on <unk> sections i should know in and out ? you should probably read the [ docs ] ( http : <unk> # docs ) that <unk> has made available about <unk> and <unk> . i would probably focus here , because apparently they use this . all of the other desired qualifications are `` experience with <unk> '' , which you ca n't really get in a short amount of time . i mean , you could take some intro to ml courses on coursera , <unk> or udacity , but it sounds like that will not be advanced enough for this job . i like <unk> <unk> 's [ pattern recognition and machine learning ] ( http : <unk> ) book , but it 's also not something you <unk> through . it 's probably a lot more <unk> to read the [ wikipedia page ] ( https : <unk> ) and make sure that you at least know what they 're talking about with all of the approaches . i would probably try to figure out some specific things about the company . are they doing computer vision ? then read a little bit about that . are they focusing on <unk> recognition ( i 'm just making stuff up here ) ? then so should you . what kind of algorithms do they use aside from <unk> ? > should i focus on those things or focus on typical `` software engineering '' questions ? like big-o of <unk> sort algorithms , writing data structures and all that . <unk> from the desired qualifications i probably would n't <unk> on this . they seem to care much more about ml in particular than they do about more general software engineering and computer science stuff . i 'd worry more about knowing various ml algorithms than the traditional cs <unk> . it 's good to know the basics though . learning the <unk> ( big-o ) of these algorithms and data structure operations should not take too much time , so it might be <unk> . <eoa> 
need help in audio signal processing so i am implementing a project on determining a <unk> <unk> from its song ( <unk> file ) i figured [ this method ] ( http : <unk> ) can be tried out ... i need help on implementing it on python ... i 'd give anything to anyone who gets it <unk> <eoq> you might want to check out [ librosa ] ( https : <unk> ) . it 's a python library that takes care of the audio processing ( by way of another library [ <unk> ] ( https : <unk> ) ) and also implements many of the common audio features . check out the tutorial . librosa makes it <unk> to get a [ <unk> ] ( http : <unk> ) and transform it into the [ <unk> domain ] ( http : <unk> ) . the filter bank mentioned sounds similar to a <unk> filter bank which is included in librosa . i did n't know we had <unk> curves for <unk> , that 's awesome . if you have that it should n't be too hard to apply it to the <unk> , once you 're familiar with the matrix representation . <unk> this helps . good luck ! <eoa> 
after 30 hours , i still ca n't figure out how to properly implement a backpropagation algorithm . help please . i spent nearly 30 hours thinking , reading , and trying to implement this . everything is correct . i checked them a hundred times , so there is no problem in the logic as far as i understand it . below are the issues i 'm facing . 1- so i want to approximate a function ( x^2 , sinx , ... ) using a neural network , but how do i make this work ? i have one input neuron , variable hidden neurons , and one output . how would my network ever produce the correct outputs if the sigmoid function has a range from 0 to 1 ? 2- my network ca n't even figure out the xor problem with 2 hidden neurons , two inputs , and one output . but the forward pass works if i manually set the weights and use a threshold activation function for both hidden and output layers . 3- it always produces the same output for any input . <eoq> code for weight correction in the hidden layer ( modified for <unk> ) , where i <unk> it 's wrong : <eoa> 
after 30 hours , i still ca n't figure out how to properly implement a backpropagation algorithm . help please . i spent nearly 30 hours thinking , reading , and trying to implement this . everything is correct . i checked them a hundred times , so there is no problem in the logic as far as i understand it . below are the issues i 'm facing . 1- so i want to approximate a function ( x^2 , sinx , ... ) using a neural network , but how do i make this work ? i have one input neuron , variable hidden neurons , and one output . how would my network ever produce the correct outputs if the sigmoid function has a range from 0 to 1 ? 2- my network ca n't even figure out the xor problem with 2 hidden neurons , two inputs , and one output . but the forward pass works if i manually set the weights and use a threshold activation function for both hidden and output layers . 3- it always produces the same output for any input . <eoq> please , can someone at least give me the correct weights and biases for a <unk> network that uses sigmoid ( <unk> ( <unk> ( <unk> ) ) for all neurons ? it 's <unk> me <unk> . i just want to know where the error is . <eoa> 
after 30 hours , i still ca n't figure out how to properly implement a backpropagation algorithm . help please . i spent nearly 30 hours thinking , reading , and trying to implement this . everything is correct . i checked them a hundred times , so there is no problem in the logic as far as i understand it . below are the issues i 'm facing . 1- so i want to approximate a function ( x^2 , sinx , ... ) using a neural network , but how do i make this work ? i have one input neuron , variable hidden neurons , and one output . how would my network ever produce the correct outputs if the sigmoid function has a range from 0 to 1 ? 2- my network ca n't even figure out the xor problem with 2 hidden neurons , two inputs , and one output . but the forward pass works if i manually set the weights and use a threshold activation function for both hidden and output layers . 3- it always produces the same output for any input . <eoq> 1 - you just give up sigmoid for the output layer . keep sigmoid for hidden units only . <eoa> 
after 30 hours , i still ca n't figure out how to properly implement a backpropagation algorithm . help please . i spent nearly 30 hours thinking , reading , and trying to implement this . everything is correct . i checked them a hundred times , so there is no problem in the logic as far as i understand it . below are the issues i 'm facing . 1- so i want to approximate a function ( x^2 , sinx , ... ) using a neural network , but how do i make this work ? i have one input neuron , variable hidden neurons , and one output . how would my network ever produce the correct outputs if the sigmoid function has a range from 0 to 1 ? 2- my network ca n't even figure out the xor problem with 2 hidden neurons , two inputs , and one output . but the forward pass works if i manually set the weights and use a threshold activation function for both hidden and output layers . 3- it always produces the same output for any input . <eoq> on mobile so please forgive <unk> errors . i 'll try to give a more <unk> answer when i get to my <unk> . in the <unk> : 1 : you can either try linear output nodes instead of sigmoid , or just scale your answers by some constant that allows signal outputs to generate answers in the range you want . stick to approximately <unk> to <unk> , not 0 ... 1 . 2 : a single hidden layer <unk> with two hidden nodes can solve xor , but backpropagation is n't <unk> to find that answer . try more nodes and different initialization . do not initialize weights to 0 . <eoa> 
[ work question ] what does the <unk> of someone working in machine learning look like ? what do you love the most about your job ? what is your least favorite thing ? what steps have you taken to get to your <unk> ? what are personal <unk> that are necessary to <unk> in machine learning ? if you could give a piece of advice to a <unk> person <unk> a <unk> in machine learning , what advice would you give ? <eoq> i would n't say i every really worked in ml , but i have had work where i made <unk> use of ml . currently i 'm a phd student in ai , which means that i 'm mostly just reading a lot of articles and then <unk> implementing some algorithms to try them out , but ml is n't my focus . i used to work for a small computer vision company where i worked on <unk> software for <unk> , which included stuff like <unk> expression recognition , video <unk> , <unk> <unk> <unk> , object <unk> and behavior analysis . when a project started we would have some <unk> about what the client wanted , what we expected to be able to <unk> , and how we might be able to tackle the challenges . we would usually have some initial ideas of what techniques could be used , after which i would <unk> into the <unk> literature about those techniques ( if it was my project ) . then at several points in time i would present my ideas and progress to the rest of the team , who would then give feedback . also at different points in time the project would get <unk> in several ways because the client wanted something different , or some algorithm <unk> out to really not work , or we <unk> our <unk> of what was possible , etc . i probably spent most of my time programming . after <unk> what techniques to use , they needed to be implemented . you can read about these things in <unk> papers , but it 's ( almost ) never quite what you need , so you have to <unk> it to your specific situation and problem , and usually this would involve <unk> together multiple ml algorithms . that also means that you have multiple points of failure , and that you need to train a bunch of separate algorithms . a lot of time was also spent <unk> or <unk> data , training the algorithms and then testing them . this often <unk> <unk> separate <unk> and training and training tools , and then actually doing the <unk> . sometimes we could tell the client what kind of setup they should use ( i.e . number and kind of <unk> and computers ) , which would mean researching what was <unk> and building the setup . otherwise , i <unk> <unk> some <unk> in order to get data from their actual <unk> , or i 'd try to recreate <unk> it to get data that was somewhat similar . once i had all of that , i could bring it all together in some piece of software for the client . even if you have all of the ideas , this still takes quite a bit of time . if you have a whole <unk> of algorithms , it can be difficult to get them to work together , and there is always something that is <unk> or too slow , which means you might have to <unk> it or <unk> more <unk> it or change the setup ( if that 's an option ) . finally , you need some nice way of <unk> the results for the client , and the program needs to actually be usable by other people who are not necessarily experts . this is usually pretty easy compared to the other things , but it can still take a lot of time . aside from that it 's just regular company and <unk> stuff . <unk> , some <unk> and reporting , <unk> tools for <unk> use , trying to <unk> the <unk> stack , <unk> the website , learning new stuff , organizing and <unk> events , and generally just having a nice time with <unk> ... <eoa> 
in machine learning is it necessary to take software engineering ? there 's a statistical and machine learning major which i 'm going into but i would like to <unk> it with either a language technology minor or a software engineering minor . right now i 'm not sure which would come in more useful in the field . <eoq> when you say `` the field '' , what field do you mean ? software engineering ( or at least programming ) is going to be relevant for all machine learning , no matter what direction you go . in fact , it is so relevant that i expect your major will already include quite a bit of it . i do n't know too much about language technology <unk> , but it sounds like it might be useful if you want to do natural language processing . if you want to do nlp , or want to learn something more `` unique '' , then you should probably that language technology minor ( you 'll have to learn to program anyhow ) . if you just want to do ml in general , then software engineering is the most obvious choice . <eoa> 
can i use machine learning to recognize professional photography ( vs amateur photography ) ? i 'm a complete beginner and have no experience with machine learning , i was wondering if it would be possible for me to train some kind of neural network to recognize professional photography reliably . an example of photos that should rate very high as professional : https : //www.flickr.com/photos/steamster/sets/72157656521743602 what kind of a dataset would i need to achieve this ? and how much machine learning do i need to learn to implement it ? ( i currently know a decent amount of basic programming in python and java ) <eoq> so actually , you 're going to need to use a neural network because you 're dealing with complex non-linear hypothesis . the theory is similar , but the the implementation is different . i ca n't help there yet . <eoa> 
can i use machine learning to recognize professional photography ( vs amateur photography ) ? i 'm a complete beginner and have no experience with machine learning , i was wondering if it would be possible for me to train some kind of neural network to recognize professional photography reliably . an example of photos that should rate very high as professional : https : //www.flickr.com/photos/steamster/sets/72157656521743602 what kind of a dataset would i need to achieve this ? and how much machine learning do i need to learn to implement it ? ( i currently know a decent amount of basic programming in python and java ) <eoq> i 'd say this is a pretty big problem . should be possible but might need some <unk> or even years of research . <eoa> 
can i use machine learning to recognize professional photography ( vs amateur photography ) ? i 'm a complete beginner and have no experience with machine learning , i was wondering if it would be possible for me to train some kind of neural network to recognize professional photography reliably . an example of photos that should rate very high as professional : https : //www.flickr.com/photos/steamster/sets/72157656521743602 what kind of a dataset would i need to achieve this ? and how much machine learning do i need to learn to implement it ? ( i currently know a decent amount of basic programming in python and java ) <eoq> what you have to first understand is <unk> vs model choice . <eoa> 
can i use machine learning to recognize professional photography ( vs amateur photography ) ? i 'm a complete beginner and have no experience with machine learning , i was wondering if it would be possible for me to train some kind of neural network to recognize professional photography reliably . an example of photos that should rate very high as professional : https : //www.flickr.com/photos/steamster/sets/72157656521743602 what kind of a dataset would i need to achieve this ? and how much machine learning do i need to learn to implement it ? ( i currently know a decent amount of basic programming in python and java ) <eoq> yes you could . there 's something called a classification algorithm , or logistic regression . fancy words that just mean `` is it probably this or is it probably that '' ? the more training examples you provide , the more accurate your results will get usually , and for this example you would benefit from having a bunch of amateur photos as well . since there will be a large number of features , you wo n't really be aware of what the algorithm knows , but you would be able to tell if one matches your training set , or does n't . maybe someone else can provide more details , because i do n't know exactly how to work with photos yet . i just know how to work with data , but with these photos the algorithms will simply look for patterns in the pixels . for <unk> <unk> , read slowly : basically , what you do is provide a set of training examples with several features that the algorithm can focus on , then you provide it a <unk> answer . so you say , this is professional . this is not . the algorithm then <unk> something called a hypothesis function that tries to match as many of the features observed in the training examples as possible , based on a set of numbers called parameters or weights , that when <unk> into the hypothesis , can predict if the data you 're <unk> it falls in the <unk> side or the not <unk> side . it <unk> something called a cost function which <unk> out how different your hypothesis is from the actual data . this cost function is set to be <unk> , or to find the partial derivative of the curve with respect to the parameters ( basically , when the hypothesis differs from the actual data the least ) , and this is done through another function called gradient descent . gradient descent is a <unk> <unk> that takes arbitrary initial values for your weights , calculates the <unk> of the cost function , and <unk> until it <unk> a minimum , meaning the difference between the hypothesis and the actual training examples is <unk> . once you have the right parameters or weights , you can then feed the hypothesis new information , and using those parameters it figured out through gradient descent , it runs a <unk> saying , yes this image likely matches the professional set , or no this image does n't match the professional set . like i said though someone else may be able to help a little bit with how to exactly do this with images . but all in all , it 's just programming a few very simple functions . nothing too <unk> complex . edit : <unk> <unk> and <unk> <eoa> 
basketball ( or any <unk> ) predictions are there generally <unk> algorithms used for predicting the outcomes of sports games ? i 'm getting <unk> to <unk> historic data for <unk> <unk> and want to actually use it for something . i 've come across the <unk> formula , but there has to be something better than a <unk> for <unk> . <eoq> check out this blog : http : <unk> what a <unk> breakdown of many different prediction algorithms ! definitely one of the best <unk> out there on basketball prediction . <eoa> 
ml <unk> here . starting to learn ml on my own . classification technique guidance required . im starting to learn ml by myself . i have a course starting ml from next <unk> , but im too <unk> to learn it before the course <unk> . i always learn by starting to solve a problem and then read up the theory behind it . so in a way , when i solve the problem id have read the theory behind it and applied it at the same time . so now , im trying to classify images into 5 categories . to make my life easier i have even got the feature vectors . i just need to classify them into multiple categories . how do i go about doing this ? what are the various techniques ? how accurate are they ? how do i <unk> the performance ? which programming language should i use ? please help me out with this . thanks in advance ! <eoq> for a complete beginner i 'd recommend playing around with [ scikit-learn ] ( http : <unk> ) in python . the site has a lot of tutorials and <unk> the difference between classification ( binary and <unk> ) and regression . it also <unk> about feature selection and normalization a bit . more than anything , it has the <unk> learning curve of any ml <unk> i 've worked with , and it has great pointers should you want to learn more . also , read up on <unk> . it 's really <unk> to getting good results and comparing models : accuracy is not everything , and sometimes it 's not <unk> . for example , if you were trying to predict whether a car has a <unk> on it from an image , you could probably get <unk> % accuracy by always predicting `` no <unk> '' , since most cars are not <unk> . <eoa> 
class of concepts i have a class of concepts c of the form ( a ≤ x ≤ b ) ^ ( c ≤ y ≤ d ) where a , b , c , d <unk> { 0 , 1 , 2 } . i have two questions . 1. how many distinct concepts c can be <unk> ? 2. what happens if we allow a , b to take also negative <unk> values , that is , a , b <unk> { <unk> , <unk> , 0 , 1 , 2 } while c and d <unk> with values in { 0 , 1 , 2 } ? how many distinct concepts as described above are there in this case ? <eoq> your notation is a little bit <unk> to me . what does ^ mean , and why ca n't you just say that x and y are elements of { <unk> } or { <unk> , <unk> , 0 , 1 , 2 } ? what are you trying to do here ? <eoa> 
general question about data sets with large number of boolean columns . so let 's say i have a data set where each row is a very long list of <unk> and i want to predict the value of certain columns given other columns , but not always necessarily the entire row ( minus the column i 'm predicting ) . so basically i will have a subset of a complete row and would like to predict the target column 's value . what methods should i be focusing on to accurately tackle this problem ? <eoq> a [ bayesian network ] ( https : <unk> ) would probably be ideal , since it lets you input the values you know and then gives you probability distributions over the rest . this can be a fairly good option if you know the structure of your domain so that you can construct the network manually so that only probability distributions need to be learned . otherwise you need to do structure learning as well , which is more difficult . neural networks may also be an option . i 've never seen them used in the <unk> , but it sounds like maybe you could use interactive activation networks ( see [ <unk> 2 here ] ( http : <unk> ) ) , which actually end up <unk> a lot like bayes nets . otherwise you could try using some kind of [ <unk> ] ( https : <unk> ) . these are basically networks that are trained to output their input . if you then <unk> <unk> input , it should <unk> . make your <unk> inputs 1 and -1 so that you can use 0 for missing data . <eoa> 
just started with introduction to statistical learning and i have some basic questions . equation 2.1 states : y = f ( x ) + e quoting : `` here f is some fixed but unknown function of x1 , ... , xp , and e is a random error term , which is independent of x and has mean zero . in this formulation , f represents the systematic information that x provides about y . '' questions : - what do they mean here by systematic information ? - if e has mean zero , then why even include it in the equation ? - are there any scenarios where e is non-zero ? if there are , what are some examples ? - is e a vector ? a set ? a scalar ? thank you . <eoq> let me see if i can help : <eoa> 
just started with introduction to statistical learning and i have some basic questions . equation 2.1 states : y = f ( x ) + e quoting : `` here f is some fixed but unknown function of x1 , ... , xp , and e is a random error term , which is independent of x and has mean zero . in this formulation , f represents the systematic information that x provides about y . '' questions : - what do they mean here by systematic information ? - if e has mean zero , then why even include it in the equation ? - are there any scenarios where e is non-zero ? if there are , what are some examples ? - is e a vector ? a set ? a scalar ? thank you . <eoq> if e has mean zero it means that the error can go on both sides of a regression line more or less with equal probability . i.e . positive and negative error ( positive error as in your f + positive value e , negative error as in your f + negative value e ) will occur normally . <eoa> 
just started with introduction to statistical learning and i have some basic questions . equation 2.1 states : y = f ( x ) + e quoting : `` here f is some fixed but unknown function of x1 , ... , xp , and e is a random error term , which is independent of x and has mean zero . in this formulation , f represents the systematic information that x provides about y . '' questions : - what do they mean here by systematic information ? - if e has mean zero , then why even include it in the equation ? - are there any scenarios where e is non-zero ? if there are , what are some examples ? - is e a vector ? a set ? a scalar ? thank you . <eoq> also , is there a subreddit <unk> to the discussion of this book ? <eoa> 
good data preprocessing python module ? i have to do a lot of data preprocessing for my machine learning task . specifically scaling , <unk> missing values , splitting large <unk> numpy <unk> into training , validation and testing sets , handling <unk> and timeseries data . i have been building small methods to handle the data with mostly sklearn and various numpy functions . is there a good library i can use that will help me with preprocessing data ? is sklearn the best option ? <eoq> for scaling , <unk> missing values and handling <unk> and timeseries data the answer is [ pandas ] ( http : <unk> ) . it has <unk> <unk> as basic objects , a wide variety of tools and methods for handling <unk> or missing data ( filling , <unk> , back filling , forward filling , <unk> , etc . ) , and <unk> <unk> and timeseries <unk> . it also has great tools for sampling , <unk> , <unk> , filtering , <unk> and plotting data , but those are just <unk> at this point . if you do n't know pandas you should learn it . there 's a [ great tutorial ] ( https : //www.youtube.com/watch ? <unk> ) by <unk> <unk> that will get you started , and after that the docs are excellent . <eoa> 
sklearn pca with pandas <unk> when <unk> along a pandas <unk> to a pca ( ) object , how can i tell which of the input vectors are being chosen when using <unk> ' ? pca = <unk> ( <unk> ' ) <unk> ( <unk> ) <eoq> pca does n't choose input vectors . it ( <unk> ) combines them all . <eoa> 
anyone care to take a <unk> at this <unk> classification task ? trying to validate my results . <eoq> i get <unk> % on a 4 x 20 x 1 relu network <eoa> 
anyone care to take a <unk> at this <unk> classification task ? trying to validate my results . <eoq> i 'm trying to validate the results i 'm getting . i <unk> ( or so i think ) an algorithm from a paper that achieved <unk> % accuracy using a deep neural network ( [ 4 , 3 , 2 ] hidden layer ) , but i can not seem to get more than <unk> % . svm gives me <unk> % , a generic mlp model from <unk> <unk> me <unk> % . the dataset has 4 inputs with domain [ <unk> ] . the single output is a binary 0 or 1. there are 1000 samples in the dataset . i recommend you come up with your own <unk> split to validate your model . i used <unk> cross-validation , with <unk> % of the training data set aside for validation . feel free to use any model you wish , but please explain how you got it : o added difficulty : do n't transform the inputs ( i.e . use them as is ) . <eoa> 
[ beginner ] what is a good ml <unk> that contains pseudo code of algorithms ? i have been looking at books . i like <unk> <unk> 's book on machine learning as it has some pseudo code for c4.5 . does anyone else know of other similar books that have pseudo code ? if not , does anybody know where decent <unk> source code is for a ml library ? i took a look at weka and it was n't the easiest code to follow . <eoq> [ introduction to statistical learning with applications in r ] ( http : <unk> ) when i help <unk> interview data <unk> , i <unk> that <unk> should be able to open this book up to any page and explain the concepts . <eoa> 
fraud detection with unsupervised learning i 'll <unk> by saying i am <unk> new to ml . i have a data set of <unk> ( <unk> ) transactions where each row is the <unk> , time of <unk> , credit card number , device used ( <unk> , android , <unk> , etc ) and amount . i need to determine which transactions are likely <unk> and which are n't . i would prefer to approach this with unsupervised learning of some kind , as going through these and manually <unk> ones i <unk> are bad would be super <unk> and not necessarily correct . is there a good approach to this problem with unsupervised learning ? i 've read about random forests and decision trees ( high level <unk> ) and maybe they would do the <unk> ? thanks <eoq> <unk> transactions are likely different from <unk> ' transactions , so you are basically looking for <unk> . you could start looking at some <unk> / <unk> of your data to begin with . you could use clustering ( unsupervised learning ) to determine what are <unk> ' transactions , and then you could find which transactions are not near clusters , and are thus likely fraud . you could start with some simple clustering algorithms such as kmeans or hierarchical clustering approaches . if you really want to do something fancy you could try using <unk> , but i would try easy approaches first . you will have to find some algorithms that will scale to your dataset though ( look for large scale clustering <unk> or something like this ) ... if you have no information about which transactions are fraud ( it sounds like you do n't have access to this information ) you can not train any supervised models such as random forests or decision trees ( those are usually supervised models , unless you are talking about some special models ) . <eoa> 
what type of machine learning or decision making should i start looking into for this task ? thank you in advance for reading this - i know enough to know that i do n't really know anything . but here is what i am trying to <unk> : i have two <unk> of <unk> , let 's call them a and b . a is a mostly <unk> group of people with the normal traits a person would have , <unk> , <unk> , <unk> , specific <unk> in some form of education , a number of years <unk> in our project , etc . they also have a history we could <unk> of <unk> and outcomes we could teach a system with . b is an ever changing population of people with the same traits but with the addition that we want them to take an action . let 's say we want them to read more books , or <unk> more . the `` game '' is that a person from a is `` up '' to talk to someone and try to convince them to take an action . i am wondering what sort of machine learning or approach to ai would watch and learn the <unk> and <unk> in population a and make the best decision on who from population b they should be <unk> up with . what would most likely result in an action taken . i simply have no idea on what direction to head with this ? a few word answer would get me going for some time on my own . maybe i am <unk> but i have always been <unk> by ai since i read <unk> <unk> 's <unk> of the mind <unk> ago . i think that was the name . i would love to <unk> <unk> some sort of machine learning in this task . <unk> aside from a <unk> and some more light reading i <unk> more into <unk> programming : ( thanks for any help , even if it is `` your <unk> , this is not something for machine learning '' thank you , <unk> <eoq> if you have data on 'successful ' <unk> . that is when a <unk> of particular a <unk> in getting a particular <unk> of b to perform an action then i would model this as classification problem for edge prediction on a graph . you have a <unk> graph of a 's and b 's and you have your known edges , when an a has successfully <unk> a b for a task . you then use this data to train a model to look for <unk> connections . you can either model each edge as 'successful ' ( binary classification ) or each edge as 'successful for action k ' then a multiclass problem . either way this differs from normal classification in that you are using attributes of both a and b to make the prediction . this may make an advantage over just modelling a to predict if they can <unk> any b . have a look at this paper : http : <unk> its a biological paper so you need to read through the <unk> but could be useful . <eoa> 
[ beginner ] error rate stays same from beginning to end i started learning nn . after some reading of theory i stumbled upon this [ blog ] ( http : //iamtrask.github.io/ ) and decided to follow it and implement by myself . i am trying to use iris dataset ( iris-setosa and iris-vericolor ) but it seems that my nn stays on 0.5 error rate . i tried to change alpha , hidden layer size but it seems that in the end error rate is equal 0.5. code is [ here ] ( http : //pastebin.com/1aamvqg9 ) . output : error ( iterations : 0 ) : 0.436383143672 error ( iterations : 10000 ) : 0.500000789314 error ( iterations : 20000 ) : 0.500000195026 error ( iterations : 30000 ) : 0.50000011538 error ( iterations : 40000 ) : 0.500000692513 error ( iterations : 50000 ) : 0.500000173484 error ( iterations : 60000 ) : 0.500000148476 error ( iterations : 70000 ) : 0.500000059061 error ( iterations : 80000 ) : 0.500000134286 error ( iterations : 90000 ) : 0.500000466787 any help or suggestions ? <eoq> if its a balanced binary classification problem then the expected error rate of a random guess should be 0.5 . <eoa> 
[ beginner ] error rate stays same from beginning to end i started learning nn . after some reading of theory i stumbled upon this [ blog ] ( http : //iamtrask.github.io/ ) and decided to follow it and implement by myself . i am trying to use iris dataset ( iris-setosa and iris-vericolor ) but it seems that my nn stays on 0.5 error rate . i tried to change alpha , hidden layer size but it seems that in the end error rate is equal 0.5. code is [ here ] ( http : //pastebin.com/1aamvqg9 ) . output : error ( iterations : 0 ) : 0.436383143672 error ( iterations : 10000 ) : 0.500000789314 error ( iterations : 20000 ) : 0.500000195026 error ( iterations : 30000 ) : 0.50000011538 error ( iterations : 40000 ) : 0.500000692513 error ( iterations : 50000 ) : 0.500000173484 error ( iterations : 60000 ) : 0.500000148476 error ( iterations : 70000 ) : 0.500000059061 error ( iterations : 80000 ) : 0.500000134286 error ( iterations : 90000 ) : 0.500000466787 any help or suggestions ? <eoq> could n't <unk> an obvious error in the code . i do n't speak alot of python though . <eoa> 
[ beginner ] error rate stays same from beginning to end i started learning nn . after some reading of theory i stumbled upon this [ blog ] ( http : //iamtrask.github.io/ ) and decided to follow it and implement by myself . i am trying to use iris dataset ( iris-setosa and iris-vericolor ) but it seems that my nn stays on 0.5 error rate . i tried to change alpha , hidden layer size but it seems that in the end error rate is equal 0.5. code is [ here ] ( http : //pastebin.com/1aamvqg9 ) . output : error ( iterations : 0 ) : 0.436383143672 error ( iterations : 10000 ) : 0.500000789314 error ( iterations : 20000 ) : 0.500000195026 error ( iterations : 30000 ) : 0.50000011538 error ( iterations : 40000 ) : 0.500000692513 error ( iterations : 50000 ) : 0.500000173484 error ( iterations : 60000 ) : 0.500000148476 error ( iterations : 70000 ) : 0.500000059061 error ( iterations : 80000 ) : 0.500000134286 error ( iterations : 90000 ) : 0.500000466787 any help or suggestions ? <eoq> try <unk> = l2 - y ? <eoa> 
can a nn be <unk> to multiply two numbers ? hi , i 've been playing with regular neural networks with relu input , hidden and output units . i have successfully trained it to add two real numbers , just <unk> binary functions for fun , of the form f ( x , y ) = z . the input is 4 numbers [ <unk> <unk> <unk> <unk> ] ex . f ( <unk> , 8 ) becomes f ( [ <unk> 0 0 8 ] ) . output is similar [ <unk> <unk> ] addition is then simply to learn direct <unk> matrix [ 1 -1 1 -1 ] [ -1 1 -1 1 ] i 've tried similar approach to learn general multiplication , but with no luck . ie . i expect it to work for any real numbers , it sort of works for numbers within a limited range , but the results become too low for large numbers it has not seen before . something <unk> me it can not be done to learn general multiplication ... yes or no ? <eoq> i 'm afraid your model does not fit the problem . you might want to think about what kind of functions you can model with a three layer relu feedforward net . as a first step : what is the <unk> neural network that can be trained to execute general addition of real numbers ? ( you might need to use another activation function ) <eoa> 
what is the best way to <unk> the vanishing gradient problem in neural networks ? i 'm relatively new to neural nets and machine learning in general , and recently stumbled <unk> this problem when <unk> to add more layers to a network used to classify digits . as i added more layers , my accuracy <unk> . i have heard of a few ways this is <unk> , but was wondering if someone could <unk> what the best techniques were , and what the pros and cons of each of them were . <eoq> relu units are the most common solution . basically the idea of these is to have a non-linear function <unk> gradient is either on or off , that way it <unk> the error signal all the way down without <unk> it at each layer . i do n't know enough myself to talk about pros and cons of different methods . <eoa> 
what 's the <unk> part about training ml algorithms ? is it getting the massive datasets and <unk> them ? or is it waiting for the algorithms to finish ? or is it <unk> against overfitting ? <eoq> <unk> > overfitting > waiting <eoa> 
cnn with engineered features if i am working with a data set of <unk> , <unk> pixels each , and i want add an engineered feature like <unk> the user wearing glasses ' . would it be reasonable to add a new row , <unk> pixels , with a binary value to <unk> glasses or not . what 's the pro / <unk> of this approach ? thanks <eoq> i suppose that an advantage of this approach is that it is easy to implement . it 's usually a good idea to try the simplest approach first and see if the result is good enough for you before you <unk> more time and <unk> developing something more complicated . <eoa> 
cnn with engineered features if i am working with a data set of <unk> , <unk> pixels each , and i want add an engineered feature like <unk> the user wearing glasses ' . would it be reasonable to add a new row , <unk> pixels , with a binary value to <unk> glasses or not . what 's the pro / <unk> of this approach ? thanks <eoq> ( mostly just <unk> what <unk> said ) adding just row is a bit odd because only some convolutional filter applications will <unk> ' the feature . so , if you add the indicator as the <unk> row , convolutional filters applied to the 1st row will take only pixels as input while convolutional filters applied to the last rows will take both pixels and the <unk> as input , which is probably a bad thing considering those filters will have their weights <unk> . instead i would add the an extra channel to the input , so instead of <unk> input <unk> to a <unk> input , where the second channel is binary value <unk> . this paper did exactly that to <unk> information about player <unk> level when it comes to playing go : http : <unk> a more efficient but maybe harder to implement method could be to add the indicator with a weight to the output of the first convolutional layer before the non-linear functions is applied . so you could perform convolution on the input , add <unk> where `` w '' is a new parameter and `` b '' is an indicator to each output unit , then apply your activation function and proceed as normal . <eoa> 
is over-training a risk when a person <unk> lots of <unk> in their preferred email <unk> as <unk> spam ? let 's say that i receive on the order of 50 <unk> a day . if i <unk> make sure that every one every day is correctly <unk> as spam vs. not spam , is over-training a risk ? if so , how do i know at what point to stop training ? my knowledge level : i 'm can describe a bit how naive bayes works , i can define the term over-training , and i know that i should be saying `` <unk> '' instead of `` not spam '' - but the latter seems more appropriate for a `` questions '' subreddit . : ) ( i use <unk> specifically , but i think this is a more general issue that would be <unk> to any email <unk> ? ) <eoq> i have n't really used naive bayes much , but i very much doubt that you 're going to `` <unk> '' your spam filter . the problem with overtraining a classifier is that it can lead to overfitting , which is when random <unk> ( from the training set ) is <unk> instead of the real underlying relationship . this requires that the classifier is flexible enough to model all of those <unk> details ; this is usually called high <unk> bias . variance typically <unk> when the classifier has more <unk> parameters , so e.g . a big neural network has higher variance than a smaller one . naive bayes classifiers <unk> to have fairly low bias and are <unk> less <unk> to overfitting . overtraining <unk> to only be a problem when a classifier 's training procedure goes over the training set multiple times to <unk> <unk> its model . one example is ( again ) a neural network ( <unk> perceptron trained with backpropagation ) . if you have 500 training items and you can <unk> to run <unk> training iterations , then you can train on each item 2,000 times and you might overfit . if your training set is larger ( e.g . 5,000 ) then you can train less on each item ( e.g . 200 times ) and you are less likely to overfit . in fact , even if you do train each item 2,000 times as well , overfitting is still less likely because it 's more difficult to <unk> all the details of 5,000 items than for <unk> expanding your training set <unk> always leads to less overfitting . with naive bayes you really only consider each training item once . the only way to increase the number of training iterations ( and potentially run the risk of overtraining ) is to expand the data set . it 's probably not <unk> impossible to <unk> up a classifier by expanding the training set . for instance , if you add ( almost ) the same item a million times ( and few other items ) , that will probably do the <unk> . but you should be fine if the <unk> that you flag form a sample that fairly represents the whole <unk> of <unk> that you 're getting . in ml terms you want your sample to be independent and identically <unk> ( <unk> . ) . <unk> ; <unk> : <unk> more <unk> probably makes your spam filter more accurate , not less . <eoa> 
what do you use for gradient descent ? do you implement your own or use <unk> software ? as i understand it , there is software that implements cross-validation and gradient descent for you , and all you have to do is <unk> the cost function and its derivative . i was wondering , do most people implement their own , or use <unk> <unk> for it ? <eoq> scikit-learn <unk> <unk> . <eoa> 
what do you use for gradient descent ? do you implement your own or use <unk> software ? as i understand it , there is software that implements cross-validation and gradient descent for you , and all you have to do is <unk> the cost function and its derivative . i was wondering , do most people implement their own , or use <unk> <unk> for it ? <eoq> i use <unk> inside of matlab . <eoa> 
what is the best classifier from the ones tested here ? for <unk> , i 'm a complete beginner in this field and i 'm just <unk> my <unk> into this <unk> of knowledge . and now , what i 'm trying to do is to test and understand which is the `` best '' classifier and the one who performed best from the below list of classifiers ( [ <unk> the <unk> ] ( https : <unk> ? <unk> ) ) - i 've <unk> what the graphs mean , in detail , down below . to put this into context , i 'm trying to write a supervised-learning application with python and sklearn and what i am trying to do is find the right classifier for correctly classifying a resume from a list of resumes . so far , my learning algorithm has these 2 <unk> : * pre-processing * model training and then prediction based on the trained model . the pre-processing phase is where i made all of the <unk> and tried different methods before <unk> both a countvectorizer matrix and a tfidfvectorizer matrix and compared the `` performance '' of the classifiers trained with them . the distinct parts of my pre-processing phase are simply using a tfidfvectorizer and countvectorizer , or in combination with the following : * stemming ( with lancaster / snowball <unk> from <unk> ) * word correction using [ peter norvig 's approach ] ( http : <unk> ) so , for training my model i 've tried combinations of all of these ( which can be found in the link ) : * a simple countvectorizer over my training text * a simple tfidfvectorizer ... over my training text * lancaster stemming + countvectorizer ( ... over my training text ) etc . * snowball stemming + countvectorizer * lancaster stemming + tfidfvectorizer * snowball stemming + tfidfvectorizer * lancaster stemming + peter norvig 's word correction algorithm + countvectorizer * snowball stemming + peter norvig 's word correction algorithm + countvectorizer * lancaster stemming + peter norvig 's word correction algorithm + tfidfvectorizer * snowball stemming + peter norvig 's word correction algorithm + tfidfvectorizer and in order to have a better overview of how my classifiers would perform in a dynamic context ( resumes can have more or less the same number of words ) , i 've used a variable value for <unk> ( minimum document frequency ) to range from <unk> - the number of documents in the test scenario to max ( word document frequency ) - which is what is <unk> on the graphs . so , actually i 'm testing the `` performance '' of my models and how they work with different numbers of training features . as mentioned at the beginning , i 'm a complete beginner and i 've <unk> the classifiers to test these based on suggestions from people both here on reddit and on other <unk> related to ml . one of my main questions , which i hope to find an answer to here is if some classifiers would be considered overfitting , since this is not currently clear to me . for example , i can understand that the multinomial naive bayes is overfitting for most of the countvectorizer approaches . * <unk> is too `` <unk> '' , which i 'm guessing makes it a poor choice of a classifier for this scenario . * but how about <unk> naive bayes ? would it be considered a `` good '' classifier ? does it overfit at the beginning but then become a more `` real '' and better performing classifier towards the end , even if it 's prediction rate is not 100 % ? * same thing for gaussian naive <unk> also , some people suggested that lda ( <unk> <unk> allocation ) is a good <unk> for this kind of scenario . i 'm hoping that someone could help me make some sense out of my results , since i 'm a bit confused on how i should interpret them . and if there 's anyone interested in how i actually did this , with code , you can find it [ <unk> <unk> ] ( https : <unk> ) . disclaimer : i 'm a terrible programmer with a very <unk> way of writing code . and in the end , thank you in advance for reading this `` <unk> '' and your help is greatly and immensely appreciated ! <eoq> http : <unk> <eoa> 
what is the best classifier from the ones tested here ? for <unk> , i 'm a complete beginner in this field and i 'm just <unk> my <unk> into this <unk> of knowledge . and now , what i 'm trying to do is to test and understand which is the `` best '' classifier and the one who performed best from the below list of classifiers ( [ <unk> the <unk> ] ( https : <unk> ? <unk> ) ) - i 've <unk> what the graphs mean , in detail , down below . to put this into context , i 'm trying to write a supervised-learning application with python and sklearn and what i am trying to do is find the right classifier for correctly classifying a resume from a list of resumes . so far , my learning algorithm has these 2 <unk> : * pre-processing * model training and then prediction based on the trained model . the pre-processing phase is where i made all of the <unk> and tried different methods before <unk> both a countvectorizer matrix and a tfidfvectorizer matrix and compared the `` performance '' of the classifiers trained with them . the distinct parts of my pre-processing phase are simply using a tfidfvectorizer and countvectorizer , or in combination with the following : * stemming ( with lancaster / snowball <unk> from <unk> ) * word correction using [ peter norvig 's approach ] ( http : <unk> ) so , for training my model i 've tried combinations of all of these ( which can be found in the link ) : * a simple countvectorizer over my training text * a simple tfidfvectorizer ... over my training text * lancaster stemming + countvectorizer ( ... over my training text ) etc . * snowball stemming + countvectorizer * lancaster stemming + tfidfvectorizer * snowball stemming + tfidfvectorizer * lancaster stemming + peter norvig 's word correction algorithm + countvectorizer * snowball stemming + peter norvig 's word correction algorithm + countvectorizer * lancaster stemming + peter norvig 's word correction algorithm + tfidfvectorizer * snowball stemming + peter norvig 's word correction algorithm + tfidfvectorizer and in order to have a better overview of how my classifiers would perform in a dynamic context ( resumes can have more or less the same number of words ) , i 've used a variable value for <unk> ( minimum document frequency ) to range from <unk> - the number of documents in the test scenario to max ( word document frequency ) - which is what is <unk> on the graphs . so , actually i 'm testing the `` performance '' of my models and how they work with different numbers of training features . as mentioned at the beginning , i 'm a complete beginner and i 've <unk> the classifiers to test these based on suggestions from people both here on reddit and on other <unk> related to ml . one of my main questions , which i hope to find an answer to here is if some classifiers would be considered overfitting , since this is not currently clear to me . for example , i can understand that the multinomial naive bayes is overfitting for most of the countvectorizer approaches . * <unk> is too `` <unk> '' , which i 'm guessing makes it a poor choice of a classifier for this scenario . * but how about <unk> naive bayes ? would it be considered a `` good '' classifier ? does it overfit at the beginning but then become a more `` real '' and better performing classifier towards the end , even if it 's prediction rate is not 100 % ? * same thing for gaussian naive <unk> also , some people suggested that lda ( <unk> <unk> allocation ) is a good <unk> for this kind of scenario . i 'm hoping that someone could help me make some sense out of my results , since i 'm a bit confused on how i should interpret them . and if there 's anyone interested in how i actually did this , with code , you can find it [ <unk> <unk> ] ( https : <unk> ) . disclaimer : i 'm a terrible programmer with a very <unk> way of writing code . and in the end , thank you in advance for reading this `` <unk> '' and your help is greatly and immensely appreciated ! <eoq> i 'm a bot , <unk> , <unk> . someone has linked to this thread from another place on reddit : - [ /r/machinelearning ] [ what is the best classifier from the ones tested here ? : <unk> ] ( https : <unk> ) [ ] ( # <unk> ) <unk> ( if you follow any of the above links , please respect the rules of reddit and do n't <unk> in the other <unk> . ) <unk> ( [ info ] ( <unk> ) <unk> ^ [ <unk> ] ( <unk> ? <unk> ) ) * [ ] ( # bot ) <eoa> 
a question about softmax layer neurons . how are the partial derivatives ∂a^k ' / ∂a^k = 0 for all k ' ! = k , where a^k is the output from the <unk> neuron of the softmax layer . should n't a change in the output of one neuron of the softmax layer <unk> a change in the output of other neurons as well ? i 'm having a lot of trouble <unk> my head around this . any help is appreciated , thanks ! <eoq> you should do the derivatives with respect to the node 's <unk> , which i 'll call <unk> . ∂a^k ' / <unk> = a^k ' \* ( 1 - a^k ) if <unk> ' , and <unk> ' \* a^k if k ' ! = k . <eoa> 
i have a square matrix of data ( from <unk> of 2d <unk> 's ) what kind of fun things can i do ? basically as <unk> says , i have a lot of data from solving some basically <unk> <unk> fields in 2d and i am interested in just playing around with the data with some ml tools ( probably with <unk> sklearn <unk> ) i just do n't want to head in <unk> at the moment , so i wanted to ask for suggestions first on what <unk> be done ! ( quite <unk> i guess , apologies for that but this entire field is very very new to me but <unk> ) thank you : ) quick edit : forgot to add these solutions give some <unk> nice pattern <unk> structures <unk> my interest in ml application <eoq> plot that <unk> <eoa> 
can you please help with a perceptron ( neural networks ) problem ? i have the following neural networks problem and <unk> find any answer on the web . any <unk> would help . i am not looking for a complete solution , just some <unk> in the right direction . problem : write the upper <unk> <unk> of the number of iterations needed to a perceptron to learn a linear separable set with the <unk> rule : w ( 1 ) = <unk> / <unk> what category of points makes the learning hard ? <unk> the answer . thank you , dan . <eoq> it 's basically asking you to prove the perceptron convergence <unk> ( easy to google for <unk> ) . <unk> that <unk> are linear classifiers , points with non-linear <unk> are impossible to learn . the <unk> example is the xor function . <unk> 's perceptron book <unk> this and started the first ai <unk> . <eoa> 
training methods other than by example hey team , pretty general question for you . every time we talk about training a machine learning algorithm , we are <unk> to training by example . are there any other methods to training ? take image classification via neural network for example . you can show a <unk> deep net <unk> images of <unk> , and eventually it learns to find patterns in the edges and so forth . is it not possible to help the algorithm along by <unk> it know that most <unk> have 2 sets of stacked <unk> , or something similar ? <eoq> that is an idea <unk> then ml . however , `` <unk> '' to algorithm <unk> about <unk> is far from <unk> . so ml is exactly trying to avoid most of <unk> and lets algorithm learn <unk> from examples . <eoa> 
what is `` deep learning '' as opposed to just `` machine learning '' ? <eoq> `` deep learning '' is a technique developed by the field of machine learning . deep learning is a <unk> of neural networks , which <unk> did n't perform very well on many problems . more recently , neural networks have been performing well because machine learning <unk> figured out a set of techniques that make it easier to learn deep ( complex ) neural networks . machine learning does not necessarily involve neural networks . <eoa> 
what is `` deep learning '' as opposed to just `` machine learning '' ? <eoq> my understanding of this topic is a bit limited , so i 'd be interested if this <unk> is off target , but here 's what i think it means : <eoa> 
what is `` deep learning '' as opposed to just `` machine learning '' ? <eoq> `` deep '' learning refers to having a hierarchical stack of models on top of each other , each of which uses the output of the previous `` layer '' to produce another intermediate result that is a little closer to what we want to get . this is in contrast to `` shallow '' models that only do a single ( <unk> very complicated ) <unk> of input to output . <unk> approaches work very well in computer vision , because images are made up of objects , which are made up of parts , which are made up of edges ... so having a <unk> of highly <unk> models detect ever more complicated features is a much more practical approach than trying to have one model detect everything from raw pixel values . currently this is almost always done with <unk> neural networks , but there is also growing interest in `` deep '' graphical models . <eoa> 
neural network to learn to play video games is it possible to use a deep <unk> network to learn and play a video game ( e.g <unk> , <unk> , <unk> ) ? i 've read about deep reinforcement learning being used for it , but i 'm unsure how far this is from a <unk> . apologies for how <unk> this may be , i 'm still new to neural networks as a whole as i 'm in the planning stage for an undergrad project <unk> it . <eoq> hey <unk> ! so i 'm not very familiar with a reinforcement method but you should look up <unk> . not sure if it 's related but hopefully it can help give you some <unk> . i 'd link it now but i 'm on mobile . <eoa> 
i 'm an r user but know <unk> about ml . help me out ... <unk> guys , i know at some point i 'll need ml in r for <unk> projects at work . i want to be ahead of the game so the transition is as <unk> as possible . <unk> experts , consider the following : 1 ) i have no idea about ml ; 2 ) i know r ; 3 ) i know <unk> well enough to understand how to model some real world issues ; given that , would you answer me the following questions : 1 ) where can i learn the absolute basic about ml ? 2 ) given that i 've learned the <unk> , is there any books focusing on r and ml ? i 've <unk> a few ones but would n't mind a few extra tips . thank you ! <eoq> [ `` introduction to statistical learning '' ] ( http : <unk> ) <eoa> 
tutorials for svm ? where is a tutorial to <unk> svm using a programming language such as r , python , java , or c # ? i would like to both gain a deeper theoretical understanding and how to implement it in a programming language please . i tried google , but have come up empty . please help . <eoq> machine learning with r by <unk> <unk> and python data science <unk> by <unk> <unk> ; <unk> <unk> might be the answer to my own question <eoa> 
relu derivative in 0 hi , recently i found a mistake in my code where i defined the derivative of relu as 1 if node unit is > = 0 this does n't make much sense because it would be 1 for all nodes in a relu layer . i <unk> it to 1 if node unit is > 0 however , i 've been running some tests <unk> back and forth , and it seems i have slightly faster convergence with the old derivative ( > = 0 ) . what do you guys use ? it 's <unk> in 0 http : //www.wolframalpha.com/input/ ? <unk> % <unk> % <unk> % 29 http : //www.wolframalpha.com/input/ ? <unk> % <unk> % <unk> % <unk> % 29 % 29 % <unk> <eoq> false <unk> , i think i might be wrong on this one <eoa> 
have a data set of more than 1gb , how to read it in r ? the <unk> ' <unk> leads to a <unk> . and r <unk> allow opening files more than <unk> . what can i do in this case ? any help will be highly appreciated . <eoq> [ found this bit of reading ] ( https : <unk> ) <eoa> 
have a data set of more than 1gb , how to read it in r ? the <unk> ' <unk> leads to a <unk> . and r <unk> allow opening files more than <unk> . what can i do in this case ? any help will be highly appreciated . <eoq> what do you ultimately need to do with the data ? ? <eoa> 
any recommended papers or books on <unk> classification ? hello everyone , so i tried solving a previous research problem ( first one i had the <unk> of working on alone ) like a noob , i.e . i did n't really understand things like hmm and dynamic time <unk> so i did n't use it . in <unk> , it would have <unk> helped out with my problems , and it would have definitely given me a magnitude of difference in results . anyway , that was my first attempt at research as an undergraduate . am i alone in this <unk> that would have solved all of my problems but i did n't use it ' <unk> ? i hope not . anyway , i 'm curious now , can you all recommend papers that would help with sequence classification for <unk> data ? my goal is to classify sensor data into one of several classes . <eoq> i am working on anomaly detection from dynamic <unk> or accelerometer measurements . in my case i try to extract some <unk> <unk> features ( <unk> parameter ) before the anomaly detection step . the following book may be helpful for you . the authors describe multiple methods for <unk> <unk> feature extraction from <unk> . <unk> , <unk> r. , and <unk> <unk> . <unk> <unk> <unk> : a machine learning perspective . <unk> <unk> & <unk> , <unk> . <eoa> 
free course : amazon machine learning <eoq> i saw the <unk> for the course , seems pretty good . the <unk> , unfortunately <unk> a bit <unk> to me at the <unk> video . in case anyone here has taken the course , would you recommend it ? <eoa> 
can artificial neural networks be programmed to 'mutate ' ? i 'm a regular scientist curious about machine learning . so , please be patient with me . can artificial neural networks be programmed to 'mutate ' ( quasi ) randomly characterised nodes that may predict or act as hereustics to reach desired output nodes better than those features which may be hypothesised in advance ? <eoq> yes , they can . the [ <unk> of <unk> <unk> ( <unk> ) ] ( http : <unk> ) algorithm is one method to do it . such an approach works well when the optimal network <unk> is n't known ( or <unk> ) in advanced , and when there is a good way to <unk> <unk> improvements . <eoa> 
can artificial neural networks be programmed to 'mutate ' ? i 'm a regular scientist curious about machine learning . so , please be patient with me . can artificial neural networks be programmed to 'mutate ' ( quasi ) randomly characterised nodes that may predict or act as hereustics to reach desired output nodes better than those features which may be hypothesised in advance ? <eoq> i do n't know enough to fully answer this question , but there are genetic algorithms in machine learning which perform similar to what you described . it is at least a starting point for you to research . <eoa> 
can artificial neural networks be programmed to 'mutate ' ? i 'm a regular scientist curious about machine learning . so , please be patient with me . can artificial neural networks be programmed to 'mutate ' ( quasi ) randomly characterised nodes that may predict or act as hereustics to reach desired output nodes better than those features which may be hypothesised in advance ? <eoq> yes , most all machine learning algorithms do use a type of `` artificial <unk> '' . even in something as simple as linear regression , or finding a line of best fit , a program will probably <unk> through `` generations '' , <unk> <unk> a hypothesis until cost between known values is <unk> . <eoa> 
interpreting improvements to logistic regression model in r i have an existing logistic regression model that works fairly well ( 100 % precision and <unk> % recall in initial training ) , but i 'm looking to make improvements to it after about 6 months time . i 've seen some false positives in that time and i 've added some features ( from about 7 to 15 ) to improve the accuracy . the initial model was fit on about <unk> <unk> ( <unk> train , <unk> test ) , and was fit using <unk> regression to determine the optimal feature set based on <unk> . i used r to fit this model using glm . i 'm looking into a couple options for <unk> , primarily pca ( <unk> ) and regularized regression ( glmnet ) . in addition to my goal to improve accuracy , i 'm also interested in keeping the model <unk> to someone without r access ( i.e . if someone were to <unk> new data , can they easily apply my model to predict the result without <unk> r ? ) my problem is that i do n't know whether to use pca and then apply logistic regression on those <unk> ( i.e . <unk> component regression ) or work toward regularized regression instead . i 'm <unk> toward regularized logistic regression using glmnet , but having a hard time interpreting the <unk> from glmnet . i 've been following the [ glmnet <unk> ] ( http : <unk> # log ) , but i 'm pretty lost , to be <unk> . i understand some of the use of the alpha and lambda parameters , but how do i use the output from glmnet to determine : a ) the appropriate feature set , and b ) the appropriate values for lambda and alpha ? when would i use <unk> over <unk> ? it 's been a while since i was in university learning about regression , so i only remember the basics ... i get lost quickly in the <unk> notation . i 'm also not able to share anything related to my data . can anyone link to a resource showing a <unk> <unk> of using glmnet , or explain to me more simply than the <unk> ? finally , i <unk> i 'll end up with a model result that i 'm not able to fully grasp . let 's say i were to use this model result and later find prediction errors in production ... how do i go back and determine where my model is <unk> ( and <unk> determine how to improve it ) ? i know the answers to these questions are <unk> <unk> on my specific data set , but i 'm looking more for general answers or links to good resources that could help me figure these answers out for myself . <eoq> [ <unk> ] <eoa> 
<unk> wo n't converge for logistic regression program i 'm trying to do the logistic regression assignment from andrew ng 's course in python . i got it to work in octave , but for some reason when i try <unk> it over to python the <unk> procedure fails . the cost function just <unk> to <unk> . i ca n't figure out what 's <unk> it . to me everything looks exactly the same . here 's my python [ program ] ( http : <unk> ) . here 's the [ dataset ] ( http : <unk> ) . here 's a description of the [ assignment ] ( http : <unk> ) . can anyone see an obvious bug ? <eoq> you should try using a <unk> or print <unk> to <unk> values <eoa> 
how many learning curves should i plot for a <unk> logistic regression classifier ? if we have k classes , do i have to plot k learning curves ? because it seems impossible to me to calculate the <unk> error against all k <unk> vectors at once . to <unk> , the learning curve is a plot of the training & cross <unk> set <unk> vs training set size . this plot should allow you to see if increasing the training set size <unk> performance . more generally , the learning curve allows you to identify whether your algorithm <unk> from a bias ( under fitting ) or variance ( over fitting ) problem . <eoq> <unk> <eoa> 
cost function confusion in many of the guides i have read online , ( in particular [ this one ] ( http : <unk> ) ) , there is a step at the beginning of back <unk> where you calculate the discrepancy between your observed output and your target output . the part that i am very confused about is that sometimes this discrepancy is <unk> as <unk> = a - y ( where `` a '' is the observed output and `` y '' is the target output ) . later on , the guides will <unk> a `` cost function , '' which seems to me to be the same thing as the difference formula above ? however , a totally different equation ( http : <unk> ) is then given . is the difference formula just an <unk> of the more complicated formula ? thanks ! <eoq> in regression , the cost function is usually the mean or sum of squared errors -- each individual term in the sum is ( <unk> ) <unk> . what you called the discrepancy is actually the derivative of this error <unk> to a as used in backpropagation : http : //www.wolframalpha.com/input/ ? <unk> % <unk> % 29 % <unk> when you do binary classification the cost function is [ the cross-entropy ] ( http : <unk> ) between the wanted distribution and what the network predicts . this is a special case of the [ negative <unk> under a multinomial distribution ] ( http : <unk> ) \* that is used in multiclass problems . \* for neural networks this formula is slightly different <eoa> 
my classifier has 100 % accuracy , is something wrong or is this good data ? hello everyone , i ran several models on a binary classification problem , each performing at around <unk> % classification accuracy . i then used [ stacking ] ( https : <unk> # stacking ) to build a new model on top of those and i got near 100 % accuracy . my process : * run several models to see how well they performed for accuracy * pick the top 4 models ( lda , random forest - 15 trees , logistic regression , and random forest - <unk> trees ) * create new features from those models - taking the predictions of each model ( using cross validation so that i would train <unk> of the data to predict the last <unk> , then <unk> out so that there would be no data <unk> ) . * run a binary classifier on the new dataset which includes those extra features * get 100 % accuracy . i tried <unk> cross-validation on the new model and it performed at around <unk> % + accuracy . do i need to check something else to make sure that this model is <unk> ? <eoq> make sure you are <unk> the ensemble on totally untouched data . by untouched i mean that this data has n't been used for fitting the base or `` <unk> '' models . edit : and also not used for selecting <unk> for your <unk> model <eoa> 
need ml algorithm <unk> ? hi , i have read ml long time ago . i 've <unk> time and in need to choose the algorithm to complete my following task : <unk> , is <unk> my website . i make them <unk> the form and have <unk> the necessary signal ( attributes ) with me like whether they have <unk> <unk> or not , whether email is <unk> is not , phone no is given or not , <unk> date is fixed , destination location is fixed or not . but along with that i have many <unk> who do n't <unk> the form completely or just uses <unk> phone number . i again <unk> , i have lot of signal available with me , and i need to filter out the <unk> who is certain to go for <unk> so that i can personally <unk> them . i also need some score as well on the scale of 10 . which ml algorithm is best suited for this job and why ? <eoq> regularized logistic regression . because you need a starting point , you ca n't expect to find the <unk> method on the first attempt . <eoa> 
day of the week with generic <unk> <unk> nn 's i have been with <unk> data and have been representing monday as `` <unk> '' and <unk> as `` 10000 '' . in general is this a good idea or is it better to represent the data as the day of week in binary ? ( <unk> ) i realize this is a pretty generic question . i am just looking for a `` rule of thumb '' . <eoq> from what i know , first method is common for categorical data ( days of the week ) , <unk> `` one-hot encoding '' . i do n't see any advantages in second method besides slightly smaller number of inputs . <eoa> 
what is the most common model for <unk> classification ? i 'm writing a few <unk> on the subject and i 'd like to get some opinions on what i should teach . <eoq> this paper here might be useful : ) http : <unk> <eoa> 
how important is performing cross validation on your algorithm 's parameters ? <eoq> a lot . if you have a parameter you are changing it is just like another feature . if you run your algorithm with 100 's or 1000 's of parameter <unk> you will have results which are <unk> ' just due to chance . <eoa> 
would you benefit from deep learning examples & tutorials ? i 'm wanting to build a sort of blog that dives into different aspects of deep learning , a project based/case study style learning experience . assuming the content was good , would you subscribe to a blog like that ? would you be interested in working through the problems ? <eoq> definitely ! it sounds like it could be fun <eoa> 
would you benefit from deep learning examples & tutorials ? i 'm wanting to build a sort of blog that dives into different aspects of deep learning , a project based/case study style learning experience . assuming the content was good , would you subscribe to a blog like that ? would you be interested in working through the problems ? <eoq> i would definitely subscribe . go ahead please do it <eoa> 
would you benefit from deep learning examples & tutorials ? i 'm wanting to build a sort of blog that dives into different aspects of deep learning , a project based/case study style learning experience . assuming the content was good , would you subscribe to a blog like that ? would you be interested in working through the problems ? <eoq> if it 's python , if it provides some math - not too hard - not too <unk> down - something that a <unk> background can figure out - pretty <unk> - links to further reading - links to <unk> reading , if the projects have <unk> <unk> so some are short and some are <unk> ( 100 lines ) and few are large ( <unk> ) . <eoa> 
would you benefit from deep learning examples & tutorials ? i 'm wanting to build a sort of blog that dives into different aspects of deep learning , a project based/case study style learning experience . assuming the content was good , would you subscribe to a blog like that ? would you be interested in working through the problems ? <eoq> <unk> with clear description of every variable would be a <unk> <eoa> 
would you benefit from deep learning examples & tutorials ? i 'm wanting to build a sort of blog that dives into different aspects of deep learning , a project based/case study style learning experience . assuming the content was good , would you subscribe to a blog like that ? would you be interested in working through the problems ? <eoq> yes please , i would definitely benefit from <unk> learning ! <eoa> 
long boolean vectors as data set - looking for suitable model / algorithm hey ! i am new to this topic , <unk> any help is greatly appreciated ! my data set <unk> of two pieces : * one factor <unk> roughly from -1 to <unk> * a rather long ( say <unk> . ) boolean array in the training process , i want to find correlations between the factor and what values in the boolean array are set . <unk> , the algorithm should be able to predict a value for the factor when analyzing an array . the previous solution used some kind of svm . unfortunately i do n't have access to the source code and ca n't really think of a way to <unk> this . thank you for any kind of help ! <eoq> my <unk> opinion : one-hot encoding for the array boolean values , assuming you have enough data for that dimension size . then run a logistic regression on it for some interpretable results ( by reading the coefficients ) , or run a random forest for high accuracy . maybe you can determine the importance of features with <unk> selection ... <unk> i ca n't link to it because <unk> is down at the moment ( they <unk> scikit-learn ) . then if you <unk> down on the features , your logistic regression model may become more interpretable . <eoa> 
i 've already implemented a naive bayes classifier to help me <unk> the sentiment of tweets . what other algorithms can i use ? i have a data set of tweets , and i need to classify them as either pro , <unk> , or <unk> with respect to some topic . i 've already used the <unk> naive bayes classifier on sklearn , and would like a second algorithm to compare the classification against . my first thought would be <unk> <unk> ( i can use <unk> to turn the tweet into a vector of <unk> , and then run it through the <unk> ) . any other suggestions ? <eoq> i 've tried using a <unk> classifier like that for sentiment analysis on larger <unk> , but it did n't work too well . you could build a long <unk> neural network and then classify it 's output , which is how sentiment analysis is often done with <unk> . take a look at : http : <unk> <eoa> 
trying to decide on algorithm for my data hi . i 've began reading a lot about machine learning , but having no advanced background in <unk> , i find it hard to relate to most of the online information for selecting the proper algorithm on my data . i do programming for a <unk> . this is a pet project as will be clear from my example , so i do n't need an absolute best fit or optimal result . with this said , here is my problem : i play path of <unk> , a free game . it 's extremely similar to <unk> . it 's got one of the most complex set of items attributes . the game allows players to have <unk> online ( so list items with prices ) as well as give the information about what the players are wearing . i spent months of time writing code to extract this data , so i have historical data about what items were worn , what they were <unk> with , what items were listed for sale , which ones sold , how long it took to sell , etc . this data is rather massive , were talking a few hundred <unk> . my data is split in 2 big data sets : * <unk> * ( what players wear ) * * items for sale * ( pricing data is very noisy , as there are no <unk> in pricing ) * <unk> a breakdown of how items work , as far as stats are concerned : * there are <unk> stats to chose from . those stats never change . * ( ex : +40 % fire resistance ) * * an item is a combination of <unk> of those stats . * every stat has one category . when an item has one stat , it can not get another stat within the same category . * ( ex : if the item has +40 % fire resistance , it can not have the 2nd stat <unk> % fire resistance ' . there are <unk> categories ) * * every stat has a known chance to occur . * ( ex : 0.25 % chance to get the stat +40 % fire resistance ) * * if the item is listed for sale , it <unk> have a listed price . the system is n't much more complicated than that . i simplified a lot the details , but this is what matters . i have 2 questions i wish to answer with machine learning : * <unk> an item <unk> * <unk> to find how <unk> related are stats together ( ie : out of the stats that players wear , estimate the <unk> ) ** i tried the andrew ng online course , as well as reading a lot of documentation online , the weka tool , and none seem to make it clear what algorithm i should pick . this is what i am planning to use as input neurons : * every category ( ie : set of stats ) is an input neuron , with the currently <unk> stat within the category being the value . * the <unk> ' of every stat in the category is simply its probability to occur . * the stats within the category are ordered from least probable to most probable ( ie : best to worst ) . * <unk> adjust the categories ranges within 0-1 . ex : ( one category ) stat | range ( of fire resistance ) | probability -- -| -- -| -- -- of the magma | <unk> | 0.10 % of the volcano | <unk> | 0.25 % of the <unk> | <unk> | <unk> % of the <unk> | <unk> | 1 % of the <unk> | <unk> | 1 % of the <unk> | <unk> | 1 % of the <unk> | <unk> | 1 % so if my item has `` of the magma '' , for that neuron , i would give it 0.10 value to that input in the nn . if it had `` of the volcano '' , i would give it <unk> . every category is given an input accordingly , with 0 <unk> none was chosen . so with this said , i am not sure which algorithm to pick to feed it the output neuron ( pricing ? ) . my data is very noisy for prices . should i use a classifier with the output neurons being slices of prices ( ie : neuron 1 = 0-1 $ , neuron 2 = <unk> $ , neuron 3 = 5-10 $ , etc . ) ? or should i have only one output neuron ? i thought the slices would effectively <unk> ' the issue of very variable pricing and at least give a good idea of the price range to expect . and as far as <unk> the <unk> of connections between stats for items that are worn , i do not see what output neuron i could map , so i was wondering if that was even possible . i though i should rank <unk> stats that are being removed ( ie : player stopped <unk> item with stats x , y , z , so input those values but as negatives ) , but that still leaves me wondering how to extract the correlation between the stats . i would <unk> any help as such . i do n't expect any <unk> , i just want a <unk> in the proper direction ! thanks again <eoq> let me <unk> this by saying that i 'm no ml expert , i 've only part gone through hinton 's coursera class . i do play <unk> though , and this is an interesting project . <eoa> 
trying to decide on algorithm for my data hi . i 've began reading a lot about machine learning , but having no advanced background in <unk> , i find it hard to relate to most of the online information for selecting the proper algorithm on my data . i do programming for a <unk> . this is a pet project as will be clear from my example , so i do n't need an absolute best fit or optimal result . with this said , here is my problem : i play path of <unk> , a free game . it 's extremely similar to <unk> . it 's got one of the most complex set of items attributes . the game allows players to have <unk> online ( so list items with prices ) as well as give the information about what the players are wearing . i spent months of time writing code to extract this data , so i have historical data about what items were worn , what they were <unk> with , what items were listed for sale , which ones sold , how long it took to sell , etc . this data is rather massive , were talking a few hundred <unk> . my data is split in 2 big data sets : * <unk> * ( what players wear ) * * items for sale * ( pricing data is very noisy , as there are no <unk> in pricing ) * <unk> a breakdown of how items work , as far as stats are concerned : * there are <unk> stats to chose from . those stats never change . * ( ex : +40 % fire resistance ) * * an item is a combination of <unk> of those stats . * every stat has one category . when an item has one stat , it can not get another stat within the same category . * ( ex : if the item has +40 % fire resistance , it can not have the 2nd stat <unk> % fire resistance ' . there are <unk> categories ) * * every stat has a known chance to occur . * ( ex : 0.25 % chance to get the stat +40 % fire resistance ) * * if the item is listed for sale , it <unk> have a listed price . the system is n't much more complicated than that . i simplified a lot the details , but this is what matters . i have 2 questions i wish to answer with machine learning : * <unk> an item <unk> * <unk> to find how <unk> related are stats together ( ie : out of the stats that players wear , estimate the <unk> ) ** i tried the andrew ng online course , as well as reading a lot of documentation online , the weka tool , and none seem to make it clear what algorithm i should pick . this is what i am planning to use as input neurons : * every category ( ie : set of stats ) is an input neuron , with the currently <unk> stat within the category being the value . * the <unk> ' of every stat in the category is simply its probability to occur . * the stats within the category are ordered from least probable to most probable ( ie : best to worst ) . * <unk> adjust the categories ranges within 0-1 . ex : ( one category ) stat | range ( of fire resistance ) | probability -- -| -- -| -- -- of the magma | <unk> | 0.10 % of the volcano | <unk> | 0.25 % of the <unk> | <unk> | <unk> % of the <unk> | <unk> | 1 % of the <unk> | <unk> | 1 % of the <unk> | <unk> | 1 % of the <unk> | <unk> | 1 % so if my item has `` of the magma '' , for that neuron , i would give it 0.10 value to that input in the nn . if it had `` of the volcano '' , i would give it <unk> . every category is given an input accordingly , with 0 <unk> none was chosen . so with this said , i am not sure which algorithm to pick to feed it the output neuron ( pricing ? ) . my data is very noisy for prices . should i use a classifier with the output neurons being slices of prices ( ie : neuron 1 = 0-1 $ , neuron 2 = <unk> $ , neuron 3 = 5-10 $ , etc . ) ? or should i have only one output neuron ? i thought the slices would effectively <unk> ' the issue of very variable pricing and at least give a good idea of the price range to expect . and as far as <unk> the <unk> of connections between stats for items that are worn , i do not see what output neuron i could map , so i was wondering if that was even possible . i though i should rank <unk> stats that are being removed ( ie : player stopped <unk> item with stats x , y , z , so input those values but as negatives ) , but that still leaves me wondering how to extract the correlation between the stats . i would <unk> any help as such . i do n't expect any <unk> , i just want a <unk> in the proper direction ! thanks again <eoq> first , have you tried just taking something like scikit-learn and just fitting a simpler regression model , like linear regression or random forest regression , to the data first ? often those will get you 90 % + of the accuracy of a more complex model in about 1 % of the time , and help you figure out which sets of attributes and outputs will work well . if you just use the <unk> ' price as the output , you can use a regression model , and if you want to <unk> the prices into ranges , then use a classifier ( where each price range is a separate class ) . if you 're determined to use a neural network , then for <unk> the output there 's two simple ways you can do it . the easiest is to have a single output neuron for price that uses a <unk> linear activation function and squared error loss ( in other words , treat it as a regression problem ) . this will <unk> your model to try to estimate the average price across that set of inputs . the second way to do it is to break the price into ranges and treat it as a classification problem : stick a softmax layer at the end with categorical cross-entropy as the loss function . this will make it estimate the probability that an item will sell in a given range . you do have to be a bit <unk> to 1 ) have a <unk> ' for every possible price , and 2 ) not have too many outputs , since this can make the model harder to fit . advantages of the linear output is that you only need one output neuron , it should fit quickly , and it will give you a flexible output . <unk> is that it wo n't give you much in the way understanding the variance of the price ( i.e . the average might be $ 1 , but is it $ 1 +/- $ <unk> , or $ 1 +/- $ <unk> ? ) advantages of the <unk> ' output is that it will give you some measure of uncertainty , so it might tell you that it 's <unk> % sure that it will sell for $ <unk> , or it might tell you that it 's 25 % likely to sell for $ <unk> , 25 % for $ 2-3 , etc . <unk> are that it will probably be slower to train , and if you <unk> the prices too small , then you could get odd results if you have very rare combinations of attributes that sell for <unk> prices . for analyzing the combinations of items , you might be able to use something like <unk> on your trained network to get some idea of if or how particular combinations of traits cluster , but i 'm less familiar with that sort of thing . it might be easier to fit a separate model , either by filtering to only <unk> items that were actually <unk> and then doing a ( <unk> neural network ) clustering on those , or maybe through building a model that tries to predict the <unk> attribute given the other <unk> on the item . <eoa> 
how do i handle large csv datasets ? i 'm a beginner messing around with some basic application of ml , applying simple classifiers and stuff , and i stumbled along a ctr challenge avazu posted a while back on kaggle : https : //www.kaggle.com/c/avazu-ctr-prediction . my problem is that the training set here is a single csv ~2gb in size , and possibly a few hundred million entries . i tried opening this dataset in the weka viewer , and even after increasing the stack size to 4gb , it stopped responding after some time , forcing me to close it . this brings me back to the question , what tools/libraries do you guys use to handle such large datasets ? <eoq> let 's <unk> the problem straight : it does n't matter to you if the dataset size is <unk> if you can hold only 1gb dataset . there is no other way to shrink down your dataset other than to take a subset of it . every <unk> approach to get this subset would work for you . <eoa> 
how do i handle large csv datasets ? i 'm a beginner messing around with some basic application of ml , applying simple classifiers and stuff , and i stumbled along a ctr challenge avazu posted a while back on kaggle : https : //www.kaggle.com/c/avazu-ctr-prediction . my problem is that the training set here is a single csv ~2gb in size , and possibly a few hundred million entries . i tried opening this dataset in the weka viewer , and even after increasing the stack size to 4gb , it stopped responding after some time , forcing me to close it . this brings me back to the question , what tools/libraries do you guys use to handle such large datasets ? <eoq> my <unk> opinion : <eoa> 
how do i handle large csv datasets ? i 'm a beginner messing around with some basic application of ml , applying simple classifiers and stuff , and i stumbled along a ctr challenge avazu posted a while back on kaggle : https : //www.kaggle.com/c/avazu-ctr-prediction . my problem is that the training set here is a single csv ~2gb in size , and possibly a few hundred million entries . i tried opening this dataset in the weka viewer , and even after increasing the stack size to 4gb , it stopped responding after some time , forcing me to close it . this brings me back to the question , what tools/libraries do you guys use to handle such large datasets ? <eoq> you could simply read it line by line with csv <unk> , put it in list then convert it to numpy array . <eoa> 
help a beginner out ? hi ! i am a college student trying to <unk> my resume with some projects that are a little different than a standard <unk> application and have always been interested in ml . i am going through <unk> in action ' right now . i am just curious if anyone could suggest a decent data clustering project ? i am completely unable to think of anything interest right now : / any advice or suggestions will be appreciated . thanks <eoq> anyone here ? ? <eoa> 
are there any datasets <unk> available on which i can test the <unk> algorithm i have developed <eoq> there is <unk> -- also ask there . <eoa> 
i need help with reinforcement learning i 'm trying to do an ai with machine learning , so i started to learn how to do machine learning , but i do n't think i understand enough of it to do one . i tried learning by watching some courses , but what they say does n't seem to be related with what i 'm trying to do . if anyone could help me , i 'm trying to do it with <unk> and python <unk> at the moment . i 'm currently trying to code the environment class . i am doing this just for fun and to learn how it works . <eoq> so what <unk> you trying to do ? without a <unk> question , i doubt anyone can really help you . what are your problems with the environment class ( from [ this tutorial ] ( http : <unk> ) i assume ) ? for a general introduction to <unk> , <unk> and <unk> 's <unk> <unk> book is <unk> available [ here ] ( http : <unk> ) . you might also like [ this udacity course ] ( https : <unk> -- <unk> ) by <unk> <unk> and <unk> <unk> , or <unk> <unk> 's [ ideal mooc ] ( http : <unk> ) although it 's a bit more advanced . <eoa> 
markov decision process in r for a song suggestion software ? okay , so i 'm not exactly sure if this <unk> here , but this is my problem : we have a music player that has different playlists and automatically suggests songs from the current playlist i 'm in . what i want the program to learn is , that if i <unk> the song , it should decrease the probability to be played in this playlist again . i think this is what 's called reinforcement learning and i 've read a bit about the algorithms , <unk> that mdp seems to be exactly what we have here . i know that in mdp there are more than one state , so i figured for this case it would mean the different playlists . like depending on the state ( playlist ) i 'm in , it <unk> the songs that it <unk> <unk> the best and get `` <unk> '' ( by <unk> ) if it has chosen <unk> . so what i 'm asking is , if you guys think this is the right approach ? or would you suggest a different algorithm ? does all of this even make any sense , should i provide more information ? if it does sound right , i 'd like to ask for some tutorials or starting points getting about mdp in r. i 've <unk> online but have only found the mdp <unk> in r and it kind of does n't really make sense to me . do you have any suggestions ? i 'm really helpful for any kind of advice . : ) <eoq> mdp <unk> here . i would avoid the mdp formulation for this problem since what you would have here is a partially <unk> ( in the state of the person selecting songs ) markov decision process , which are <unk> difficult . depending on your requirements , i think this could be a good way to proceed that does n't take a lot of advanced ml education : * <unk> your <unk> to a single playlist . <unk> playlists is a separate and likely more complicated problem that you do n't need to deal with . * if you have the resources to build song features that describe the attributes of each song ( think of these like <unk> 's <unk> ) then you can build a model where an action ( <unk> ) for one song can <unk> whether you want another song to be played . if you do n't have the <unk> to create song features , then your modeling <unk> become very simple . * assuming you have song features , you should start with a logistic regression . from there , many <unk> and improvements are possible . <eoa> 
can i use sklearn 's naive bayes to classify tweets ? i have a data set of tweets with keywords <unk> to <unk> <unk> . i want to be able to classify a new tweet as either <unk> , <unk> , or <unk> . i hear naive bayes is a way to do this , but i can not find any documentation on how can implement the classifier with words as <unk> rather than numbers . can anyone provide some insight ? <eoq> yes you can , but you have to figure out a way of representing tweets so that the classifier can do its work . [ these <unk> ] ( https : <unk> ) go over the <unk> representation and how you use it to fit the multinomial naive bayes model . other groups have done this kind of stuff with <unk> word vectors , where individual words are given a representation in an arbitrary vector space <unk> from a huge text corpus via neural network models ( see [ word2vec ] ( https : <unk> ) ) . if you 're using python , the sklearn library has <unk> functions for this kind of feature extraction , see [ here ] ( http : <unk> ) . <eoa> 
trouble pre-processing large <unk> of image data this [ kaggle competition ] ( https : <unk> ) is my first time applying my limited understanding of ml . my knowledge is mainly from andrew ng 's coursera class . any how , i 've <unk> up a [ class ] ( https : <unk> ) to get started with the preprocessing of these images . the basic function of the code is to create a data <unk> for each image that has attributes like its id and rgb values for each pixel ( found using <unk> ( ) ` ) . now , my computer is pretty old and only has <unk> of <unk> ... i 'll be getting a new computer soon with <unk> and nearly <unk> the processing speed , but in the mean time , i 'd like to find a way to more <unk> generate , store , and represent this data so my ml scripts can work with it reasonably at a later stage of the project . the method you see in [ the code ] ( https : <unk> ) <unk> up my computer and does n't finish <unk> . i thought of <unk> a <unk> statement in the <unk> of <unk> ( ) ` that <unk> if i 'm at , let 's just say , a 5 % <unk> of the total data set -- > if so , then write the binary of the <unk> , <unk> , to file and then set it to none so i can free up memory -- > continue where i left off , <unk> <unk> to the same binary file until i 've gone through all <unk> training examples . the problem with this is that after testing this out on just 5 images , my binary file is about <unk> ( <unk> each , which <unk> for all <unk> images <unk> up to a <unk> ! ) . how the <unk> can i possibly work with this much data <unk> ? ! what do ml experts usually do to <unk> high <unk> image data ? how do i <unk> the rgb values of each pixel in an image for every image in this huge of a data set ? i know that 's more than a couple questions , but i 'm not really sure where to go from here . advice , please and thank you ! <eoq> i 'm sorry to say it more or less <unk> down to resources . a dataset that big really requires that faster <unk> you 're talking about . <unk> of <unk> just is n't enough to do what you need . i just took a look at your code and the competition . for a start you want to use [ <unk> ] ( https : <unk> # <unk> ) over <unk> . it is many , many , many times more <unk> than standard <unk> . that alone may solve a lot of your issues . <eoa> 
classification algorithms - a discussion hi there , i 've been <unk> with writing my own implementation of any classification algorithm for an assignment . i was going to pick the c4.5 algorithm but i thought i 'd ask you guys what you think ( as i thought it 'd be interesting to see all the different opinions ) . i chose the c4.5 because i am quite familiar with how it works and feel that it would be <unk> for a beginner to attempt it ( i have never tried to write my own implementation of a classification algorithm before ) . my data deals with a bunch of measurements and i have to decide if the next entry is a certain type of animal . just in case you were wondering ! : ) so if you were a beginner , what algorithm would you choose ? better yet , if you had to decide on an algorithm with your current knowledge , would it be different than if you were just starting out ? what <unk> did you encounter when you were first starting ? also , if you could think of any resources that may be beneficial to my <unk> ( not direct answers as i 'd like to learn from my assignment ! ) ... <unk> away ! : p sorry if this is the wrong place , i got a <unk> and i thought it 'd be pretty cool to hear form an expert or two ! thanks ! <eoq> the first classifier i implemented was logistic regression . maybe that 's too simple for your assignment though . <eoa> 
classification algorithms - a discussion hi there , i 've been <unk> with writing my own implementation of any classification algorithm for an assignment . i was going to pick the c4.5 algorithm but i thought i 'd ask you guys what you think ( as i thought it 'd be interesting to see all the different opinions ) . i chose the c4.5 because i am quite familiar with how it works and feel that it would be <unk> for a beginner to attempt it ( i have never tried to write my own implementation of a classification algorithm before ) . my data deals with a bunch of measurements and i have to decide if the next entry is a certain type of animal . just in case you were wondering ! : ) so if you were a beginner , what algorithm would you choose ? better yet , if you had to decide on an algorithm with your current knowledge , would it be different than if you were just starting out ? what <unk> did you encounter when you were first starting ? also , if you could think of any resources that may be beneficial to my <unk> ( not direct answers as i 'd like to learn from my assignment ! ) ... <unk> away ! : p sorry if this is the wrong place , i got a <unk> and i thought it 'd be pretty cool to hear form an expert or two ! thanks ! <eoq> i think c4.5 will be just fine . i guess you could also start with <unk> and then <unk> it to learn more about the differences . to <unk> overfitting , you may want to look into some <unk> algorithms . then if you want to go further , you could look into random forests were really popular a few years ago i think ( maybe they still are , but ml is n't my main field and i got the <unk> their <unk> has <unk> now that deep learning works so well ) . in my <unk> days i mostly implemented neural networks and genetic algorithms . you could do those too , although genetic algorithms may be slightly harder to apply to your problem . <eoa> 
